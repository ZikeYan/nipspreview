graphflow large displacement scene flow graph matching abu germany germany germany approach computing dense scene flow large displacement rgb-d large displacements step estimate motion state-of-the-art approaches rgb depth achieve graph matching sparse depth edges additional continuous-label energy sparse graph matching challenging state-of-the-art computer vision estimate dense correspondence field pair small motion textured task general work scene camera large objects walking large displacements time camera intermediate frames sequence task smooth large displacements scene flow estimation variational approaches achieve motion large displacements estimate general motion objects task work general motion computing dense scene flow pair rgb-d images moving pixel matches pixel local corresponding point compute dense flow large abu alhaija sparse matches frames utilize estimate dense flow points matched surf sift scene textured active depth active stereo depth scene flow sparse matches utilize depth edges rgb-d images describe object large edge description occlusion robust edge edge matches robust matching approach graph matching assign edge graph matches structure graph moving camera scene work depth construct graphs respect moving objects camera second dense scene flow graph matching sphereflow method hornacek energy function left-right consistency well smoothness data energy alpha respect state-of-the-art method large displacement scene flow rgb-d image graph matching depth continuous-label energy scene flow left-right consistency well smoothness local work scene flow approaches scene flow estimation frames input input images compute depth structure depth rgb-d images variational approaches scene flow estimation rgb-d gradient depth additional global second order variational approaches small large motion graphflow large displacement scene flow graph scene flow approaches good large estimate scene flow based large constant image hornacek patchmatch based algorithm local data term generate motion proposals large displacement approaches textured approach model term left-right consistency variational optical flow matching scene flow hornacek sparse surf feature matches match surf features image dense scene flow approaches optical flow feature matches large displacement estimation image matches image feature matches dense motion additional images improve feature feature matches approach graph matching improve feature graph matching field pattern recognition graph approach graph structure depth edge contrast edge matching rgb images features depth channel structure graph features depth depth graph method graph matching scene flow approach depth edges input rgb-d construct associated match second sparse motion graph matching assign smooth rigid motion observed pixel graph matching rgb-d images depth color channel abu alhaija graph edge description segments represented center average appearance descriptor describe foreground region normalized depth gradient order compute description segment neighboring edge pixels descriptor lower graph description segments centers connected description segment center connected nearest respect geodesic distance depth map large depth geometry term graph depth channel edges depth map edge edge pixel depth gradient foreground region sift descriptor three dierent foreground region describe appearance edge edges appearance frames description edge pixels appearance description description segment represented center point normalized depth gradient based description segments construct graph set descriptor segments centers image graph edges nearest description geodesic distance depth geodesic distance graph edge segments connected depth object motion set graph edges description segments centers represented graph edge graphs image set description segment matching graphs represented graphflow large displacement scene flow graph matches matching satisfy description segment matching function app app geom geom occ occ energy three weighted app geom occ appearance term app term appearance matched descriptor euclidean distance sift feature vectors set app penalizes vectors note pairwise term penalizes consistency occlusion term occ penalizes description segments energy active contrast constant utilize occlusion image depth image occlusion normalized full image set occ average descriptor description segment center geometry term geom pairwise term pairs neighboring neighbor set edge normalized depth gradient vectors geometry matching map neighboring description segments description segments satisfy geometry term global energy function dual decomposition graph matching torresani good close global abu alhaija scene flow result graph matching step set sparse matches descriptor result dense flow field work scene flow minimize energy data term rgb gradient consistency scene flow points pixel image image nearest neighbor weighting constant weighting pairwise smoothness term weighted smoothness motion pixels point term consistency scene flow minimize energy three rigid motions sparse matches corresponding points pixels associated sparse match motion matched point respect depth second patchmatch hornacek minimize data term smoothness term consistency term proposals matches scene motions second satisfy corresponding graphflow large displacement scene flow graph hungarian torresani visual graph matching rgb images correct match correct match wrong result motions step scene flow proposals global energy alpha expansion expansion alpha expansion motions second contrast optimize global energy rgb-d image pairs image pair objects large motion quantitative graph well evaluation dense scene order graph matching algorithm description segments image generate ground truth matching description segment image corresponding description segment note description segments centers images correct matches correct description segments ground truth assign three web abu alhaija sphereflow sequence hammer flow field image image result pixels depth flow vectors result constant small motion result sphereflow graph matching points patchmatch hammer sequence better hammer points flow color walking sequence degenerate cases motion estimation description segment matched graph matching correct correct match wrong full quantitative evaluation table correct matches correct matches compare graph matching nearest neighbor matching hungarian matching description segments matching graph structure lower compare graph matching approach torresani approaches appearance term optimization dierent geometry occlusion cases method torresani observed tea weighted occlusion term constant scenes occlusion result better scene matches correct euclidean distance wrong matches correct table approach average distance better optimization energy dual decomposition respect lower graphflow large displacement scene flow graph sequence hammer tea walking sift hungarian torresani table quantitative evaluation graph matching correct matches correct matches algorithm approach well approach tea full energy constant occlusion sequence hammer tea walking sift hungarian torresani lower table quantitative evaluation graph matching associated optimization average euclidean wrong matches ground matching lower graph matching cases global table cases global three cases lower model close energy dense scene method dense scene graph rgb-d scenes large displacements challenging data ground truth scene flow compare sphereflow state-of-the-art large displacement scene flow scenes result sphereflow observed degenerate result motions close degenerate sphereflow surf matches textured graph weighting improve abu alhaija time time time time point point images generate intermediate images dierent time note pixels depth web matching good matches optimize energy sequence consistency energy smoothness term data optimize energy consistency better additional web image pairs scene flow intermediate frames dierent smooth work graph matching depth edges estimate large displacement motions rgb-d continuous-label energy dense scene achieve step additional gradient work optical flow estimation rgb depth edges image challenging correspondence time visual graphflow large displacement scene flow graph dense optical flow estimation springer correspondence algorithm image robust springer matching object recognition tpami energy graph tpami large displacement optical descriptor matching variational motion tpami approach edge tpami graph matching pattern model dense variational scene flow rgb-d graph matching pattern recognition depth springer scene scene flow tpami rgb-d dense motion estimation color ieee scene flow ieee variational method scene flow estimation stereo ieee dense rgb-d scene ieee matching motion occlusion ieee image features scene flow ieee motion field estimation stereo image springer optical dense image vision computing abu alhaija dual decomposition approach feature tpami scene ieee ieee rigid scene ieee dense scene flow rgb-d springer large displacement optical flow ieee motion optical flow tpami approach visual correspondence computer vision pattern ieee computer ieee robust segment matching approach based descriptor pairwise visual image
multi-view scene flow view variational approach tel aviv tel aviv israel israel tel aviv tel aviv israel novel method structure scene flow calibrated multi-view point cloud parametrization structure scene flow allows directly desired global energy functional proposed incorporate sequences simultaneously recover depth scene functional multi-view consistency brightness constancy piecewise smoothness assumptions directly large work representation variational framework directly multi-view minimization functional despite non-convex optimization proposed method tested real synthetic structure motion space motion maps motion field problem scene flow scene flow defined dense motion field non-rigid scene directly recovery surface scene flow simultaneously compute structure scene flow system consists calibrated synchronized cameras fields variational framework proposed incorporate sequences simultaneously recover depth scene elaborate parametrization spatial temporal set variational method methods scene flow surface estimation problem compute projection desired disparity optical flow parametrization allows assumptions unknowns motion field scene smoothness assumptions parametrization point cloud parametrization structure pixel reference depth motion vector parametrization allows multiple spatio-temporal leads estimation scene flow structure methods motion structure suggested independent computation optical flow field camera consistency flow consistency stereo motion disparity map recovery scene flow structure suggested methods limited views method involves multi-view spatio-temporal set sequences solving global energy functional defined incorporate multi-view constancy assumption regularization imposed piecewise smoothness directly motion data term constraints large discontinuities motion depth non-quadratic cost approach optical flow variational approach method multiple views minimization functional solving associated euler-lagrange approach tested method challenging real synthetic ground truth evaluation error stereo optical flow algorithms suggested ranking stereo algorithms errors novel accurate global energy functional desired assumptions structure scene functional large representation variational framework leads better problem directly minimize functional despite challenging non-convex optimization studies evaluation work point cloud representation considered scene flow recovery view mesh representations representations scene considered limited representation moving large number cameras order space independent studies huguet proposed simultaneously compute optical flow field disparity maps time disparity time variational framework solving scene flow structure studies regularization imposed disparity optical flow assumptions refer directly methods multiple multi-view energy minimization framework zhang stereo algorithm method imposed optical flow stereo constraints discontinuities image view additional set parallel camera multi-view method suggested variational error shape motion shape motion multi-view methods representations simultaneously surface neumann object method furukawa initial mesh rigid motion shape shape motion assumption smoothness surface methods furukawa limited fixed mesh method simultaneously surface scene scene flow static cameras calibrated sequence brightness constancy spatial temporal energy functional minimize variational framework solving associated system set calibrated synchronized sequence camera projection camera projection surface point image sequence time figure point projected pixels cameras location projected third displacement vector point location point displacement projection image time points reference functions energy functional total energy functional minimize edata reference image directly surface scene flow respect parametrization stereo data term edata result surface scene flow minimization edata regularization image regularization occluded pixels relative terms regularization elaborate data data term assumption spatial temporal projection images displacement parametrization solution scene camera constraints assumption cameras time data term three penalizers reference image deviation assumption bcs1 bcs2 deviation assumption reference image views time penalizers pixel defined note surface point projected pixel time projected image pixel image point reference time three three functions unknowns image recover functions number unknowns independent number multi-view system methods additional cameras additional unknowns optical flow disparity representation image bcs1 bcs2 function occluded pixels computation cost non-quadratic cost function pixels model linear large note additional constancy studies estimating optical flow scene flow imposed assumption assumption spatial smoothness piecewise smoothness assumptions imposed motion field model total generally field function discontinuities scene flow euler-lagrange equations points projected points sequences corresponding pixels time image terms equations respect result penalizer deviation motion smoothness assumption penalizer note order work smoothness second order smoothness total energy function pixels reference data neumann image euler-lagrange equations respect functional respect involves computing derivatives images desired data space euler-lagrange equations images non-linear functions unknowns computation image derivatives respect requires refer bcs1 bcs2 motion surface optimization functions minimize functional leads set equations euler-lagrange associated euler-lagrange equations generally parametrization functional desired model challenging minimization data terms non-quadratic penalizers non-linear system unknowns problem multiple result non-convex equations additional projection non-linear unknowns figure sphere rotating plane reference view image associated data term computing view three occlusion maps three maps penalizer data term computed maps data term estimating occlusion maps outer iteration order unknowns note error evaluation method fixed point suggested approach image image pyramid projection level solution computed level assumption small solution equations level functional approach global relative smoothness term smoothness term pyramid solution pyramid level fixed point outer iteration data order outer small total solution images images derivatives function iteration linear system equations linear system refer additional approach optical scene flow algorithms image computed error deviation projection ground truth disparity optical evaluation error surface motion compute deviation estimated ground errors compute normalized rms error nrmsp nrmsp number nrmsv error computed motion vector scene flow angular error evaluated computing absolute angular error vector proposed evaluation errors image errors error pixel magnitude error point relative camera error reconstruction scene flow evaluation result ranking computed errors stereo algorithms middlebury ranking rms occlusions occlusions computed surface point cameras time points occluded accuracy experiments synthetic real algorithm variational method requires initial depth motion experiments motion field experiments stereo algorithm proposed initial depth map reference camera third parallel real elaborate stereo consists real rigid scene viewed three static scene viewed camera method middlebury stereo data consists images images considered cameras time camera motion motion motion generally scene flow algorithm proposed huguet compute absolute angular error optical flow normalized error optical flow disparity fields time better optical flow disparity time error optical flow error disparity figure figure ground truth depth motion figure computed views views leads better synthetic tested method challenging synthetic scene viewed calibrated sequence consists rotating sphere rotating plane sphere plane sphere occlusions large discontinuities motion depth accuracy ground truth depth evaluated computing nrmsp nrmsv errors table computed errors three regions regions corresponding discontinuities nrmsp nrmsv views views views views views views discontinuities occlusions pixels table evaluated errors computed scene flow structure three regions table evaluated errors ground projection scene flow structure huguet normalized rms error optical flow disparity time disparity time absolute angular error corresponding optical real set experiments sequences moving sequences three cameras cameras calibrated location cameras fixed sequences image proposed variational approach simultaneously estimating scene flow structure multi-view novel point cloud directly model desired allows smoothness assumptions imposed directly scene flow desired unknowns spatio-temporal brightness constancy energy functional smoothness assumptions consistency multiple views constraints representation variational framework challenging optimization image unknowns non-linear optical flow associated euler-lagrange equations involves multiple views requires occlusions view occluded occlusion views variational time figure reference view time depth map masked computed occlusion magnitude computed scene flow time corresponding warped time projection computed scene occluded pixels colored depth parallel reference three dataset involves rigid motion small object static second dataset depth object moving piecewise third consists rotating motion generally involves non-rigid three large occlusions dataset magnitude estimated projection scene flow reference view motion pixels occluded images colored note errors computed occluded regions depth estimated depth masked occlusion order images warped reference method recover scene flow warped images reference figure reference view time view time warped images regions computed magnitude scene flow depth map masked computed occlusion projection computed scene occluded pixels colored figure three views time corresponding views time warped image warped image regions computed magnitude scene flow depth map masked computed occlusion projection computed scene occluded pixels colored multiple views functional despite accurate dense real synthetic data errors depth discontinuities occluded errors consists fields view camera considered method better multi-view scene flow view variational accuracy optical flow estimation volume multi-view scene non-rigid shape dense accurate spatio-temporal multi-view furukawa dense motion synchronized huguet variational method scene flow estimation stereo dense motion disparity estimation scene flow stereo volume neumann spatio-temporal stereo multi-view stereo reconstruction scene flow estimation global dense depth map minimization regularization approach scharstein middlebury stereo scharstein evaluation dense stereo scharstein stereo depth maps volume scene scene shape motion volume dense scene flow dense stereo global stereo reconstruction second order smoothness zhang scene flow structure recovery volume zhang scene flow structure
int comput vis multi-view scene flow view variational approach 2012 2012 2012 novel method recovering structure scene flow calibrated multi-view propose point cloud parametrization structure scene flow allows directly desired global energy functional proposed sequences simultaneously recover depth scene functional multi-view consistency brightness constancy piecewise smoothness assumptions directly challenges large work representation variational framework directly multi-view formulation allows unknowns time optical flow proposed method nonlinear mapping additional challenges optimization experiments real synthetic data demonstrate proposed method recovers structure scene flow despite nonconvex optimization structure scene flow multiple view structure motion dynamic motion maps dynamic scene field problem scene flow scene flow defined dense motion field scene directly surface recovery scene flow simultaneously compute structure scene flow system consists calibrated synchronized cameras fields variational framework proposed sequences simultaneously recover depth scene elaborate parametrization spatial temporal set global energy functional variational framework solving methods scene flow surface estimation problem zhang kambhamettu vedula huguet devernay wedel compute disparity projection desired optical projection motion relation scene flow projection presented assuming stereo optical flow scene work computer int comput vis point projected pixels cameras location projected motion optical relation corresponding points views time flow disparity flow structure directly shape stereo time scene flow optical flow propose point cloud parametrization structure respect reference view pixel reference depth motion computed parametrization reconstruction parametrization allows assumptions imposed unknowns constant motion field scene project discontinuous field smoothness assumptions hold parametrization parametrization allows multiple changing number unknowns number contrast parameterization additional view requires additional set parameters disparity optical flow spatio-temporal simultaneously recovering scene flow approach contrast previous scene flow structure vedula zhang kambhamettu wedel scene flow structure spatio-temporal vedula optical flow field computed camera consistency flow limitations wedel consistency stereo motion disparity map computed previous recovery scene flow structure overcome limitations vedula huguet devernay neumann aloimonos limitations limited method simultaneously multi-view representation improve extend methods multi-view representation parametrization spatio-temporal set sequences global energy energy functional multi-view geometry brightness constancy assumption regularization imposed assuming piecewise smoothness directly motion avoid data term constraints large discontinuities motion depth nonquadratic cost approach optical flow variational approach brox method extend multiple views minimization nonconvex functional solving associated euler-lagrange multi-resolution approach tested method challenging real synthetic ground truth propose evaluation scene flow errors errors stereo optical flow algorithms correlate ranking stereo algorithms szeliski change errors novel formulation accurate global energy functional describes desired assumptions structure scene functional int comput vis challenges large representation variational framework leads better problem directly multi-view demonstrated recover structure scene flow despite challenging nonconvex optimization studies describes evaluation work point cloud representation considered scene flow recovery view vedula mesh representations ponce courchay neumann aloimonos dynamic contrast representations provide scene considered limited representation moving large number cameras required order discretization independent resolution studies devernay wedel huguet devernay proposed simultaneously compute optical flow field disparity maps successive time wedel disparity time step extend variational framework wedel solving scene flow structure studies regularization imposed disparity optical flow assumptions directly methods multiple multi-view energy minimization framework presented zhang kambhamettu stereo algorithm method imposed optical flow stereo constraints discontinuities image view additional set parallel camera multi-view method suggested variational formulation error shape motion middlebury stereo scene constant image points camera horizontal component projected scene optical depends point location shape motion multi-view methods representations simultaneously surface neumann aloimonos object optimizing method applied ponce initial mesh assuming rigid motion courchay shape shape motion recovered optimizing assumption smoothness surface methods courchay ponce limited fixed mesh method simultaneously surface scene scene flow static cameras calibrated successive frames assume brightness constancy spatial temporal energy functional minimize variational framework solving associated system parameters consider set calibrated synchronized sequence camera projection matrix camera projection surface point int comput vis image sequence time matrix third displacement vector point represent location point displacement denoted projection image time denoted assume points coordinate system reference coordinates functions contrast previous methods huguet devernay wedel additional cameras require additional unknowns optical flow disparity representation require image energy functional total energy functional minimize edata scaled focal reference image directly surface scene flow respect time surface scene flow data term edata result recovering surface scene flow minimization edata regularization image regularization occluded pixels relative impact terms regularization elaborate data term data term brightness constancy assumption spatial temporal intensity projection images displacement parametrization solution geometry scene camera constraints brightness constancy assumption cameras time data term integrating three penalizers reference image bcm deviation brightness constancy assumption bcs1 bcs2 deviation brightness constancy assumption reference view views time penalizers pixel defined note surface point projected pixel time projected image pixel fixed time temporal image point reference time three three determined functions unknowns image point recover functions number unknowns independent number multi-view system changing bcm bcs1 bcs2 int comput vis model total generally field function discontinuities scene flow pixels camera pixels gradient pixel generally gradient corresponding chosen cost function occluded pixels expected brightness constancy assumption nonquadratic cost function brox outliers outliers pixels model linear approximations large frames parameterization method leads nonlinear mappings coordinate reference image coordinate extend mappings appendix parameterization optical flow mappings coordinates coordinates optical flow disparity fields mapping parameterization chosen scene assumption hold pixels considering overcome previous studies optical flow brox scene flow huguet devernay imposed additional gradient constancy assumption order deviation gradient assumption hold spatial additional gradient constancy method deviation assumption multiple data term integrating deviation assumption view relative impact total deviation deviation motion shape smoothness note order future work smoothness second order smoothness depends depth account depth values pixel total energy function integrating pixels reference bcm data bcs1 bcs2 motion surface optimization functions minimize functional leads set partial equations euler-lagrange associated euler-lagrange equations generally euler-lagrange equations consider points projected points sequences difference corresponding pixels time smoothness term assume motion field surface changing piecewise reference int comput vis denote image terms equations respect result div div neumann image euler-lagrange equations respect functional respect involves computing derivatives images desired data pixels nonlinear functions unknowns perspective computation image derivatives respect requires nontrivial parametrization functional represent desired model approximations minimization data terms nonquadratic penalizers nonlinear system unknown functions problem multiple result nonconvex discretization equations additional perspective projection nonlinear unknowns cope challenges multi-resolution warping method nested fixed point iterations suggested brox multi-resolution approach input image image pyramid original projection level intrinsic parameters unknowns fixed pyramid level optical flow disparity scaled pyramid solution computed level assumption small changes solution equations taylor functional approach global avoid resolution relative impact smoothness term smoothness term respect pyramid required resolution level depends initial depth flow resolution small solution pyramid level nested fixed point iterations responsible removing nonlinearity outer iteration responsible linearizion expressions order taylor outer small increments total solution images image derivatives appendix responsible removing nonlinearity function iteration final linear system equations expressions fixed appendix final linear system applying successive method occlusions scene points viewed reference camera time occluded time method correspondence pixels images projection point point occluded computed correspondence brightness constancy assumption multi-view system method point occluded occluded pixels overcome associated component occluded pixels algorithm occlusion occluded map reference image points reference point projection point int comput vis reference pixels reference image pixel occluded pixel determined associated points projection camera algorithm algorithm occlusion maps computed maps outer iteration order include increments unknowns note error evaluation scene flow error defined deviation estimated corresponding ground truth errors rms error nrmsp defined nrmsp data computing view three occlusion map data term computed maps data term multiple scene point viewed reference camera expected point reference solution determined smoothness consider occluded pixels approach directly consider minimization problem computing occluded pixels optical optimization include set pixels brightness constancy assumption methods account scene scene geometry camera parameters occluded pixels occluded pixels additional set computed shape motion iteration surface point cameras time approach huguet devernay wedel applied directly computing occlusion maps computed reference map pixel camera time nonoccluded number note independent nrmsv error computed motion vector scene flow angular error evaluated computing absolute angular error vector optical scene flow algorithms image computed error deviation projection values ground truth disparity optical proposed evaluation errors image well correlate errors error pixel depends magnitude error error sign demonstrates sign error errors depend depth well location int comput vis illustration difference disparity errors sign green ground truth depth blue represent projection camera absolute values disparity error comparison ranking order computed average rms error algorithm demonstrates changes ranking account error evaluation reconstruction scene flow evaluation result ranking evaluated top ranked stereo algorithms middlebury stereo evaluation ranking rms require error errors computed three middlebury stereo teddy venus szeliski three nonoccluded discontinuous intrinsic parameters camera set figure average rms error three three average rms error demonstrate changes ranking rms second third ranked algorithm rms ranked improve motion initial depth pyramid optimizing full optimization flow depth presented directly motion input images stereo algorithm proposed third avoid input initialization parallel initialization real depth scene initialization improve time method order magnitude huguet devernay consider time chosen stereo algorithm parallel method left future elaborate stereo consists real rigid scene viewed three regarded static scene viewed method middlebury stereo teddy venus szeliski data huguet devernay algorithm variational method requires initial depth motion stereo algorithm initial depth experiments motion field int comput vis table evaluated errors ground projection scene flow structure compared huguet devernay rms error optical flow disparity time disparity time absolute angular error corresponding optical rms views views huguet devernay 2007 teddy views views huguet devernay 2007 venus views views huguet devernay 2007 dataset consists images images considered cameras time camera motion motion constant motion generally algorithm method requires full set projection matrix camera relative cameras reference parameters set horizontal respect reference intrinsic parameters computed scaled focal determined image note parameters comparison scene flow algorithm proposed huguet devernay project compute absolute angular error optical flow error optical flow disparity fields time time table better optical flow disparity time disparity time rms error optical flow rms error disparity views views leads better synthetic data multi-view rotating sphere tested method challenging synthetic scene viewed calibrated sequence consists rotating sphere rotating plane sphere plane sphere occlusions large discontinuities motion depth computed depth motion demonstrated ground evaluated computing nrmsp nrmsv errors table computed errors three nonoccluded continuous regions removing regions corresponding discontinuities analysis discontinuous orthographic rotating sphere tested method rotating sphere dataset huguet devernay scene rotating input images orthographic cameras method perspective camera input images parallel cameras perspective parameters cameras chosen focal interpretation scene longitudinal initial disparity set occluded depth set figure recovered depth compared ground truth object longitudinal recovered depth regions large depth change large depth int comput vis illustration rotation sphere rotating green plane blue reference view rotation top figure left ground truth depth motion figure computed method table multi-view rotating evaluated errors computed scene flow structure three continuous nonoccluded pixels nrmsp discontinuities occlusions pixels nrmsv perspective interpretation correlate regions comparison scene flow algorithms proposed huguet devernay wedel project compute optical flow disparity despite recovered structure scene depend projection projection ground truth values orthographic note errors computed optical flow error error comparison disparity optical flow compared ground truth values errors errors large changes depth outliers error compute absolute angular error optical flow rms disparity optical optical flow change disparity errors computed sphere nonoccluded computed errors table initial disparity map computed method demonstrates method full int comput vis illustration scene motion top reference view top figure left ground truth disparity time disparity time optical horizontal figure computed method blue ground truth shape longitudinal disparity green recovered depth note perspective projection interpretation object shape table evaluated errors ground projection scene flow structure compared huguet devernay wedel algorithm wedel rms error optical flow optical flow disparity absolute angular error optical flow occluded regions rmsof rmsof occluded regions rmsof rmsof huguet devernay wedel method methods method cope larger number method improve additional views real data set experiments sequences moving sequences three cameras cameras int comput vis calibrated location cameras fixed sequences image depth parallel reference three dataset involves rigid motion small object static second dataset larger depth object moving piecewise rotation third consists rotating motion generally motion three large occlusions dataset magnitude estimated projection scene flow reference motion pixels occluded images red note errors computed occluded regions depth estimated depth masked occlusion order images warped reference method recovers scene flow warped images reference proposed variational approach simultaneously scene flow structure multi-view novel point cloud directly model desired allows smoothness assumptions imposed directly scene flow desired unknowns spatio-temporal brightness constancy energy functional smoothness brightness constancy assumptions consistency reference view time depth map masked computed occlusion magnitude computed scene flow time corresponding warped time projection computed scene occluded pixels colored red three views time corresponding views time warped image warped image regions computed magnitude scene flow depth map masked computed occlusion projection computed scene occluded pixels colored red int comput vis three views time corresponding views time warped image warped image regions computed magnitude scene flow depth map masked computed occlusion projection computed scene occluded pixels colored red multiple views constraints improve representation variational framework challenging nonconvex optimization relation image coordinates unknowns nonlinear optical flow associated equations involves nontrivial multiple views requires occlusions view occluded occlusion views variational time multiple views recovers structure scene flow despite accurate dense real synthetic data demonstrate challenges future include larger regions data term provide occluded regions expected consists larger fields view cameras considered views provide partial demonstrated errors depth discontinuities occluded method directly cope smoothness terms occlusion appendix mapping images parameterization presented framework nonlinear minimization proposed functional nontrivial mapping coordinates reference camera coordinate pixel reference corresponding camera maps point proj mapping corresponding component depend reference camera int comput vis projection matrix camera denote time step maps denoted proj component iit iit iit iit defined derivatives directly computed compute iit respect iit warping appendix nonlinear mapping plane reference warping iit reference image estimated values iit reference image required computed warped image warped appendix image derivatives respect unknowns step numerical solution euler-lagrange equations requires computing derivatives intensity functions respect final expressions nonlinear relation unknowns image plane considered appendix appendix analysis continuous frames well unknowns regarded continuous equations approximations time intensity functions iit iit elaborate computation derivatives iit iit respect denoted iit iit iit derivatives respect iit regarded function reference image corresponding considering intensity function mapping defined appendix iit warped image gradient original image iit jacobian matrix change original image derivatives iit jacobian computing derivatives respect involves derivatives jacobian involves derivatives respect reference camera appendix linearizion appendix describes linearizion euler-lagrange equations numerical approximations pyramid linear system equations small increments total level considering equations data nonquadratic cost function iit regarded function iit considering applied computing partial iit iit int comput vis numerical approach suggested brox nested fixed point iterations pyramid level outer iteration responsible removing nonlinearity nonlinear data fixed point iteration outer solution iteration previous solution unknown step linearizion nonlinear order denote fixed values associated euler-lagrange equations respect unknown increments duk div expressions order taylor expressions div duk duk computed order taylor proj proj equations duk equations applying approximations euler-lagrange equations nonlinear system equations unknowns nonlinearity additional fixed point iterations expressions discretization linear system equations solution applying successive occlusion motion estimation multi-view scene flow view variational ieee vision variational stereo vision discontinuities occlusion vision optical flow estimation european vision computation image derivatives respect unknowns appendix int comput vis multi-view scene shape international journal computer dense accurate spatio-temporal multi-view computer vision international journal computer dense motion synchronized ieee vision variational method scene flow estimation stereo vision dense motion disparity estimation computer vision scene flow stereo computer vision image international spatio-temporal stereo multi-resolution international journal computer formulation continuous european vision multi-view stereo reconstruction scene flow estimation global international journal computer dense depth map minimization regularization approach european vision middlebury stereo vision evaluation dense stereo correspondence international journal computer stereo depth maps ieee vision dense multiple vision scene vision scene ieee transactions analysis shape motion ieee vision scene flow computation motion international journal computer dense scene flow dense stereo european vision global stereo reconstruction smoothness ieee transactions analysis methods solving partial difference equations transactions scene flow structure recovery image ieee vision scene flow structure ieee vision
bounding segmentations object recognition scene flow estimation autonomous computer vision autonomous vision computer vision geometry existing methods scene flow estimation fail presence large displacement local reflective dynamic motion estimation exploiting investigate recognition coarse bounding box estimates instance segmentations fine-grained object compute cues dataset stereo images integrate crf-based model robust scene flow estimation approach term instance scene analyze recognition cue ablation study instance segmentation cue method challenging kitti 2015 scene flow benchmark achieve state-of-the-art performance time bounding box instance segmentation object coordinates figure consecutive frames kitti 2015 scene flow large displacements challenging current scene flow estimation recognition provide geometric cues bounding instance segmentations object 2015 scene rigidly moving existing methods local data term state-of-the-art methods fail presence large displacements local ambiguities reflective consider image kitti 2015 scene flow dataset large displacement frame motion estimation problem computer vision autonomous car predict scene flow estimation autonomous consecutive stereo images predict motion challenging kitti kitti joint second reflective car surface matching recognition investigate semantic fine-grained geometric recognition task illustrated formulate output recognition task local appearance crf-based semantic consider bounding box semantic segmentation semantic output detailed mask scene flow task segmentation cues provide additional geometric rigid body pixels single rigid object integrate cues scene flow task views time pixel instance region box frame mapped instance pixels instance rigid bounding box segmentation existing state-of-the-art approach multi-task network contrast achieve dense depth map additional input train stereo images kitti 2015 benchmark fine-grained geometric recognition train convolutional network instance segmentations mnc predicts continuous object object coordinates pixel inside instance object coordinates location surface respect local coordinate frame provide detailed geometric cue matching pixels presence large fine-grained geometric recognition semantic prediction recognition task continuous object geometric segmentation mask bounding continuous object parts challenging predict segmentation mask bounding work level recognition scene flow local geometric crf-based based instance scene recognition cues scene flow challenging kitti 2015 benchmark detailed ablation study levels recognition bounding segmentation continuous object instance segmentation cue instance scene flow method scene flow challenging foreground test errors methods time scene flow geometric cues achieve state-of-the-art performance challenging kitti 2015 scene flow time detailed ablation study recognition coarse bounding boxes instance segmentations fine-grained object scene flow annotations stereo images kitti 2015 benchmark work works scene flow semantic object rgb-d methods refer scene work formulate scene flow estimation variational optimization problem variational optimization displacements scene rigidly moving exploiting optimization methods performance challenging kitti benchmark fail correspondences reflective regions ambiguities data term approach problem fine-grained instance recognition geometry accuracy semantic works semantic stereo optical flow model stereo estimation object model estimated disparity proposed model joint optical flow semantic scene dynamic regions constraint measures well superpixel epipolar semantic segmentation object instances flow predictions discreteflow proposed model car instances predicts rigidly moving scene epipolar flow existing methods leverage recognition motion paper consider recognition scene flow contrast methods correspondences epipolar method semantic cues well geometry estimate rigid motion frames robustly leads compared investigate impact recognition granularity coarse boxes fine grained object coordinates object continuous object object pose estimation camera tracking approach train random forest object object coordinates respect local coordinate predictions fit model object object coordinates continuous labeling surface points train cnn predict object coordinates car integrate bounding instance object coordinates cues analyze cue scene flow estimation analyze impact levels recognition describe inputs method scene flow estimation challenging large motion predictions integrated model scene flow bounding boxes instances objects state-of-the-art multi-task network proposed bounding boxes segmentation masks cars mnc rgb provide network xyz denotes scene location pixel scene camera location pixel computed disparity network coarse annotations kitti images instance annotations kitti 2015 stereo benchmark model fine accuracy validation set compared rgb object coordinates object coordinates location surface object respect local coordinate fine grained geometric labeling surface correspondences robust appearance correspondences based sparse feature random estimating object coordinates target instance approach leads better object coordinates predictions cnn proposed estimating object coordinates input cnn image method describes approach scene flow work flow approach illustrated rgb images points pixel camera coordinate stereo method based rgb xyz train network predict bounding boxes instance rgb input images dense xyz scene coordinates instance boxes coordinates instance scene flow figure work flow note input rgb input images compute points instance bounding instances object coordinates object coordinates integrated instance scene flow method well instance prediction output cnn regression coordinates refer architecture cnn supplementary set convolutional set regression predicts object region reference image describes plane object superpixel associated denotes objects background object object associated describes rigid body motion superpixel associated object rigid motion parameters plane parameters scene flow pixel inside energy left input images consecutive frames geometry superpixel reference association objects rigid body motion object formulate scene flow estimation task energy problem smoothness instance data scene flow model describe scene flow model based instance appearance term term object coordinates paper describe inputs scene flow model described denote set views illustrated denote input image corresponding view compute mnc predicts instance label maps predicted semantic instance label pixel maps coarse bounding box segmentations accurate instance segmentations computed background pixels label foreground pixels note instance frames scene denote object coordinates predicted network describe parameters model denote set superpixels reference view denote set objects superpixel instance variables optimization weight parameters data data term corresponding points images denotes set target views location pixel reference view mapped view presence instance motion weight defined figure geometric reference target pixels reference view mapped pixels target view depth rigid body target view rigid body motion corresponding object plane parameters associated superpixel data appearance pixel reference image appearance pixel target view robust optimization local robust loss loss measures respect sparse discreteflow correspondences flow terms depth estimates stereo term smoothness smoothness term adjacent superpixels terms motion instance instance term measures appearance object coordinates detected instances appearance labeling defined geometry motion denotes disparity plane pixel reference set pixels superpixel superpixel robust weight defined denotes set adjacent superpixel superpixels object adjacent superpixels note measures appearance object coordinates image location reference view target data term work well textured background presence additional constraint appearance instances leads better estimates works direct measures estimate camera pose textured contrast constraint estimate pose textured object term instances reference frame target frame estimated geometry appearance well labeling object coordinates term evaluated reference target pixel detected instance algorithm optimization initialize described samples samples trw-s problem terms model instances background regions additional compared instances additional term association leverage trw-s proposed optimization algorithm variables samples current trw-s approach local energy associated appearance foreground sampling algorithm leads better algorithm continuous variables rigid body sampling current map samples based appearance term object sampling energy pixels inside instance based predicted depth rigid body motion measures reference target views sampling trw-s samples compared estimates current map sampling geometry variables parameters model estimated coordinate optimization problem parameters describe initialize geometry parameters dense disparity maps object variables sparse flow predictions predicted bounding robustly fit superpixel parameters disparity estimates sparse flow estimates instance robustly fit rigid body motion refer input sparse flow estimates based instance masks robustly fit rigid body motion high quality instance leads robust leverage recognition initialize objects based sparse scene flow estimates background based initialize robust refer reader color pose prediction instances sparse flow dense geometry additional term integrated challenging association kitti 2015 scene flow average optimization described algorithm leads scene single inputs methods dense disparity sparse flow predictions predicted bounding average leading evaluation recognition granularity study impact levels recognition granularity estimating scene flow dynamic recognition sparse optical flow sparse discreteflow correspondences dense disparity maps superpixel table quantitative comparison performance osf recognition isf-bbox bounding boxes recognition isf-segmask instance segmentations recognition isf-segmask-objcoord instance segmentations object coordinates recognition validation set kitti 2015 scene flow report error respect second frame optical flow osf isf-bbox isf-segmask isf-segmask-objcoord isf-segmask-cnndisp table quantitative ablation study kitti 2015 validation report disparity flow scene flow error validation set osf recognition isf-bbox box isf-segmask isf-segmask-objcoord object coordinates report isf-segmask-cnndisp quality cnn based disparity scene flow recognition bounding box instance large optical flow estimation scene flow foreground parts note recognition background objects paper estimated well note instance segmentations input performance attribute fine grained segmentation input compared bounding boxes kitti 2015 benchmark cars scene dynamic cars defined scene flow estimation model car instances detected cnn figure qualitative comparison scene flow errors methods levels recognition reader supplementary material additional object coordinates performance instance segmentations weight object coordinate terms attribute quality state-of-the-art object coordinate accuracy object coordinate predictions state-of-the-art methods scene flow estimation high level accuracy current pixels error refer reader second supplementary material detailed analysis object coordinate predictions provide cnn based disparity evaluated method level recognition granularity based ablation study quality disparity inputs computed instance pixels validation report top method isf-segmask-cnndisp validation set table kitti benchmark top method isf-segmask-cnndisp evaluated kitti 2015 scene flow evaluation table performance method leading methods prsm osf osf-tc state-of-the-art performance time method methods temporal frames prsm figure scene flow errors leading methods fail estimate scene flow foreground scene flow estimation challenging region car large displacement second method recognition cues better recognition regions reader supplementary material additional qualitative method state-of-the-art object coordinates prediction model network object coordinate prediction network robust loss learning performance cnn model existing state-of-the-art object train random forest depth rgb image depth map input feature random points quality object coordinate predictions better cnn architecture average osf isf-segmask-cnndisp table quantitative kitti 2015 scene flow evaluation table disparity flow scene flow errors test images method comparison leading methods osf-tc prsm kitti methods temporal figure qualitative ablation study kitti 2015 validation top predicted masks bounding boxes input method left camera image time figure left scene flow error osf recognition isf-bbox box isf-segmask color box figure qualitative comparison test input scene flow error maps osf osf-tc prsm method color comparison error random forest attribute error cnn based predictions well cars high refer reader supplementary material detailed cnn architecture comparison quality object coordinate predictions state-of-the-art flow dynamic foreground recognition cues bounding box instance segmentation provide large foreground parts scene presence large displacement local method state-of-the-art performance challenging kitti scene flow european european impact levels recognition granularity problem estimating scene exploiting semantic matching optical european computer vision multi-view scene flow view variational international journal computer vision learning object pose estimation object european computer vision pose estimation objects single rgb ieee computer vision pattern recognition image labeling ieee computer vision pattern recognition semantic segmentation multi-task network ieee computer vision pattern recognition direct sparse direct european computer vision stereo motion robust pattern recognition autonomous kitti vision benchmark ieee computer vision pattern recognition ieee international computer vision stereo ambiguities object ieee computer vision pattern recognition rgb-d dense motion estimation color ieee international scene flow rgb-d ieee computer vision pattern recognition variational method scene flow estimation stereo ieee international computer vision joint optical flow semantic european computer vision convolutional architecture feature international energy ieee pattern analysis machine model based tracking object coordinate computer vision continuous optimization approach efficient accurate scene european computer vision large dataset train convolutional optical scene flow object scene flow autonomous ieee computer vision pattern recognition optimization optical pattern recognition joint estimation scene image analysis pose estimation instances object coordinate machine vision object scene flow temporal computer vision dense tracking ieee international computer vision multi-view stereo scene flow estimation matching international journal computer vision dense scene flow estimation rgb-d european computer vision optical flow semantic segmentation ieee computer vision pattern recognition model foreground background ieee computer vision pattern recognition multi-view single images convolutional european computer vision dense correspondences pose ieee computer vision pattern recognition exploiting regression accurate camera ieee computer vision pattern recognition joint estimation geometry stereo european computer vision scene ieee computer vision pattern recognition scene ieee pattern analysis machine scene flow estimation european computer vision scene flow estimation rigid motion ieee international computer vision rigid scene ieee international computer vision scene flow estimation rigid scene international journal computer vision scene flow motion international journal computer vision efficient dense scene flow sparse dense stereo european computer vision robust epipolar flow ieee computer vision pattern recognition efficient joint stereo flow european computer vision local european computer vision stereo matching convolutional network image journal machine learning
basis constrained scene flow dynamic proxy existing scene flow approaches reconstruct representation scene variational formulation scene flow relative coarse proxy better linear basis represent temporal surface allowing temporal fewer formulation proxy motion proxy human trajectory reconstruction single view appropriate proxy framework generalizes existing approaches scene illustrate static moving proxy formulation basis constrained scene variational formulation scene flow proxy underlying proxy surface motion coarse formulation views directly structure flow image linear basis scene flow formulation allows time fewer basis geometric reconstruction proxy surface appropriate proxy basis formulation generalizes optic flow proxy surface image basis-constrained optical stereo flow frame scene static moving linear skinned image-based recovery dense scene motion scene dynamic reconstruction process temporal improve structure reconstructions existing approaches consider motion frames utilize coarse scene structure coarse scene motion overcome scene flow estimation problem directly proxy approach generalizes image-based methods formulation proxy image formulation proxy coarse tracking human allowing reconstruction process exploit observations motion longer range formulate recovery scene motion terms temporal basis optic flow low dimensional representation fewer temporal proxy depth reconstructions stereo scene image-based multi-view depth set dense methods proxy advantage proxy surface regularization surface proxy image capture methods garment capture scene flow dense stereo data track dense geometry frames structure motion methods utilize number textured temporal reconstructing surface temporal basis reconstruct shape methods exploit temporal smoothness static scene flow better geometric reconstructions correspondences temporal temporal constraints improve set stereo temporal regularization case time subspace constraints optic flow rigid subspace constraints flow matrix multi-frame flow rigid scene moving rank flow matrix helps overcome non-rigid structure motion represent deforming surface linear deforming shape image observations deforming surface low rank representing surface low rank motion basis point motion basis temporal motion points coefficients shape basis reconstruct dense per-pixel surface motion variational formulation per-pixel shape coefficients regularization long per-pixel basis functions optic flow estimation variational approach variational scene flow structure functions data smoothness simple temporal constraints linear motion constant cosine basis constrain reconstructing trajectories observed moving previous constant velocity basis dense constraints utilize basis constraints moving allows representation exploit ability structure reconstruction overlap camera basis constrain temporal point motion coarse scene motion case correspondences temporal basis represent surface ability overcome reconstructions missing consider simple single moving camera camera motion dynamic correspondences moving formulate estimation process unknown moving constraints motion points time trajectory constant fewer variables representation allows reconstruction camera motion constant velocity temporal basis time-varying temporal basis functions framework correspondences uit coarse rigid motion scene uit basis-constrained motion basis constrained scene flow example illustrate cameras point reconstruction longer basis represent temporal smoothness appropriate basis helps overcome fewer reconstruction missing consider multi-view coarse geometry motion bone displacement residual displacement residual displacement residual reconstruction residual reconstruction error reconstruction error missing missing missing number basis elements number basis elements number basis elements figure images camera displacement residual displacements varying basis image-based reconstruction data better basis basis elements reconstruction missing data motion displacement residual image-based image-based missing data surface residual non-rigid motion skeleton point coarse motion corresponding bone motion points linear displacement trajectory scene flow proxy previous temporal basis recover flow coarse scene motion correspondences coarse motion motion moving approximate proxy low dimensional basis directly scene flow scene structure relative proxy formulate problem proxy surface formulate scene flow estimation relative proxy surface uit problem input cameras time reconstruct dense structure surface surface flow approximate camera proxy motion tracking illustrate ability low dimensional basis approximate reconstruct non-rigid trajectories human motion deformed surface displacements data set displacements relative bone frames discrete cosine figure number basis basis functions surface motion parameters unconstrained reconstructing displacements views image observations fewer variables basis better variables lower dimensional basis helps ability reconstruct missing data figure observations camera unconstrained motion reconstruction motion lower dimensional proxy surface proxy surface discrete time surface normal surface tangent frame matrix defined proxy time proxy time figure represent surface relative moving proxy surface observed reference frame surface displacement proxy surface represented displaced point flow displacements flow scene motion modeled relative moving scene proxy time temporal displacement normal reference temporal normal tangent frame offsets represented coordinate frame surface figure surface motion represented coordinate previous recover surface long range directly image displacements flow displacement flow recovered low stereo parameters term wijt wit1 image observations stereo stereo displaced surface flow optic points reference frame regularization displacements varying flow time temporal basis-constrained flow shape displacement surface offsets time allowing displacements number variables term temporal motion modeled coefficients varying scalar wit1 displacement displacement basis functions displacement reference terms wijt ijt set basis constant velocity basis time reference number unknown scalar basis elements flow motion basis functions set utilize discrete cosine constrain trajectories moving points observed moving cameras proxy motion reconstruct flow trajectory surface advantage framework generalizes number variational approaches dense optic scene corresponding proxy basis functions number euler-lagrange equations scalar euler-lagrange problem optic flow optic flow basis stereo basis depth displacement proxy scene flow scene flow proxy time proxy image plane image plane image plane image plane basis-constrained scene flow proxy generalizes number optic simple scene flow proxy image basis functions reconstruction offsets optic depth multi-frame scene flow top dynamic example basis cosine cosine wijt iijt implementation skinned proxy mesh tangent space defined optimization wit1 iit1 skinned mesh skinned mesh set skeleton set triangles relative bone set representing deformed vertex wijt iijt wit1 iit1 iijt iit1 terms iijt vertex bone motion proxy mesh temporal sequence tangent space iijt triangle corresponding set texture linear point triangle tangent space triangles surface normal vertex triangle normal tangent triangle triangle iit1 euler-lagrange equations solved methods iit1 implementation skinned proxy mesh formulation previous moving proxy figure input recovered displaced textured displaced figure input images camera synthetic skinned illustrate motion texture case vertex triangles optimization proxy mesh set stereo points manually case approximate proxy motion relative proxy skeleton track surface skeleton image-based image wijt set wijt camera flow wit1 defined euler-lagrange equations solved multiple resolutions tangent space input lower resolutions resolutions simple image figure top camera top geometry reconstruction reconstruction constant velocity illustrate implementation synthetic data static depth figure framework depth static views time recover depth simple manually proxy temporal basis skinned camera overlap synthetic data set illustrate basis constraints case motion overlap camera motion surface basis improve surface sequence views time motion input camera motion skeleton underlying motion frames surface motion constant velocity basis unconstrained constant velocity basis allows reconstruction depth unconstrained basis recover depth error time unconstrained constrained reconstructions surface static multiple views illustrate proxy motion data static stereo deforming geometry triangles manually cosine basis basis elements flow model motion motion modeled geometric surface rigid moving single view illustrate motion proxy single view previous flow multi-view case rigid motion time-varying depth texture cosine basis elements coordinate model motion geometry recover variational approach scene flow reconstruction directly image temporal basis longer range formulation advantage motion proxy monocular proxy representation representing structure proxy appropriate scene underlying unknown scene depth constraints surface flow relative surface motion data term data term number basis functions long range data terms implementation consider tracking flow single tracking advantage motion parameters flow motion basis set point flow figure input frames sequence static textured reconstructions trajectories surface texture figure texture reconstructions time-varying depth reconstruction frames figure input images time deformed textured euler-lagrange surface recovered sequence cosine basis functions model displacement normal figure time-varying depth recovered skinned proxy example moving skinned proxy observed multiple proxy moving views track geometry shape skeleton optic flow euler-lagrange equations solved point wijt ijt wit1 it1 wijt iijt wit1 data terms iijt iijt terms iit1 defined data equations wijt ijt wit1 it1 wijt ijt it1 wit1 optimization framework optic flow trajectory reconstruction moving points monocular image acm multi-view scene flow view variational cvpr monocular depth scene flow constant garment acm non-rigid shape image cvpr motion iccv dense motion capture dense multi-frame optic flow non-rigid subspace framework texture iccv variational scene flow estimation stereo multi-frame optical flow estimation subspace iccv geometry reconstruction multiple variational optical reconstruction moving point dynamic multi-view image cvpr garment motion capture estimation structure geometry stereo scene shape motion cvpr mesh multi-view acm reconstructing shape recovery dynamic cvpr
scene flow estimation growing correspondence seeds inria simple seed growing algorithm scene flow stereo cameras scene output sequence image algorithm computes disparity map image pairs optical flow maps consecutive representation scene proposed method correspondence seeds correspondences accurate large produces stereo disparity optical flow algorithm fast search space comparison methods spatiotemporal stereo variational optical scene flow disparity horizontal optical flow vertical optical flow sequence image pairs cameras estimate motion image temporal estimate disparity estimate motion spatiotemporal methods estimate scene flow stereo optical flow correspondence scene flow dense motion variational methods well simple discrete mrf discrete local methods correspondences efficient seed growing algorithm estimate scene flow seed growing methods correspondences small neighborhood initial set seed figure output proposed algorithm eth dataset color colors optical color colors left colors motion color unmatched stereo best scene fast performance compared global variational mrf good accuracy compared local pixel relations proposed algorithm estimate accurate disparity optical flow maps scene large motion time small local disparity flows captured growing process large displacement boundaries objects well preserved algorithm produces stereo prematcher scene flow predictor figure proposed algorithm dense rest proposed algorithm evaluation comparison figure sequence consecutive rectified stereo seed correspondence neighborhood computes disparity map disparity map seeds input images input subsequent algorithm jointly disparity map optical flow maps solution time solution time frame proposed algorithm seeds growing time motion pixels previous predictor estimates correspondence seeds seeds seeds growing prematcher frames capture dynamic scene objects process subsequent algorithm growing scene rest algorithm proposed algorithm growing correspondences scene flow sequence stereo images time instance input rectified image pair time consecutive pair time output time instance disparity map stereo correspondences frame disparity map correspondences horizontal vertical optical flow maps consecutive images notice camera representation fully scene points points prematcher initial subsequent growing prematcher correspondences points left images consecutive seed correspondence point point motion seed stereo optical flow set scene flow prematcher output stereo seeds set correspondences stereo seeds grown stereo algorithm growing scene flow algorithm input rectified image pairs pair set initial correspondence seeds disparity map previous frame parameters consistency flow output maps disparity optical flows algorithm computes consistency statistic correspondence average correlation correlation small pixels algorithm growing scene flow rectified images initial correspondence seeds disparity map parameters similarity seeds seed best similarity update output best neighbors optical flows seeds small correlation exceeds threshold pixels unmatched match step output maps step match step seeds growing process seeds step pixels matched update output update seed disparity map flow maps relations consecutive images left mncc statistics pixel seed correlation small temporal step set correlation priority seed step consistency exceeds threshold output maps neighbors seed best step instance set local search stereo temporal disparity notice previous disparity neighbors optical flow well aperture joint stereo seed correct flow correlation default values algorithm parameters set temporal consistency parameter step trade-off temporal capture fast observed temporally matches noise image small matched points better priority observed matching wrong seeds step parameter growing process aperture observed artifacts optical flow estimation growing process matches local correct solution noise small solution seeds parameter trade-off solution mncc statistic local image pixels optical camera scene motion statistic experiments algorithm well motion simple set parameters local stereo growing stereo seed growing algorithm stereo matching images growing algorithm wrong matching solution correspondence growing compare algorithm algorithm proposed priority grown correct seeds harris seeds seeds assumption complexity algorithm algorithm low algorithm correspondences complexity frame search space disparity horizontal vertical proposed algorithm complexity correspondences neighborhood seeds discrete correlation pixels prematcher prematcher correspondences matching harris points tracking version stereo seeds harris points epipolar mncc correlation exceeds threshold scene flow seeds tracking stereo seeds point matches epipolar algorithm harris matching tracking experiments experiments proposed algorithm produces accurate joint disparity optical flow sequence stereo proposed method compared spatiotemporal stereo algorithm variational scene flow algorithm optical flow experiments algorithm disparity optical flow predictor predictor estimates seeds frame solution motion simple point image optical flow matched pixel seed synthetic data compare experiment synthetic scene moving performing complicated moving small vertical bar moving fast left moving scene textured scene sequence frames images frame ground-truth optical flow maps stereo motion algorithms tested noise independent noise image stereo experiment noise noise image measured average ratio matched pixels number pixels mismatches number frames evaluation algorithms fully dense output maps notice stereo seed map seed representation grown stereo disparity map pixels matched motion motion assumption correct dynamic motion despite predictor correct assumption motion seeds wrong low correlation images ratio correct pixels disparity ratio correct pixels optical flow sizintsev-2009 huguet-2007 gcs gcsfs brox-2010 huguet-2007 gcf gcsfs ground truth ground truth ground truth error statistics evaluated scene gcsfs gcsfs gcsfs ratio correct pixels disparity sizintsev-2009 huguet-2007 gcs gcsfs ratio correct pixels brox-2010 huguet-2007 gcf gcsfs optical flow sizintsev-2009 brox-2010 brox-2010 error statistics evaluated vertical huguet-2007 huguet-2007 huguet-2007 figure synthetic disparity optical flow maps frame ground-truth maps tested compare mismatches unmatched correlation threshold synthetic parameters default values experiments statistic measured disparity optical flow optical flow evaluated average proposed algorithm pixel level accuracy evaluation optical flow pixel matching stereo epipolar capture optical flow mismatches pixel evaluation optical flow ground-truth experiment case compared proposed algorithm jointly estimates disparity optical flow seed growing algorithm computes disparity maps frame-by-frame scene flow algorithm spatiotemporal stereo gcsfs gcs low level figure algorithm accuracy noise gcsfs level algorithm performing well moving severe difficulties moving bar performance compared proposed algorithm severe difficulties disparity map gcsfs mismatches boundaries well preserved small similarity small mismatches threshold case optical compared flow proposed gcsfs algorithm seed growing algorithm frame-by-frame epipolar growing compare variational method large displacement scene flow better noise gcf gcf gcsfs small unmatched pixels motion level noise gcsfs frame-by-frame seed growing gcsfs low level noise methods better global methods optical flow maps gcsfs motion boundaries well notice motion motion motion despite optical flow maps motion boundaries good artifacts moving bar small rest algorithm error statistics evaluated error statistics vertical bar low performance compared proposed gcsfs algorithm compared frame-by-frame independent seed growing methods consequence joint disparity optical flow estimates good temporal consistency data matched seeds growing process step threshold produces mismatches grown disparity optical flow maps seeds algorithm number seeds real data proposed algorithm tested real data default values parameters proposed gcsfs dataset stereo camera dataset eth stereo tested algorithms disparity optical flow inria proposed gcsfs dense textured scene notice preserved boundaries disparity optical motion performing left moving eth dataset scene camera motion motion pixel consecutive motion cameras well captured motion boundaries small mismatches disparity complicated small mismatches optical flow consequence aperture spatiotemporal stereo disparity maps map threshold set disparity map inria objects textured artifacts large spatiotemporal matching method severe difficulties eth scene cameras fast motion captured matching well large displacement large displacement optical flow fully motion boundaries motion small mismatches variational scene flow algorithm disparity maps algorithm stereo complexity scene complicated optical flow method better stereo artifacts motion consequence images gcsfs gcsfs gcsfs sizintsev-2009 brox-2010 brox-2010 huguet-2007 huguet-2007 huguet-2007 figure real inria figure better version gcsfs sizintsev-2009 brox-2010 huguet-2007 algorithm jointly estimates disparity optical flow stereo sequence growing correspondence accurate temporally frame-by-frame independent tested ground-truth comparison methods spatiotemporal variational methods optical scene proposed algorithm well trade-off simple local methods global mrf local relations pixels matching initial seeds process correspondence average running time frame temporally case despite running time tested algorithms average running time frame tested algorithms measured synthetic sequence gcsfs algorithm tested algorithms images gcsfs gcsfs gcsfs sizintsev-2009 brox-2010 brox-2010 huguet-2007 huguet-2007 huguet-2007 figure real eth figure better version scene flow variational large displacement optical matching variational motion ieee efficient correspondence ieee efficient disparity space fast accurate joint disparity disparity flow estimation variational method scene flow estimation stereo dense motion disparity estimation matching match match ieee disparity estimation stereo scene image stereo variational scene flow estimation similarity spatiotemporal stereo matching spatiotemporal stereo spatiotemporal scene dynamic
dense accurate spatio-temporal multi-view stereovision ome novel method simultaneously accurately shape motion dynamic scene calibrated variational approach previous work stereo reconstruction scene flow adopt representation dynamic scene animated polygonal mesh fixed vertex positions representation consistent shape motion method accurately shape motion optimizing positions vertices animated optimization energy function multi-view inter-frame smoothness spatio-temporal surface velocity work image-based computed partial eectiveness method challenging dynamic spatio-temporal scene motion capture methods spatiotemporal models dynamic scenes marker-less motion desirable dense accurate shape motion accurate initial methods geometric ome courchay accurate motion estimation applications motion time motion estimation temporal consistency shape motion performance capture scene flow dense motion field accurately sparse correspondences methods motion estimation representation high computational animated meshes despite correspondences shape motion estimation desirable marker-less motion capture methods spatio-temporal consistency shape computed time motion shape motion estimated initial mesh scene silhouette stereo method temporal sparse correspondences dense scene shape motion accurate spatiotemporal spatio-temporal temporal inter-frame matching shape motion estimated simultaneously algorithm approach high computational limited smoothness shape methods desirable dense accurate shape motion shape motion time-varying estimated simultaneously optimizing multi-view inter-frame multi-resolution representation optimization motion spatio-temporal input scenes constant work dynamic despite eectiveness full depending computed novel method simultaneously accurately shape motion dynamic scene variational approach previous work stereo reconstruction scene flow estimation dense accurate spatio-temporal multi-view stereovision applications limited time-varying map scene spatiotemporal consistency adopt representation dynamic scenes animated polygonal mesh fixed time-varying vertex positions representation consistent shape motion computer computer performance capture video time-varying point video method accurately shape motion optimizing positions vertices animated optimization energy function multi-view inter-frame smoothness spatio-temporal surface velocity work image-based computed partial discrete geometric variational energy function eectiveness method challenging dynamic approach discretize optimize variational methods area computer optimize discretize energy depending spatiotemporal representation gradient energy computed evolution flow adopt discretize optimize define energy function depending discrete spatio-temporal standard optimization approach mesh computer vision discrete spatio-temporal representation animated mesh representation animated polygonal meshes meshes dierent time shape ome courchay scene time motion shape motion consistent fixed topology human constant method limited topology animated mesh constant optimization spatio-temporal delaunay deformable models variational dynamic calibrated synchronized video composed animated polygonal mesh input positions vertices animated mesh dierent time temporal animated refer animated mesh positions energy respect composed data attachment regularization term spatio-temporal surface regularization term velocity multi-view matching defined camera pairs pairs time frames dissimilarity measure image reprojection animated detailed term spatio-temporal area animated term gradient discrete velocity field animated animated mesh gradient velocity detailed term energy function standard gradient spatio-temporal positions multi-resolution optimize frames animated mesh dense accurate spatio-temporal multi-view stereovision representation facet animated input additional time frame position previous time optimize time temporal spatio-temporal mesh image mesh data attachment term gradient additional camera denoted method surface refer temporal visible image point camera animated mesh frame denoted define map points points representation figure vertex animated mesh time frame define function triangular pixel image pixel triangular facet barycentric coordinates time vertex position time vertex facet define image map positions positions animated ome courchay reprojection image time image time animated reprojection image time image time animated mesh figure data attachment term oriented camera pairs oriented pairs time frames dissimilarity measure image defined reprojection animated dissimilarity computed region image image region pixel image visible surface time point surface time visible image image visible image region computed optimization graphics partial derivative energy term respect variation position animated oriented pairs time frames variation dense accurate spatio-temporal multi-view stereovision animated mesh image partial derivative derivative measure respect denoted input points image triangular despite visible dierent full image triangular facet visible pixel barycentric geometric refer detailed numerical additional numerical barycentric coordinates velocity field regularization term velocity field animated mesh field defined constant triangular facet vertex area energy term velocity field regularization term ome courchay variation respect vertex partial energy term numerical image animated mesh gradient data attachment term image dissimilarity measure time-varying animated mesh spatio-temporal based computational geometry mesh edge applied time edge time edge time topology mesh delaunay deformable models coordinates animated mesh time frame topology human algorithm challenging multi-view video dataset composed applied algorithm frames high image multi-resolution accurate spatiotemporal reconstruction figure frames spatio-temporal figure spatio-temporal approach multi-view stereovision method approach initial time temporal approach approach simultaneously shape scene dense accurate spatio-temporal multi-view stereovision text multi-view stereovision approach spatio-temporal approach text ome courchay text dataset calibrated synchronized video applied algorithm frames multi-resolution optimization standard algorithm time reconstruction multi-resolution figure silhouette stereovision dataset based full spatio-temporal novel variational approach dense accurate shape motion reconstruction multi-view video method animated mesh image-based discrete geometric optimization algorithm challenging dense accurate spatio-temporal multi-view stereovision spatio-temporal shape silhouette delaunay ieee international conference computer human multi-view dense reconstruction ieee conference computer vision pattern performance capture sparse multi-view acm marker-less deformable mesh human shape motion ieee conference computer vision pattern dense motion capture synchronized video ieee conference computer vision pattern evolution ieee conference computer vision pattern volume spatio-temporal stereo multi-resolution international computer vision multi-view stereo reconstruction scene flow estimation image-based matching international computer vision surface capture performance based ieee computer graphics applications dierent ieee transactions computer graphics temporal surface mesh conference computer volume image-based spatio-temporal dynamic acm transactions graphics shape motion ieee conference computer vision pattern mesh multi-view acm transactions graphics scene ieee transactions pattern scene flow image ieee conference computer vision pattern volume variational method scene flow estimation stereo ieee international conference computer ome courchay variational stereovision scene flow estimation ieee international conference computer volume dense scene flow sparse dense stereo conference computer scene flow ieee conference computer vision pattern volume animated meshes point computer graphics geometry reconstruction geometry time-varying point geometry stereo reprojection triangular surface vision surface evolution applied deformable ieee conference computer vision pattern volume multi-view ieee conference computer vision pattern delaunay deformable meshes based delaunay ieee conference computer vision pattern multi-resolution international conference computer graphics dense accurate spatiotemporal multi-view
multi-camera scene flow tracking 3-d points surfels scene flow 3-d motion points optical flow 2-d motion classical methods compute scene flow optical propose compute tracking 3-d points surface elements multi-camera setup cameras methods translation 3-d point matching 2-d camera time full pose surfel recovered matching image projection texture template attached visibility changes occlusion rotation surfels methods lost points scene flow multi-camera scene flow vedula 3-d vector defined point surface 3-d motion points time optical flow simply projection scene flow camera image compute scene flow reconstruct optical flow cameras dense stereo reconstruction scene flow optical approach work scene track 3-d scene points surface elements 2-d image work carceroni scene set surfel described planar texture local surface motion 3-d affine camera parameters position recover surfel surfels 3-d motion computed 3-d method follow surfels method reconstruction scene recovered method number surfel parameters limited scene flow limited dellaert propose surfel tracking method extracting super-resolved surfel limited propose 3-d reconstruction scene flow method visibility scene point cameras 3-d 3-d scene reconstructed propose methods compute scene flow work scene scene extracting scene flow position propose approach classical 2-d good tracking detected original initial surfel pose translation texture parameters reconstructed 3-d point surfel tracked time sequence multi-camera extension lucas-kanade algorithm time frame extract translation images visible pyramidal compute full pose parameters surfel visibility proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee updated time surfel orientation respect surfel texture images set trajectories 3-d point surfel visible camera time computing dense scene flow time frame previous set 3-d trajectories scene flow time scene motion compute full 3-d trajectories motion vector trajectories scene flow drift actual scene case surfel tracked surfel drift point surface texture multi-camera extension lucas-kanade tracking tracking 3-d tracking 3-d initialization 3-d points based tracking method scene flow trajectories computation work visible energy lucas-kanade algorithm order respect expression parameters multi-camera extension lucas-kanade lucas-kanade method classical method compute optical flow track 2-d points reference formulations problem additive forward additive lucas-kanade tracked feature described vector parameters texture template texture feature parameters warp function point texture template point image method feature parameters order energy texture template approximation hessian parameters updated multi-camera extension lucas-kanade tracking 3-d points 3-d surfels tracking 3-d points cameras lucas-kanade tracked 3-d parameters vector simply coordinates order compute parameters update appearance feature camera texture templates attached warp feature proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee texture templates warps jacobian warp templates extracted set previous images projection image 3-d position previous time frame warps 2-d template coordinates image coordinates image warp translation image 2-d coordinates projection image projection perspective residuals energy residuals scale estimator coordinates projection camera projection matrix homogeneous projection function homogeneous compute jacobian warp image parameters update computed hessian expression hessian camera point visible 3-d position point recovered standard normal visibility values computing hessian parameters update 3-d point tracker robust visibility changes robust estimator case number number cameras visibility changes scene scene surface smooth surfel visibility values estimated lost points condition number smallest singular singular parameters update consider point main problem 3-d point tracker tracked point surface drift small 3-d setup cameras stereo pair baseline 3-d point complete 3-d point visible projection cameras 3-d point considered lost track computing visibility appearance 3-d point changes visibility change occluded cameras point point visible cameras energy considered problem visibility values robust estimator robust scale standard residuals residuals follow normal scale pyramidal implementation main lucas-kanade method size texture proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee appearance surfel changes updated parameters vector pose template variance appearance pose figure surfel defined appearance texture template pose position rotation surfel tracking energy considered case texture template case surfel texture template warp surfel texture template camera perspective texture pixel translation rotation corresponding surfel projection camera warp warp homogeneous texture coordinates vector represented tracking images pixel time order track pyramidal implementation based 2-d tracker 3-d motion resolution original image scale scale parameters vector projection corresponding image tracking 3-d surfels 3-d surfel defined small planar 3-d space pose appearance appearance represented texture resolution texture size texture pixel resolution surfel resolution resolution surfel reference frame attached texture image normal appearance surfel extracted time frame inverse warp camera warp perspective time frame best best texture template updated time estimated variance intensity texture pixel texture template pose surfel represented position parameters rotation reference frame surfel reference frame consider surfels extended local affine function rotation parameters actual rotation rotation vector rotation matrix computed matrix rotation matrix translation vector corresponding surfel expression texture warp simply warp respect surfel parameters complete expression jacobian condition number smallest singular singular small consider surfel 3-d point tracking surfel tracking work described dellaert surfel surfel optical computing visibility method 3-d point tracking compute visibility surfel robust estimation problem case surfel geometric surface surfel method compute surface approximation warp affine warp surfel approximation surfel small proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee camera warp coordinates affine warp order affine values inverse dkf update equations camera template pixel surface surfel geometric approach surfel occlusion parts compute surfel texture template image visibility camera set occluded surfels detected pose pose estimation time geometric visibility computation described surfel orientation time pose estimation measurement texture state variance measurement multiple intensity change update equations change measurement noise visibility expression measurement noise dkf equations pose visibility update template pixel cameras lost surfels detected condition number visibility surfel surfel considered lost track handling large surfel tracking 3-d point tracking large motion surfel tracking intensity surfel pyramidal implementation propose surfel tracking time frame 3-d point tracking update surfel position pose parameters estimated surfel texture template update texture template extracted time frame camera inverse warp occlusion texture better extract camera images surfel initialization images time frame pose visibility surfel texture extracted time texture texture template variance propose dellaert super-resolved texture dkf state measurement state parameters measurement noise state state texture measurement state inverse image surfel measurement noise good 3-d points surfels track classical 2-d point tracking warp function image points image matrix image point best work extended warp image intensity space case 3-d tracking compute proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee point point 3-d space point point space surfel 3-d propose approach select points 3-d points matched images time frame cameras setup small baseline 3-d point tracking surfel tracking 3-d point tracking compute hessian jacobian 3-d warp matched select smallest condition number point visibility state images reconstruct 3-d point stereo pair reconstruct 3-d point considered visible matched points stereo matching points fact points classical method select 3-d points reconstructed reconstruction multi-camera 2-d feature local normal scene estimated method lucas-kanade tracking affine warp surfel texture template image reconstruction texture template variance set large update visibility surfel tracking method time frame time order position update texture surfel tracked figure 5-camera setup cameras stereo pair figure 3-d point tracking cameras visibility 3-d cameras cameras multi-camera cameras small baseline 3-d points surfel initialization dense stereo reconstruction cameras tracking tracked points surfels parts scene stereo cameras time initialization method tracking methods presented points surfels points sequence tracked tracking presented 5-camera sequences tracking method sequences 3-d point sequences surfel sequences presented original image 3-d trajectories better scene 3-d point tracking 3-d points drawn camera visibility surfel tracking full surfel drawn camera image visible drawn surfel main surfel tracking surfels 3-d point tracking good scene surface smooth textured sequence surfel tracking better 3-d points presented multi-camera extension feature proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee initial 3-d reconstruction scene 3-d point tracker recover scene flow point condition number forward additive formulations tracking problem forward additive original formulation full warp template space image formulations computational forward compositional formulation energy figure surfel tracking camera time frame 3-d surfels small visible surfels drawn actual size 3-d points coordinates track planar surface elements full pose methods recover scene flow trajectories method scene flow visibility problem surfel scene surface planar textured order extract surface normal initial surfel tracking method local texture follow surface orientation difficult parts scene textured surfel tracking warps difficult track surfels 3-d point tracking multi-camera reconstruction computational cost cost tracking 2-d points single number robust standard 2-d lucas-kanade point tracking method 5-camera setup single visibility handling robust estimation number method drift small 3-d position estimation 3-d point surface scene good surfel tracking points scene robust smooth textured 3-d point tracker computational cost fact jacobian warp computed inverse compositional formulation hessian set warps identity identity condition formulation surfel warp identity perspective projection texture template identity cameras single forward compositional inverse compositional formulations surfel inverse additive formulation set warps surfel tracking 3-d point compositional formulation warp identity computational cost respect forward additive formulation warp image jacobian hessian forward additive formulation scene flow vedula work 3-d multiple extracting 2-d optical flow image scene flow considered motion 3-d proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee main problem fact scene flow difficult vedula reconstruct optical optical flow computation work carceroni scene flow estimation difficult optical track image small 3-d small visibility camera occluded parts methods propose compute scene flow based lucas-kanade algorithm extension multiple cameras tracking tracking 3-d points handling visibility based robust surfels visibility standard extended robust tracking scene flow extracted number cameras complete 3-d reconstruction scene methods computing scene flow ieee june scene flow stereo ieee image stereo conference image june ieee good orientation scale scene ieee ieee scene scene flow image june ieee lucas-kanade pyramidal implementation feature carceroni scene surfel super-resolved texture tracking planar surface computing 3-d images 3-d vision robust proceedings 2006 ieee computer society conference computer vision pattern recognition 2006 ieee
rigid scene flow lidar scans tipaldi dynamic environment autonomous robot propose novel method estimating dense rigid scene flow lidar problem energy minimization local geometric constancy regularization smooth motion dynamics point multiple sequences kitti odometry estimate multiple motions dynamic test approach dataset method adapts case non-rigid comparison ground truth kitti method outperforms icp-based dynamics environment robot environment dynamics scene object point pointwise motion dense motion field motions scene robot rigid motion field points static scene estimation sensor better robot human pointwise motion computer vision methods proposed estimating pointwise motion images color depth methods scene flow estimation color depth images methods point cloud data problem constancy gradients lidar lidar sparse nature lidar gradients well concept neighborhood well images image well defined structure methods computer work european colorized translational estimated rigid motion black points represent static three moving green moves opposite linear motion data collected high frame data collected high frame lidar assumption linear motion propose novel approach estimating rigid scene flow problem energy minimization introduce concept geometric local structure introduce novel neighborhood structure point approach scene points neighbors estimate rigid linear translational motion proposed method estimate dense rigid scene flow lidar figure rigid motion flow estimated test approach multiple sequences kitti odometry dataset method estimates motion ground truth evaluate approach icp-based ground truth motion estimate alignment advantages pointwise motion approach dataset experiments method adapts case non-rigid work problem estimating motion flow methods motion optical flow describes translation motion image describes translation rigid scene flow describes rigid optical optical term scene depth estimate extended approach include depth estimate scene algorithm based method estimate scene flow rgb-d methods discussed estimating dense flow rgb-d energy minimization regularization estimate smooth proposed method estimate motion structure rgb-d work local rigid regularization proposed method dense rgb-d surfaces estimate dense motion methods color depth approach relies sparse depth comparison methods discussed outdoor propose method object scene flow scene pixels estimate rigid motion association pixels assumption rigid structure outdoor environment assumption local rigidity method estimate motion non-rigid object method robust methods proposed tracking lidar data outdoor proposed approach detecting tracking dynamic method based local detecting object icp lidar data tipaldi proposed methods detecting estimating motion previous work proposed method detecting tracking lidar estimated multiple rigid motion ransac approach points scene motion estimating pointwise motion motion object method extended dynamic objects assumption local rigidity deformation proposed approach icp registering deformable surfaces sparse lidar perform pointwise smooth deformation method method estimate feature based nearest neighbors data assumption nearest neighbors surfaces dynamic object moves direction opposite direction sensor feature based method cases estimate large registering deformable extended approach concept estimate pointwise deformation approach cases non-rigid objects registering deformable dense surfaces estimating pointwise motion sparse lidar comparison work approach lidar scan set lidar scans find dense rigid motion field motion rigid body point translation equation introduce representation translation motion scene rigid motion field represent problem factor graph factor nodes state nodes set connecting state variable nodes figure factor graph factor graph describes 2np regularization term data term estimate defined estimating pointwise motion case rigid motion point correspondences optimization smooth motion include regularization term energy regularization term energy minimization case estimating motion field image local neighborhood well consider pixels image case point cloud concept neighborhood calculate neighborhood consider points point method relies assumption rigidity local neighborhood structure neighborhood neighborhood points surface objects neighbors objects problem structure surface consider points neighbors error term regularization term defined factor graph representation red represent state scans green represent regularization represent data factor nodes energy data term regularization term term set corresponding set neighboring find energy data term approach relies assumption geometric local geometric structure local feature methods constancy consider geometric structure gradients geometric structure well sparse nature lidar constancy assumption points scan local geometric find point correspondences approach discussed previous work correspondences data term defined state variable nodes corresponding error data term motion neighboring nodes representation energy regularization term defined kep figure green square connecting neighboring state variable nodes represents error data regularization problem sparse square equation kep 2np corresponding points scans represented rigid motion data term defined figure square connecting state variable nodes scans represents algorithm find estimate regularization term neighboring problem multiple point correspondences estimate robust kernel ransac correspondences kitti dataset sequences kitti odometry dataset evaluate sequence calculate dense rigid motion figure motion point cloud direction estimated translational figure color figure colorized point scans sequences better ground truth odometry sensor static scene sequences represented black color figure dynamic objects moving opposite method estimates motions advantages motion estimate points sensor points closer arm points motion points closer black alignment cases scans aligned static scene well dynamic estimated motion points arm approach points optimization problem points motion field ground calculate error translation calculate error table translational error error method icp three sequences error method outperforms icp icp method result expected icp case sensor alignment ground truth motion static motion estimate dynamic alignment scans error kernel robust kernel error data term equation kep 2np initial error perform robust kernel error perform optimization robust kernel motion expected include term energy minimization estimate graph estimates previous points motion transformed frame perform data association transformed points points scan estimated data association perform optimization state variable nodes initial estimates large initial perform multiple experiments evaluate set experiments sequences kitti odometry estimating ground truth motion point moving object scene static motion point motion evaluate three sequences moving objects sequences multiple dynamic advantages estimating pointwise evaluate method dataset estimate motion non-rigid collected dataset lidar sensor experiments ground points kernel error method icp initial ransac calculate initial estimate alignment estimated motion motion flow sequences corresponding aligned colorized color figure static structure represented black color dynamic objects red moving opposite points black represents scan green points represents scan transformed table error rigid motion flow sequence sequence sequence icp crispness score nearest point point set transformed point point set points crispness score point closer point table crispness score outdoor sequence sequence sequence sequence sequence icp color direction translational non-rigid objects test approach non-rigid collected dataset human moving moving bending bending figure approach method cases arm moving upwards body bending case multiple arm moves upwards arm moves body method estimates motions scan icp-based body crispness method method result expected outdoor objects rigid points high optical flow estimation based european conference computer vision sparse computer detection tracking lidar ieee international conference robotics automation algorithm conference optical flow computer vision image autonomous kitti vision ieee conference computer vision pattern icp algorithm objects motion robust rgb-d dense motion estimation color ieee international conference robotics automation dense rgb-d scene ieee international conference robotics automation object detection tracking ieee international conference robotics automation graph ieee international conference robotics automation ieee international conference robotics automation surface methods large ieee international conference robotics automation object scene flow autonomous ieee conference computer vision pattern tracking objects ieee international conference robotics automation tracking non-rigid ieee conference computer vision pattern dense scene flow estimation european conference computer vision tipaldi motion estimation international conference local surface european conference computer vision moving object detection motion ieee international conference robotics automation scene ieee international conference computer scene flow estimation rigid motion ieee international conference computer alignment rigid structure arm moving upwards body bending points scan red scan scan aligned approach green icp result alignment proposed method better represents table crispness score cases method better method adapts non-rigid case outperforms rigid motion based advantages estimating pointwise motion nature table crispness score human motion moving arm upwards moving arm bending bending icp novel method estimating rigid scene flow lidar introduce concept geometric constancy estimate dense rigid motion novel method estimating neighboring points point cloud approach multiple sequences kitti dataset dataset kitti dataset translation error alignment dataset method outperforms rigid motion based advantages method estimate multiple motions case rigid objects adapts cases non-rigid
second vision dense variational scene flow warping higher order regularization computer vision method estimate dense motion scene depth intensity method formulated convex energy motion warping scene point estimated projection back-projection directly utilize higher order regularization input data anisotropic diffusion enables calculation dense flow field smooth non-rigid movements motion boundaries depth numerical order enables method variety scene flow calculation existing approaches terms speed applications camera pose estimation depth image enabled high accuracy proposed applications depth sensors microsoft kinect pmd nano figure consecutive depth intensity estimate variational energy minimization framework anisotropic total generalized variation components better color components corresponding depth scene motion estimation accurate motion scene flow scene motion estimation computer vision optical flow work horn estimation accurate motion image estimates projection problems motion movements translation camera estimated estimated depth reconstruction multi-view range sensors microsoft camera variety time sensors allows multi-view depth directly dense depth proposed approach depth intensity data calculate dense approaches define warping directly projection back-projection space standard camera model higher order regularization total generalized variation model smooth non-rigid motion boundaries depth motion input depth weight direct regularization anisotropic diffusion depth intensity accurate dense motion model smooth flow transitions objects non-rigid sharp boundaries flow field objects work model depth intensity image constraint difference calculated projection back-projection higher order regularization anisotropic diffusion based input data better rotations non-rigid proposed model convex energy minimization problem application method variety numerical visual approaches method terms speed applications enabled high accuracy camera depth image dense optimization framework optimization linear problem visual intensity images depth framework range flow estimation microsoft kinect global energy minimization caused imaging variational estimation better objects motion directly rgb-d point existing approaches estimation sequence corresponding intensity depth images local global local methods estimate global methods dense flow field data regularization method global optimization methods depth method dense directly warping projection back-projection method estimates components method object image space caused object movement order regularization higher order regularization motion boundaries objects well smooth transitions caused rotation non-rigid compared order method based motion object boundaries anisotropic diffusion tensor depth image flow gradient optimization work work horn work approaches methods images calculate utilize field existing methods estimation acquisition multi-view calculation acquisition depth intensity method clearly definition calculate framework camera multi-view approach work enabled depth sensors methods based depth intensity data hadfield proposed method problem flow field approach local methods result estimate method method estimation motion scene points observed time consecutive acquisition scene time depth intensity image pairs motion scene point define movement input data optimal linear brightness difference flow field order intensity constraint brightness difference evaluated intensity illumination brightness constraint depth flow definition depth difference figure flow scene point acquired frame second movement defined flow projection image space point defined warping time dense flow field image depth error solve problem constraints noise regularization general flow model frames frame frame energy minimization model implementation solve minimization optimal estimation depth constraint optimization model order constraints data terms convex optimization scene flow scene points observed image depth scene point camera projection image movement frames image space defined energy optimization calculate dense flow field convex global intensity depth constraint constraints three solve regularization regularization terms methods order piecewise constant higher order tgv problems order tgv allows reconstruction piecewise image warping space enables define brightness difference intensity image brightness constraint definition second order tgv formulated tgv min problem point problem size min order second order parameters second order terms tgv convex allows compute optimal tgv flow field depth weight regularization gradient input depth image anisotropic diffusion tensor tgv reconstruction depth map tensor calculated convex sets result direction depth image gradient calculated noise magnitude energy optimization model intensity depth constraint tgv regularization anisotropic min minimization scheme proposed solve optimization scheme solve high frame numerical scheme supplementary constraints optimization image warping space camera projection parameters terms energy constant evaluation qualitative quantitative evaluation atgv-sf real pmd nano camera resolution microsoft kinect camera depth image resolution error measurements average error average point error error flow aae calculated k4wv2 depth set depth measurements stereo sensors illumination model implementation numerical implementation optimization method convex clearly solve optimization problem constraints parameters optimization model set constant parameters illumination set tensor parameters set time method 100 visual groundtruth sphereflow atgv-sf scene flow evaluation synthetic real datasets experiment properties terms comparison sota quantitative evaluation synthetic dataset defined static dataset groundtruth input depth image pairs well intensity image pairs scene point depth noise input data acquisition pure translation object size direction pure translation camera rotation table compared nl-tv-ncc methods method sota sphereflow calculate error result method image result methods space noise depth qualitative evaluation camera moving rigid non-rigid objects average method synthetic pmd nano k4wv2 experiment points properties terms order model well constant movements direction model smooth flow transitions rotations non-rigid movements piecewise constant flow anisotropic tensor estimation directly tgv regularization object rgb-d based method delivers flow magnitude problem observed figure flow estimation synthetic input depth input intensity images groundtruth pure movement pure methods estimate flow field magnitude estimates constant flow better movement higher noise noise illumination pmd nano method estimation sota methods object movements image real model object movements constant movement direction estimate method three estimate motion image middlebury evaluation method compared sota methods evaluated existing stereo intensity disparity maps middlebury venus datasets depth maps calculated defined pure camera movement stereo intensity allows calculate scene motion point scene movement movement direction estimated image space direct comparison disparity table evaluation compared sota methods stereo depth intensity data method delivers compared sota methods solve problem utilize disparity maps compute stereo methods general applicability parameters three datasets epeof aaeof nl-tv-ncc sphereflow atgv-sf atgv-sf tensor atgv-sf epesf aaesf epeof aaeof epeof aaeof epesf aaesf epesf aaesf table evaluation synthetic comparison method methods object movements terms aae evaluation methods terms best result movement second best epeof rmsvz aaeof hadfield atgv-sf epeof rmsvz aaeof venus epeof rmsvz aaeof table quantitative comparison methods middlebury error aae best result dataset second best methods calculate stereo middlebury venus dataset 100 distance icp kfusion applications accurate estimation computer vision applications estimation method estimate camera pose static scene model model resolution depth image moving object depth camera pose estimation accurate enables application pose moving camera space static method directly estimate movement scene point frame estimated flow field consecutive frames corresponding pose point sets general rotation translation calculated motion estimation camera pose figure evaluation camera pose estimation method standard icp model based icp error real estimated camera pose relative distance consecutive frames camera frames pose estimation numerical evaluation camera pose pmd nano camera real scene acquired camera poses moving linear movement direction distance consecutive sequence supplementary estimation accuracy compared standard icp 100 model based icp proposed kfusion framework camera frame defined difference camera poses poses calculated terms error error movement direction kfusion compared relative rotation error pose estimation three static average frame error relative error kfusion method clearly standard icp camera pose relative error movement compared model based kfusion model accuracy method superresolution calculated measurements superresolution delivers sharp moving method estimation depth intensity estimation formulated convex energy minimization directly utilize depth projection warping space depth intensity image regularization formulated higher order smooth flow rotations non-rigid sharp boundaries flow field input depth image weight direct regularization anisotropic diffusion quantitative qualitative evaluation method clearly existing sota methods calculation terms speed computer vision applicability depth image superresolution camera pose estimation enabled accuracy properties camera object pose estimation depth superresolution framework dense non-rigid model superresolution camera pose depth superresolution experiment estimation depth superresolution moving objects compute consecutive depth intensity image pairs sequence point set acquisition frame point depth image point sets higher resolution image space points map image depth points experiment consecutive images real scene depth map size input visual rigid non-rigid movements superresolution approach compared input depth map sequence supplementary work multi-view scene flow variational total generalized imaging scene flow estimation convex problems applications imaging object motion image rgb-d dense motion estimation color resolution input experiment applicability method problem depth image figure evaluation depth image superresolution real image object input depth map corresponding superresolution result resolution input intensity nl-tv-ncc sphereflow atgv-sf atgv-sf tensor atgv-sf figure evaluation method real image input intensity images second flow acquired k4wv2 pmd nano camera flow result scene evaluated sphereflow atgv-sf second order regularization atgv-sf anisotropic diffusion method motion image depth anisotropic total generalized range flow kinect hadfield based scene flow depth hadfield scene scene flow horn optical scene flow rgb-d variational method scene flow estimation stereo scene flow depth color image application stereo constraints estimation image dense pmd scene flow intensity depth scene flow accurate motion field estimation stereo image stereo variational stereo stereo depth maps quantitative optical flow estimation scene piecewise rigid scene scene flow motion motion estimation total variation dense scene flow based depth
dense variational scene flow novel model dense variational scene flow rgb-d data horst novel method dense variational scene flow estimation based ternary census transform combination patchwise closest points depth data ternary census transform intensity data term illumination low patchwise closest points search depth data term low structured higher order regularization input data anisotropic diffusion calculate dense flow field smooth well non-rigid movements preserving flow numerical algorithm solved based primal-dual high frame qualitative quantitative evaluation novel method scene flow calculation existing method dense depth intensity data microsoft kinect structure motion objects structure motion static scenes well scenes pose scene flow applications analysis range camera pose work pure optical flow estimation motion estimate estimation combined depth range direct depth measurements multi-view depth novel sensors microsoft kinect dense variational scene flow camera level sensors scene multi-view directly dense depth data figure scene flow consecutive depth intensity depth data term calculated patchwise closest point search intensity data term calculated ternary census transform regularization propose anisotropic total generalized variation flow color approach sensors calculate dense combination depth intensity data novel constraints model motion image warping directly propose intensity data term estimate scene correspondences ternary census transform local depth data term depth measurements directly patchwise closest point compared traditional constancy terms method invariant illumination robust acquisition noise delivers regions low structure low constraints combined higher order regularization total generalized variation anisotropic diffusion tensor based input work propose novel model data terms tct intensity patchwise search depth data constraints flow warping projection proposed data terms combined anisotropic higher order regularization variational energy minimization problem solved primal-dual work scene flow estimate motion multi-view image multi-view approach work depth methods combined depth intensity acquisitions local approach hadfield calculation particle particle based estimation flow proposed method directly calculate dense variational scene flow model dense optimization estimation dense flow field optimization scheme proposed combined intensity set depth correspondences calculated proposed method depth camera estimate dense optical flow depth flow combined global energy optimization depth intensity variational object cek motion directly patch matching point estimate motion estimation patch rgb-d patchmatch algorithm input noise illumination existing particle based approaches estimate set approaches dense flow field approaches calculate local correspondences depth images global flow estimation based intensity correspondence model global optimization methods methods estimate flow pixelwise depth model intensity fidelity patchwise ternary census transform invariant depth fidelity directly calculated point patchwise distance closest point iterative closest point low structured regions robust acquisition regularization methods order higher order regularization edge preserving smooth transitions rotations non-rigid movements anisotropic diffusion tensor based depth images flow gradient gradient direction optimization method motion estimation calculate motion acquired scene points consecutive acquisition scene time consecutive depth intensity image scene points image depth scene point camera projection matrix homogeneous image motion scene point time scene motion calculated dense variational scene flow correspondence image space movement image space variational model estimate flow minimization figure flow scene point acquired frame second movement acquisitions defined flow projection image space point defined warping projection direction variational optimization model scene flow estimation approach formulated variational problem min intensity depth data estimation constraints noise constancy regularization traditional intensity data terms calculated pixelwise derivatives data term invariant illumination robust ternary census transform intensity fidelity computing tct intensity images comparison pixelwise hamming distance local pixelwise dense variational scene flow census transformed image ternary pixel illumination invariant local hamming method intensity data term pixel census image hamming distance ternary census transformed images invariant changes acquisition noise changes global depth fidelity calculated directly matching traditional depth data calculated pixelwise derivatives depth image space traditional pixelwise constraints homogeneous regions low depth propose flow error based iterative closest point algorithm calculated patchwise point directly space local depth acquisitions camera calculate error point flow point correspondence closest point transformed matching depth pixel method robust homogeneous depth regions acquisition direct matching image intensity well depth fidelity term data term propose direct data terms flow field defined --2 second data term --2 positive matrix positive approximation defined --2 dense variational scene flow positive second order derivatives derivatives approximation estimation regularization optical flow approaches order variation total variation piecewise constancy flow higher order total generalized variation second proposed second order derivatives piecewise second order tgv formulated min flow field tgv edge preserving edge input depth images anisotropic diffusion tensor based input depth image tensor calculated direction depth image gradient anisotropic diffusion tensor motion gradient gradient direction optimization regularization term stereo depth image combination convex tgv regularization anisotropic weighting smooth transitions object rotations non-rigid flow moving objects based energy optimization model defined min parameters data pixelwise data measurements set depth measurements stereo sensors numerical proposed optimization problem convex weighting optimization problem convex constraints transform problem point problem image size dense variational scene flow point problem solved primal-dual optimization scheme proposed scheme high frame local approximation constraints primal-dual calculation image iterative optimization warping space camera projection matrix level camera matrix level level level weighting parameters terms energy second order derivatives search patchwise closest point search computation patch size set tct term calculated numerical optimization scheme evaluation qualitative quantitative evaluation analysis terms method real synthetic datasets numerical evaluation terms compared scene flow methods visual real pmd nano time camera microsoft kinect camera flow error calculated error measurements error point error middlebury evaluation disparity error visual scene flow evaluation synthetic real datasets algorithm terms compared sota optical flow synthetic real dataset moving objects static synthetic dataset static scene depth intensity image well simulate acquisition noise noise input object movements pure object size direction pure camera table sota methods methods best method cek k4wv2 dense variational scene flow epesf cek cp-census tensor cp-census cp-census aaesf epesf aaesf epesf aaesf cp-census table scene flow evaluation synthetic comparison method sota methods object movements terms evaluation methods terms best movement second best input method pure movement error methods calculated projection space depth visual evaluation real acquisitions moving rigid non-rigid objects acquired pmd nano k4wv2 camera depth model traditional object rotations non-rigid movements higher order regularization smooth transitions order regularization higher order model order approaches work well pure movements model smooth flow transitions rotations non-rigid movements flow anisotropic tensor quality flow field object rgb-d patchmatch approach cek delivers flow noise illumination changes pmd nano middlebury evaluation order quantitative comparison sota method existing scene flow stereo intensity disparity maps middlebury venus datasets simulate scene images acquired pure camera scene motion point scene pure movement direction movement compared disparity maps simulate depth calculated image space direct comparison disparity table evaluation compared sota methods stereo depth intensity data method delivers quality compared sota methods parameters datasets venus dataset dense variational scene flow epeof rmsvz aaeof hadfield cek cp-census epeof rmsvz aaeof venus epeof rmsvz aaeof table evaluation middlebury error best dataset second best methods calculate stereo intensity cp-census cp-census tensor cek cp-census figure evaluation methods real image input intensity images row middlebury second row flow k4wv2 row acquired pmd nano scene cp-census second order regularization anisotropic diffusion method cek compared method motion figure best proposed method estimation scene flow depth intensity estimation formulated convex energy minimization problem data terms anisotropic higher order method scenes low low structure robust illumination smooth flow rotations non-rigid flow field quantitative qualitative evaluation method existing high estimation quality applications non-rigid structure motion depth image camera pose estimation dense variational scene flow work multi-view scene flow variational michael christoph carsten patchmatch stereo stereo matching thomas total generalized thomas primal-dual algorithm convex applications rgb-d dense motion estimation color horst image depth anisotropic total generalized computing range flow kinect simon hadfield richard particle based scene flow depth simon hadfield richard scene scene flow optical michael christoph carsten depth rigid michael carsten scene flow rgb-d variational method scene flow estimation stereo scene flow depth color constraints estimation image dense variational scene flow pmd scene flow intensity depth scene flow stefan thomas horst stereo variational stereo richard stereo depth maps structured computation optical flow census stefan quantitative analysis optical flow estimation simon scene scene christoph stefan piecewise rigid scene thomas scene flow computation motion convex approaches high thomas horst motion estimation total variation local computing visual dense scene flow based depth iterative point matching
fusion stereo motion robust perception obstacle mobile vision based depth robust fast detection moving order reach paper vision fusion depth motion image moving image points estimated simultaneously detection moving moving objects objects fast estimation motion perception mobile three-dimensional stereo vision stereo reveal motion depth detected objects tracked time order standard approach detection moving objects front stationary front parking vehicles figure overcome problem constraint measured optical flow ego-motion stereo moving objects constraint approach small ego-motion frames mentioned simultaneously depth motion images time approach paper fusion stereo motion segmentation standard stereo track points depth stereo vision frames kalman result improved accuracy estimation considered point vector point motion mentioned segmentation problem fusion practical tests reveal detection moving front moving car standard inertial sensors ego-motion image points stationary mentioned accuracy system described tracked images stereo estimation relative stereo motion decide points track 2000 image depth estimation based based optical flow estimation stereo system paper system model measurement equation proposed kalman rate convergence considered system multi-filter system improved practical crossing objects oncoming system coordinate system points points franke distance point coordinate system estimated coordinate system moving camera positive system model vehicle constant velocity rate time described car coordinate system position point time described car coordinate system time velocity vector matrix position velocity vector system model equation state matrix matrix noise assumed gaussian white noise covariance matrix measurement model measurement image tracked disparity stereo vision fusion stereo motion real stereo filtered velocity distance frame 100 real initial speed initial speed initial speed frame 100 distance estimation velocity estimation moving three point speed estimation kalman considered point initial position observer moves constant speed positive camera measurement equation point camera coordinate system stereo camera noise assumed gaussian white noise covariance matrix three-dimensional measurement figure estimated relative distance point measured observer moving speed initial position point white gaussian noise image position disparity position result speed point initialized filter point point moves speed positive figure estimation three initialized initial speed convergence multi-filter approach described franke multiple filters improved rate convergence time initial overcome multiple kalman filters initialized speeds position velocity decide state distance real predicted innovation measurement predicted measurement innovation covariance matrix decide measurement kalman filter multiple model small point order nis figure low pass filtered nis three initialized filters figure quality measurement measured predicted three filter initial state tracked point filters real state quality kalman measurement quality measurement low pass figure result three filters initialized speeds figure filter initialized multi-filter approach figure system three fusion stereo motion velocity initial speed initial speed initial speed frame 100 nis real nis state initial speed frame 100 low pass filtered innovation filters figure improved convergence kalman filter kalman filter estimation result velocity estimation cyclist moving front parking predicted position point image estimation stationary real practical stationary vehicles vehicle small relative oncoming traffic traffic multi-filter approach filters order reach fast crossing situation figure result velocity estimation figure cyclist front parking vehicles observer moves constant speed predicted position point estimated order franke velocity estimation oncoming observer moves constant speed image figure situation image figure estimation oncoming traffic situation observer moves constant speed velocity tracked real position practical tests reveal robust measurement point moves filtered kalman measurement problem overcome standard proposed fusion stereo optical flow simultaneously depth accuracy position motion considered segmentation based fast moving objects objects speed motion detected image objects fusion based kalman frames robust optical proposed multi-filter approach speeds rate convergence fast practical tests crossing cyclist detected fusion stereo motion described approach track 2000 image points optical flow images ego-motion assumed inertial sensors ego-motion vehicle based inertial sensors kalman obstacle motion motion order reach ego-motion image points robust approach ego-motion estimation mobile stereo international motion 2004 estimation tracking optical flow stereo disparity ieee international conference intelligent stereo vision traffic ieee conference intelligent vehicles kalman filter based depth motion estimation fast ieee conference intelligent vehicles real time fusion motion stereo constraint fast obstacle three-dimensional relative tracking 2004 position estimation tracking 2000 ieee international conference 2000 perception 2004 detection tracking point
ieee computer photogeometric scene flow high-detail dynamic reconstruction photometric stereo technique high-detail reconstruction geometry surface integration combined multiview stereo dynamic reconstruction problem optical flow image alignment rapid changes current methods compute optical flow mvs independent errors scene flow methods estimate geometry fine detail photogeometric scene flow dynamic pgsf mvs based key image alignment surfaces improve surface gradients term mvs continuous depth demonstrated quality rgb photometric stereo multiview stereo photogeometric stereo photogeometric scene flow figure photogeometric scene flow motion rgb albedo simultaneously solving photometric stereo multiview stereo optical flow mvs low-frequency shape shadow regions geometry appearance video quality model facial low geometry appearance improve resolution detail geometry appearance dynamic photometric stereo technique capture high-detail geometry appearance real observed illumination conditions detail initial multiview stereo computed mvs complementary continuous depth values ieee integration mvs absolute estimates matching errors spatial regularization fine mvs computed dynamic object motion additional images changes remains optical flow independent detail 3-color capture geometry single spectrally multiplexed 3-color propose approach dynamic capture simultaneously coupled key approach fact benefits facilitates computation mvs solution subproblems highly detailed instantaneous motion approach photogeometric scene flow scene flow pgsf dense estimates motion detail object introduce pgsf simple acquisition setup cameras directional lights lights multiplexed time spectrally provide adequate sampling instantaneous appearance video motion reflectance model insufficient sampling problem 3-color pgsf algorithm dynamic reconstruction motion compensation changes approach general setups adopt optical flow theory stereo time-multiplexed image alignment required assumption image alignment methods task work image alignment varying illumination linear small motion light shading small illumination changes face tracking frames acquisition computed pairs tracking frames frames linear linear assumption demonstrated propose complementary illumination conditions simultaneously aligned tracking frame method gradient illumination tracking frames temporal color colored lights require motion compensation capture three illumination conditions simultaneously rgb sensor 3-color recover surface gradients rgb albedo three require introduce reconstruction 3-color constraint observed light calibration difficult compute assumption method introduce errors appearance space simple solution face fine surface detail ability recover face color approaches assumption image color image single cameras aligned color spectrally light surface reflectance requiring calibration reconstruction time-multiplexed illumination combined dynamic image alignment constant approach colored lights yields surface estimates video 3-color object three directions shadow problem remains work scene flow previous work closely common approach dynamic capture mvs regions mvs capture surface state-of-the-art approaches introduce photometric constraints recover missing detail improve illumination insufficient define surface gradients recovered appearance shading scene flow based mvs provide geometry time-multiplexed single camera multiple images object varying illumination images recover depth pixels fine detail surfaces multiple absolute depth low-frequency depth integration detail geometry mvs benefits simultaneous computation mvs problem method depth linear dynamic captured time backwards scene flow forwards scene flow views figure binocular pgsf setup camera colored light sources illumination conditions 3-frame video pgsf reconstruction problem surface time backwards forwards motion 3-frame sliding unknowns depthmaps flow images aligned direction light previous propose simple setup color time sampling directions setup regularization motion photometric calibration simple require additional assumption diffuse pgsf problem image alignment rapid changes illumination requiring linear result full rgb geometry motion spatial detail temporal relations adequate sampling surface reflectance surface rgb appearance video reflectance model acquisition setup pgsf colored lights capture dynamic facial time window instantaneous surface geometry adopt simple lambertian model freedom rgb highlights outliers highlights light sensor lambertian model surface patch observed illumination time-multiplexed video frames color fact adequate motion frames insufficient provide surface regions number sensor propose setup colored light sources reflectance set directional 3-frame time setup note 3-frame pgsf motion compensation required adjacent frames directions result photogeometric scene flow photogeometric scene flow detailed reconstruction current state-of-the-art approaches mvs difficult subproblems solved combined propose pgsf simultaneous solution faced solved benefits alignment absolute depth relations fact mvs surface stereo matching continuous spatial detail facilitates changes illumination surface geometry relight input images closely light small single matching rgb camera sources define rgb sources time video frame supplementary rgb defined light sources space shadow target missing mvs required low-frequency consider binocular setup cameras target object illumination setups cameras video acquisition performed 3-frame window pgsf recover geometry rgb albedo surface time task performed 3-frame sliding window current frame camera full photometric constraints time image alignment estimate surface motion frames backwards motion forwards motion alignment images direction unknowns image time surface depthmaps forwards backwards optical flow consistency constraints vector pgsf defined terms unknowns changes assumption constant pixel adequate assumption rgb albedo remains consistent time adjacent video derive surface flow define lambertian model basic albedo albedo consistency consider lambertian surface patch rgb albedo normal vector time patch simultaneously three directional lights colors denote light direction vectors corresponding light normal light vectors time camera general acquisition color patch rgb pixel multiplexed color rgb color light camera color space camera illumination diffuse unknowns derive albedo consistency consider corresponding pixels images time values color define basic image relation pgsf low surface flow energies relations coupled common coarse-to-fine approach gaussian image pixel views rapid consider pairs color images adjacent time relation standard assumption time-multiplexed assumption small motion model common flow methods relation defined motion geometry requiring albedo fact large number rgb albedo computed pgsf solution surface reconstruction surface estimation simultaneous binocular current estimates flows surface estimation time problem motion derive binary surface energy terms flows surface energies terms depthmap camera esm pairs images 3-frame three rgb color independent constraints freedom number constraints color small number pixels constraints adopt regularization method stereo matching basic pgsf constraint image pairs time views time images illumination denote pixel coordinates time 3-frame stereo matching full advantage complementary constraints directly terms compute vectors photometric stereo consider basic pgsf relation images view bptt camera combined photometric energy term bptt pixels pairs images linear forwards solving large linear depthmap constant vector bptt linear constraint gradient depth values coordinates pixel image view terms camera stereo matching solving directly result set constraints task coarse-to-fine optimization pgsf approach directly continuous depth stereo matching constraints previous photometric constraints define spatial term stereo matching surface vector projection view camera stereo matching energy term defined esm occlusion based consistency pairs energy esm current estimate yields linear term surface update consistency enforce provide consistent surface define additional energy input images shading cross-shaded based projection energy set linear figure relighting photometric images closely match shading values surface estimate light motion estimation motion estimation image camera based algorithm task illumination key advantage surface illumination relight input images closely algorithm approach spatial regularization albedo consistency general method propose advantage flow current depthmap define binary motion energies terms flows direction low relighting relation illumination relight closely match figure relighting facilitates computation illumination pixels small shading values high photometric note pixels missing input pixel color spectrally multiplexed flow relighting relation image flow image alignment photometric photometric flow energy alignment error cross-shaded spt energy energy total photometric flow scene flow energies standard optical image energy yields constraint update spt spt result gradient standard constraint photometric flow consider basic relation pairs images view illumination normal shading terms spt relighting spt spt scene flow defined cross-shaded coordinates defined flow note surface shading terms defined image time image time scene flow energy term flows consistent vector freedom introduce errors estimated denote corresponding pixels defined depthmaps flow vectors pixels projection algorithm photogeometric scene flow 3-frame window time highlights input compute gaussian resolution update flows update flows alternation images update depthmaps alternation compute rgb albedo time flow surface normal error flow flow surface surface normal normal 100 figure motion geometry method linear constraints scene flow view update enforce flows compute consistent single scene enforce consistency scene flow energy capture setup cameras light target cameras color resolution light camera setup stereo face model motion capture dense motion geometry performance pgsf algorithm three independent linear motion mvs method spatial regularization mvs error estimated normal vectors coupled pgsf error face surface errors occlusion mvs motion geometry occlusion figure reconstruction real video frame setup geometry real surface estimates pmvs algorithm pmvs state-of-the-art mvs method based patch matching require dense total 100 video pgsf pmvs estimate depth supplementary pmvs highly fine projection occlusion optimization large motion high surface motion estimated coarse-to-fine gaussian image adopt optimization update surface motion estimates unknowns alternation binary energy algorithm total number number gaussian define number image flow initial surface absolute depth camera surface reconstruction large linear flow algorithm based reconstruction performed 3-frame sliding result previous window initial shadow simple image highlights outliers image values sensor outliers constraints figure detailed reconstruction real frame supplementary profile view overlaid pmvs depth 100 recovered surface overlaid image profile figure rgb geometry estimated motion image supplementary pgsf mvs pgsf figure mvs method capture figure detailed reconstruction surfaces colors depthmaps rgb recovered surface depth accuracy real face overlaid profile figure detail pgsf recover detail albedo accuracy cross-shaded photometric flow real images supplementary motion images aligned pgsf method supplementary video quality appearance motion estimated pgsf ability pgsf highly detailed surfaces colors demonstrated advantage pgsf 3-color ability capture temporal rgb full color albedo relighting task faced captured match pgsf relighting captured supplementary figure captured temporal rgb albedo changes flow supplementary propose photogeometric scene flow simultaneous estimation dynamic pgsf solution three difficult subproblems faced solved capture surface full rgb albedo video propose simple binocular setup colored lights spectrally multiplexed three shadows motion key pgsf general acquisition setups number cameras light color photometric stereo computer photometric stereo technique surfaces highlights ieee multi-view scene flow view theory facial performance capture acm high accuracy optical flow estimation based theory binocular photometric computer multiview ieee facial performance photometric stereo photometric stereo colored shadows photometric shadows photometric ieee optical photometric stereo dynamic surface high-detail capture alignment facial motion rapid acquisition diffuse normal gradient acm photometric dense scene flow estimation linear face acm binocular facial performance capture acm scene dynamic shape capture multi-view photometric acm siggraph photometric stereo face computer algorithm optical approaches motion performance relighting reflectance time-multiplexed acm siggraph temporal performance geometry photometric acm photometric method surface multiple gradient consistent depth video ieee shape motion varying photometric multi-view high resolution capture acm siggraph
computing range flow multi-modal kinect data heidelberg image heidelberg computing heidelberg computing framework range flow estimation multi-modal imaging device essential flow computation calibration alignment range color introduction novel multi-modal range flow algorithm robust range estimation introduction novel multi-modal depth imaging device kinect computer vision time kinect computer vision area framework estimation range flow fields multi-modal sequences captured computation optical flow image sequences range flow depth image sequences range computer vision object camera motion estimation gesture flow estimation time algorithms optical kinect standard range flow algorithms simply computation range flow fields device main result kinect data color depth channels depth channel large areas invalid values edges main paper novel channel alignment algorithm image areas valid previous range flow approaches invalid depth proposed methods applied data captured kinect heidelberg computing range flow multi-modal kinect data kinect device raw data kinect device color color depth channel raw color depth image provided depth raw image projected point multi-modal cameras projected pattern color range flow estimation kinect approaches method source kinect calibration alignment color depth channels method data set large areas invalid values flow estimation proposed method computation actual range flow based ideas ideas paper depth imaging main raw data range flow calibration process novel algorithm alignment depth color actual flow algorithm presented introduction kinect imaging hardware depth imaging device based approach range projected point pattern captured camera figure hardware kinect point pattern garbe estimation computed hardware second vga depth resolution range main kinect dense depth estimation scene second camera vga color images kinect data main computer vision computation hardware process edge provided depth estimation problem approaches camera regions projected pattern depth positions depth problem depth values object dense values positions projected point cameras color depth images aligned cameras optical kinect calibration data alignment multi-modal flow algorithm essential color depth image image object novel alignment algorithm kinect approach based previous methods inverse mapping depth image color image mapping color image depth method compute areas invalid depth areas alignment figure vga resolution result hardware resolution projected computing range flow multi-modal kinect data camera calibration camera calibration simply standard standard provided data alignment actual data alignment algorithm based raw depth values provided kinect disparity pixels color image raw depth values standard stereo positions calibration process approach warp color image aligned depth image disparity field warped color integer pixel values regions valid depth hand color color image regions color mapping depth image color compute inverse disparity field problem computation inverse optical flow proposed case approach disparity flow ideas pixel computed region invalid depth values excluded garbe depth values warped case invalid compute alignment depth figure qualitative proposed method stereo calibration source paper approach data data camera calibration problem raw depth values real values point pattern methods proposed computing raw proposed method raw depth values pixel images computing accurate pixel calibration depth values valid color depth stereo calibration aligned warping color image aligned warping depth image range flow range flow scene flow term optical motion image range flow field field image motion standard optical flow motion depth computing range flow multi-modal kinect data algorithms proposed standard optical flow proposed method standard method optical flow flow paper proposed algorithm methods focus range kinect terms focus real kinect method optical flow image data color image proposed term depth data motion depth range flow motion terms depth term based range flow estimation color images depth data optical flow estimation algorithm data term robust flow estimation multi-modal data computed data alignment algorithm dense color invalid values depth channel object depth essential regions robust flow regions invalid depth values simply depth channel raw depth pixels hardware depth integer integer object borders edge pixels excluded data term color image data computation compute depth image pixels exclusion region pixels excluded valid regularization range flow garbe strong regularization flow fields motion motion edges edges depth estimation exclusion strong regularization valid weak regularization excluded regions motion edges regularization depth image algorithm approach standard image flow image second warped flow computed previous weak regularization invalid regions mask edge weak invalid regions motion borders data range flow fields kinect case optical flow provided accurate algorithm qualitative qualitative range flow estimation hand rows figure computing range flow multi-modal kinect data algorithm kinect range flow algorithm multi-modal image flow warp invalid set exclusion mask set exclusion mask compute flow data terms strong regularization weak regularization set algorithm usage region usage inverse disparity column image range flow second range flow result standard optical flow flow flow flow flow column depth column raw depth data exclusion depth invalid depth pixels pixels exclusion mask edges depth hand strong regularization applied valid depth flow result rows hand area hand borders usage depth channel algorithms area gesture focus sequences garbe range flow estimation result image hand rows warp depth warping color warp depth warping color optical flow range flow depth depth image exclusion mask exclusion mask compute presented novel framework robust range flow estimation multi-modal kinect calibration alignment algorithm mapping robust color depth channels provided applied range presented range flow estimation flow fields hardware framework algorithms computing range flow multi-modal kinect data captured kinect data alignment flow sequences gesture flow computing methods data optical los alamitos optical range imaging vision kinect calibration depth color optical kinect source computer real time computer accurate flow computation computer vision large optical flow los alamitos heidelberg range flow computer vision image optical flow estimation los alamitos optical heidelberg scene
particle based scene flow depth sensors motion field scene object provide scene flow motion field estimate optical current approaches smoothness cost object paper scene flow moving particle multiple hypotheses motion paper scene flow making modern depth sensors monocular appearance algorithm applied existing scene flow comparable approaches multiple fraction estimate estimated frame rest approach allows multiple hypotheses oversmoothing structure motion scene flow work approach estimating scene flow optimisation global brightness constancy authors additional estimating camera additional constraints stereo matching optimisation approach generally time performance implementation algorithm presented paper oversmoothing object structure motion reduce input images smoothness constraints compared brightness constancy matching estimation point smoothness constraints system applied number number authors based point approach making generally point scene flow motion field optical flow projection field image estimate scene observations image particle scene point modern depth current approaches scene flow estimation multi-view camera scene structure motion estimated scene flow estimation depth sensors paper kinecttm system provide accurate scene motion estimated single image estimating motion approaches estimate dense motion performed sparse scene flow estimation motion estimates tracking coverage previous work stereo multi-view authors depth reconstruction provide structure motion work making depth estimating scene flow monocular appearance depth sensor scene flow constraints appearance sparse motion estimation depth performance work paper performed monocular depth hypotheses frames estimate structure dense motion multiple hypotheses algorithm presented estimates motion field velocity hypotheses position work coarse scene regions based background exploring additional scene probability space estimating dense motion velocity vector position scene probability space structure point motion probability existing set observations posterior probability likelihood probability probability posterior previous motion likelihood separate observations appearance depth likelihood brightness constancy optical flow scene flow point set cameras multi-view input set images cameras projection point scene camera image sequence frames scene point velocity cameras frames paper structure rest scene flow estimation modern depth scene particle approach coverage estimated motion field algorithm presented dense qualitative system performed sequence kinecttm algorithm applied existing scene flow finally paper number image based generally images camera kinecttm system depth camera projection matrix producing image sequence appearance camera projection matrix producing image sequence scene depth projection matrix equation point scene probability space cost finally likelihood equation estimation scene flow approaches scene particle algorithm coarse reduce local probability input images view applied scene number scene particles smaller images hypotheses camera previous estimate spatial fewer hypotheses coarse second performed particles gaussian standard gaussian allows particles converge scene particles algorithm implementation implementation second structure depth sensor previous position motion estimate point previous scene likelihood equation scene particle algorithm high probability scene particle algorithm posterior population weighted particle vector position motion vector scene scene particles spatial position separate motion hypotheses scene scene coverage particle converge global maximum scene particle algorithm output set local scene particles local estimated motion scene particle previous based frame allows scene coverage resampling performed particle number gaussian population number particle areas scene space high probability areas probability resampling depth sensor scene particles estimating algorithm dense flow estimation particle based particle scene particle particle single motion estimated flow field scene particle weighted average scene particles position additional ray ray resampling scene particle ray camera pixel appearance resampling scene particle population performed approach separate particle motion particles frame structure point depth sensor motion estimated number scene middlebury existing scene flow dataset stereo middlebury scene images cameras images camera camera second camera scene object allows ground motion point existing scene particles motion velocity range set maximum velocity cameras field view application middlebury velocity position camera velocities maximum velocity range motion estimated camera images output depth sensor estimated scene flow image ground motion field estimate pixel weighted average scene particles error error accuracy estimated motion ground comparable average error accuracy motion finally coverage input motion compared approaches scene particle approaches estimate scene scene multiple optical flow appearance depth scene particle algorithm single estimates motion optical flow algorithm scene particle algorithm estimates motion previous making fewer velocities image kinect sequence qualitative kinect figure output motion field images motion field algorithm standard depth figure point depth appearance regions regions regions areas appearance image depth regions depth motion field figure velocity estimated structure flow moving background scene motion estimated motion estimates leg moving areas leg single rest areas background previous leg projection flow structure cost structure visible previous figure standard oversmoothing optimisation approaches scene particles position object scene particle velocities scene particles local scene flow estimation frame figure qualitative images kinect output depth output appearance motion vector field vector algorithm scene particle scene particle flow depth scene particle scene particle flow depth scene particle scene particle flow depth dataset cones cones cones cones cones cones teddy teddy teddy teddy teddy teddy venus venus venus venus venus venus optical flow stereo flow coverage scene particle motion monocular appearance compared multiple estimate structure compared optical flow depth approach outperforms optical flow depth allows accurate flow image estimation accuracy existing single view motion vector pixel projection smaller expected projection making motion optical flow depth approach performance standard algorithm ray resampling expected scene particles standard algorithm converge areas scene estimate accuracy scene ray resampling producing accurate estimates magnitude scene particle algorithm middlebury dataset single time optical flow depth based approach scene particles velocity smaller search space fewer scene particles spatial application view time work figure velocity search fraction maximum optical flow cones scene flow view high accuracy optical flow estimation based multi-view scene video dense accurate multi-view single camera scene flow tracking high accuracy velocity estimation motion dense motion video scene flow estimation stereo scene flow stereo stereo multi-view stereo reconstruction scene flow estimation global matching accurate motion field estimation stereo image estimating scene flow multiple optical range flow tracking reconstruction motion monocular video range flow estimation structure stereo scene dense flow sparse dense stereo scene flow structure image reduce search application cost number scene particles accuracy figure number particles scene particle algorithm outperforms existing exploring velocities maximum visible scene particles moving maximum velocity scene single visible exploring velocities range additional particles performance approach magnitude search ray resampling approach outperforms standard number scene scene particle approach motion estimation provide comparable performance current current fraction algorithm single making scene flow estimation system making modern depth sensor kinect stereo matching work scene particle algorithm application multi-view multi-view appearance multi-view depth appearance expected estimation motion field existing making accuracy magnitude
intelligent cost functions scene flow estimation intelligent cost functions simon hadfield richard vision processing motion estimation based assumption brightness constancy assumptions gradient common cost functions motion estimation demonstrate assumptions real functions simple ability learning nonlinear techniques machine demonstrate context nonlinear combination provide additional improvement performance scene flow estimation gains optical flow estimation scene flow optical motion field view paper simple fundamental scene flow underlying assumption brightness motion estimation fundamental computer multiple vision estimation scene flow depth dense motion estimation well brightness constancy assumptions lighting optical flow estimation occlusion estimation local optical flow fundamental estimating motion images scene flow estimation multiple response depth analysis commonly employed motion estimation metrics cost machine learning providing intelligent cost functions provide performance gains scene flow metric brightness inconsistencies intelligent cost functions motion estimation scene work motion estimation techniques generally local approaches work motion local contextual generally patch patch warping warping patch motions techniques motion estimation tracking local approaches provide performance areas scene contextual motion estimation techniques variational approaches work techniques optimization motion based total variation techniques well dense motion estimation motion total motion estimation based scene flow estimation variational approaches higher estimation local variational schemes generally optimization order motion field energy energy samples common encoding constancy assumptions motion estimation metrics serve energy samples field optical flow assumptions work scene multiple optical flow metrics number simple quadratic cost including norm robust function gaussian model sun brightness constancy assumption gradient constancy assumptions pixel learn cost function based combination standard cost sun response number linear previous analysis visual error true motion fields modelling scene analysis performed behaviour motion field estimated matching metrics accurate modelling ground truth visual correct motion fields metrics ability erroneous motion fields optical flow sensor responses work intelligent cost functions existing data matching approach areas common multi-scale estimation iterative warping schemes local energy approaches order scene model brightness estimation techniques robust paper number motion estimation metrics based machine learning robust metrics performance existing motion estimation matching metrics order range motion estimation metrics image pixel positions associated motion defined sensor associated function pixel positions sensor motion defined pixels supporting point sensor time point sensor time figure figure supporting pixel positions single supporting pixel set associated pixel values images supporting pixel values metrics metric determine deviation brightness constancy approaches norm norm intelligent cost functions robust function experiments norm cost functions paper supporting pixels equivalent appearance variance supplementary metrics note metrics applied well set supporting pixel values input metric based colour constancy input images gradient set supporting gradient values associated metric sqg based gradient constancy assumption complex commonly flow ofc metric supporting pixels time serve gradient image combination gradient images metric linearised brightness constancy assumption quadratic higher truth errors ideal metric provide cost true motions high cost incorrect cost error figure function responses ideal applied erroneous motion ideal case true motion field violation underlying constancy assumption pdf responses incorrect motions pdf note responses normalized figure response applied ground truth high error motion fields middlebury paper erroneous motion fields motion metrics behaviour directional ground truth motions occlusion responses erroneous linearised brightness constancy metric ofc overlap intelligent cost functions ideal ofc sqg ofcg figure ideal response pdf ground truth error motion distribution responses motion estimation applied ground truth error motion fields real responses normalized range gradient based metric sqg erroneous motions distribution version ground truth best separation simple brightness constancy metric performance error overlap response erroneous motions scene constancy assumption directional lighting metric response correct motions metric provide cost error estimated motion figure response metrics averaged scene motion error center figure change erroneous true motion relates true motion relates examine optimization schemes motion estimation gradient metric response order relates reduction motion brightness constancy display violation true despite overlap linearised brightness constancy ofc average correct multi-scale approaches commonly allowing gradient constancy metric sqg motions ground metric employed motion intelligent cost functions figure assumption violation motion including true violation averaged varying motion error intelligent cost functions motion estimation simple linear quadratic deviation appearance errors input metrics motion errors scene machine learning techniques intelligent matching robust appearance inconsistencies real inconsistencies motion matching function input motion features supporting pixels learning nonlinear metric complex expected image variation expected appearance behaviour serve responses change appearance colour change appearance erroneous gaussian employed model input features level motion complex estimating distribution set cost analysis model trained samples set scene flow sequences performed range approaches encoding visual features simple approach fvar single equivalent metric variance fvar approach pixel fdi intelligent cost functions fvar figure distribution responses applied ground truth erroneous fdi figure distribution responses icfs including contextual equivalent input difference metric allowing icf learn raw pixel values fpix figure performance based input range additional sequences supplementary note supplementary sequences trained middlebury sequences fvar features learning nonlinear additional separation case fdi icf input features ability nonlinear simple raw fpix encoding icfs motions well fdi despite including machine learning icf fpix note true difference feature fdi true pixel features fpix features colour encoding schemes lead metrics display reduction original metrics motions true response number fvar based icf despite local context addition allowing complex nonlinear supporting higher level features fdi fpm fdi fph fdi fpv fpix intelligent cost functions figure average response icfs context varying motion best standard metric rescaled version based local context local image supporting pixel motions motions local lighting supporting equivalent robust version fpix local supporting contextual additional experiments local context note contextual features techniques employed local motion estimation contextual pixels context icfs employed figure local variance feature fpv motion contextual features fpm proves pdf deviation overlap complex local features fph prove contextual number features determine figure average response varying motion response original brightness constancy icfs lead optimization based motion addition expected gains rescaled version best brightness constancy range equivalent metric high response pdf center rescaled metric better standard context based icfs provide robustness motion estimation icfs previous analysis standard motion estimation cost functions real robustness intelligent cost functions motion estimation techniques estimation iterative standard cost examine icfs accurate motion scene flow estimation icfs algorithm scene flow estimation cost performance pixels occlusion error motion error motion error error averaged sequences middlebury display gains existing techniques cost fvar improvement original simple nonlinear complex icfs provide performance difference features providing best contextual fdi fpm providing improvement improvement directional reduction runtime metrics additional cost icfs proves simple feature encoding schemes fpix prove original metric fvar fdi fpix fdi fpv fdi fph fdi fpm multiview multiview multiview multiview multiview multiview multiview runtime table performance scene flow estimation based original range runtime single optical flow estimation icfs intelligent metrics scene flow demonstrate trained icfs optical flow approach sun technique middlebury provide note case optical motion supporting features icfs table icf ofcg metric errors case standard error defined additional sequences supplementary icfs ability learn nonlinear linear nonlinear linear icf standard metric ofcg fvar fdi fpix fdi fpv fdi fph fdi fpm intelligent cost functions runtime secs secs secs secs secs secs secs table performance optical flow ofcg based approach sun range ofcg fpix ground truth figure motion fields original approach icf ground optical flow experiments icfs provide robust cost responses multiple raw pixel features fpix prove encoding contextual ofcg metric proves cost figure estimated flow note icfs lead local motion better center interesting note original technique modelling robustness icfs performance scene interesting icfs prove techniques icfs learning areas prove technique analysis performed motion estimation metrics scene flow previous metrics response true erroneous motions underlying constancy assumption incorrect true cost machine learning provide improvement separation true erroneous high response motion scene existing scene flow optical flow icfs energy lead joint data matching real addition complex patch based metrics normalized prove interesting examine icfs better high higher level determine icfs work visual intelligent cost functions algorithm visual international journal computer image simon richard optical international journal computer scene flow view variational june structure motion scene computer vision pattern recognition ieee conference june scene flow view variational international journal computer robust estimation optical computer international conference high optical flow estimation based local flow scene flow tracking volume june richard tracking pattern analysis machine ieee transactions andreas vision international journal simon hadfield richard scene international conference image analysis intelligent cost functions simon hadfield richard scene based scene flow ieee pattern analysis machine horst optical flow brightness pattern analysis machine ieee transactions optical variational scene flow estimation stereo october dense motion estimation optical flow data multi-scale scene flow stereo multi-scale scene flow stereo ieee motion volume volume iterative image technique stereo international joint conference colour learning optical pattern analysis machine ieee transactions image processing nonlinear volume accurate flow international journal computer thomas andreas accurate motion field estimation stereo image sequences intelligent cost functions gaussian machine estimating scene flow multiple optical international machine vision image processing motion estimation point international journal computer stereo depth ieee computer computer vision pattern volume range flow varying october learning optical october optical flow estimation computer vision pattern recognition ieee conference june motion pattern joint estimation structure stereo motion volume june dense scene flow dense stereo volume october andreas thomas horst algorithm optical approaches visual motion thomas horst motion estimation total variation computer vision pattern recognition ieee conference motion optical flow pattern analysis machine ieee transactions intelligent cost functions scene flow structure multiview image ieee transactions
ieee transactions pattern analysis machine march 2014 scene unregularized particle-based scene flow estimation senior ieee algorithm presented estimating scene optical approach operates magnitude faster techniques well performance algorithm multiple hypotheses motion traditional smoothness oversmoothing errors providing performance previous approach capable operating combination appearance depth estimating structure motion algorithm time resolve estimation approaches smoothing motion field benefits multiple hypotheses probabilistic approach occlusion estimation percent tracking approach estimate trajectories hands sign model appearance scene motion particle optical hand sign occlusion probabilistic bilateral motion segmentation techniques proposed multiview scene flow estimation combined appearance depth sensor current approaches scene flow estimation based generally optical flow viewpoints frames well optical flow estimation estimating scene approach ambiguity regularization term smoothness motion field regularization oversmoothing artifacts scene particles algorithm operating image sensors approach capable combination appearance depth sensors incorporating probabilistic occlusion awareness motion field smoothing provide analysis algorithms including robustness propagation current scene flow estimation techniques provided probabilistic scene flow estimation collection input sensors presented sparsity discussed approaches occlusion awareness motion smoothing presented sections accuracy algorithm ieee computer society flow applications dense motion field points scene flow incorporating structure well three-dimensional motion field projected image plane optical estimating scene flow ambiguity approach scene flow estimation proposed propagating multiple hypotheses time allowing future observations resolve estimating motion field structure scene number segmentation tracking gesture recognition approaches estimating scene motion field structure techniques matching sparse frames approaches limited scene flow multiple allow dense motion estimation applying scene flow estimation techniques depth authors centre signal university received number digital object 2014 ieee hadfield scene unregularized particle-based scene flow estimation estimated motion field motion appearance performance sequences benefits propagation discussed smoothing schemes performance algorithm sampling sparsity sensor noise sections example scene particle algorithm calculate hand trajectories sign work previous techniques estimating scene flow incorporating smoothness constraints structure authors tracking sparse number relating dense structure motion scene smoothing object discontinuities initialization motions changes generally estimate sparse scene flow proposed fitting initialization sparse number techniques proposed employing oversmoothing techniques regularization based local image reduce smoothing object lead performance highly employed zhang smoothness term data matching term approaches region oversmoothing discontinuities limited approach presented employing image estimating motions smoothness matching assume roughly constant cameras scene assumptions camera setup fitting parameters motion model segmentation approach oversmoothing artifacts small highly techniques scene particle algorithm based developed analysis approaches space loss reduced approach number additional scene particle algorithm operates continuous collection discrete basha developed highly structure motion scene particle algorithms single motion estimate method provided utilizing future scene particle algorithm smoothness assumptions single probabilistic scene flow three-dimensional set structure location continuous space referred vector continuous three-dimensional space vector velocity space scene flow estimation set structure points corresponding motions set input observations scene probability distribution provide estimate structure motion estimate single optical providing estimate note set images number appearance depth case sensors changes technique directly long scene probability distribution frame set estimated ieee transactions pattern analysis machine march 2014 prior probability propagating distribution previous constant velocity motion transition defined frame likelihood defined terms based appearance depth appearance sensors observations appearance sensors probability structure point estimated color point assume true color point average defined image error equivalent projected color sensors previous current frame cost likelihood projection function camera pixel location current structure previous likelihood current values note employed cost function traditional gaussian model motion estimation set data note parameters modified appearance depth note sensor term reduced entirely remaining sensor set defined approach combination scene particles space loss large number samples simple setup cameras leads structure equivalent number discrete subset weighted samples particle filtering approaches allows continuous probabilistic remaining scene particles spatial location velocity scene particle weight scene depth sensors depth ambiguity depth sensors likelihood observations collection depth likelihood true structural point well depth back-projection previous depth average position sensors back-projection calculated projection function depth sensor corresponding back-projection back-projection pixel position depth three-dimensional vector vector corresponding structure observations scene particles previous frame constant velocity motion prior probability allowing propagation scene flow long processing greater single frame density scene particles combined leads system frame randomly uniformly scene particle cloud particle filtering resampling scheme employed hypotheses regions probability residual resampling scheme scene particle population previous population probability scene particle equal observations depth sensors additional performed scene reduction spatial scene particle calculated depth sensors subset sensors valid depth estimate projected location randomly provide spatial location case sensors structure process particle population hadfield scene unregularized particle-based scene flow estimation iterative estimation set observations likelihood estimated scales scene particles small local maxima scale resampling population gaussian aid additional iterative estimation performed level gaussian diffusion allows particles transition standard particle filtering relates scene particle large collection leads well particle single reduce resampling scheme modified technique ray resampling particle population based optical particles standard residual resampling applied partition system particles created particle partition population particles normalized weight samples remaining based residual particles partition process applied input particle multiple resampling despite population size number samples partition particles approach resampling scene equal number scene scene particle cloud property dense scene flow stereo reconstruction dense ray resampling particle filter ray allowing hypotheses multiple particle transition resampled population particles uniformly prior entirely density presented prior sampling density lead standard scene particles optical scene particles randomly occlusion employing multiple large large scene visible subset estimate region suffer occlusion sensor frame current prior probability high occlusion viewing ray sensor region point high prior details visibility probability distribution viewing ray occlusion prior produce performed current system estimated prior previous limited probability ray prior distribution continuous discrete visibility probability relates weight comparison weight points visibility probability created note previous approaches motion estimation points occlusion entirely occlusions consistency employed likelihood valid presented occlusions estimated structure motion points labeled scene flow likelihood modified increased consistency viewpoints visibility probability ieee transactions pattern analysis machine march 2014 visibility probability discussed case continuous discrete discrete example scene particles varying note visibility probability particle weight greater particles applied structural likelihood scene flow estimate visibility iterative system points visible single data consistency additional based visibility points visibility constraints relating constraints scene hypotheses greater average visibility three images set motion smoothed remaining gaussian function standard deviation motion smoothing multiple hypotheses scene particles algorithm allows frames aid smoothness produce artifacts smoothness motion field valid number techniques benefits post filtering approach previous authors smoothing image plane projection referred image created based scene particle cloud camera pixel weighted average values scene particles note filtering scene particle estimated motion field errors smoothing oversmoothing structure motion bilateral filtering technique bilateral filtering vision bilateral filter gaussian smoothing additional based values pixel produced image bilateral filtering reduces smoothing artifacts compared simple gaussian weighted relates size discontinuities estimated discontinuities standard suffer object segmentation details standard deviation three-dimensional post filtering filtering projection filter scene directly image plane allows smoothness constraints small region details scene particles assumptions motion structural smoothness smoothed scene particle scene particle set scene particles particles spatial standard smoothed motion hadfield scene unregularized particle-based scene flow estimation example frames scene flow estimation appearance depth flow flow relates hands relates algorithm algorithm scene scales diffusion level particles occlusion calculate occlusion calculate weight ray ray ray ray population particles ray population particles ray population scene particles algorithm operating single frame provided algorithm computation performed scale diffusion visibility calculated particles ray resampling diffusion performed iterative estimation particles image particle population constant velocity motion model produce prior scene flow estimation discussed additional data sets including long sequences including appearance depth computer society digital small number example frames presented experiments performed scene particles particle filter equal percent maximum visible standard deviation gaussian diffusion resampling equal percent maximum visible percent ieee transactions pattern analysis machine march 2014 table scene flow estimation combined depth appearance sensor system scene particles ray resampling compared combined optical flow depth estimation discussion image note parameters maximum velocity aid estimated middlebury data sets developed stereo reconstruction demonstrates performance algorithm data performance synthetic data leads set data multiview sphere basha data set simple synthetic motion estimation current scene flow estimation techniques process generally analyzed single number frame sequences middlebury data images frames sequences images frames allows equivalent set cameras viewing scene point motion field simple projection images techniques sections prior including scene particle cloud motion pixel weighted average motion scene particles optical depth sensors ground truth structure number image basha errors middlebury data sets analyzed image average rms error optical flow disparity disparity flow absolute error measures directional accuracy error measures previous occlusion regions allow comparison basha errors sphere analyzed normalized rms errors position velocity magnitude velocity error normalized errors equivalent standard rms presented ground truth motions maximum ground truth proposed error measures space presented reconstruction accurate scale data sets rms error scene flow current scene flow estimation algorithm capable utilizing observations appearance depth comparison optical flow algorithm applied appearance combined depth data technique referred experiments labeled table depth observations produced ground truth depth observations hadfield scene unregularized particle-based scene flow estimation ground truth estimated images scene structure motion sphere data scene particle algorithm appearance sensors produced matching stereo algorithm depth maps lower quality observations true depth estimated depth maps stereo initialization approaches motion accuracy scene particles algorithm greater ofd note despite ofd utilizing optical flow motion magnitude error image plane implies depth data allows accurate flow ofd algorithm greater scene coverage standard scene percent coverage ray resampled scene ofd technique robustness quality depth quality directional reduction accuracy regions stereo reconstruction regions optical flow errors assume standard scene particle algorithm lead particles regions high reduced coverage increased performance standard resampled ray resampled approaches roughly implies subset local maxima reduced appearance scene flow traditional motion estimation scene particles algorithm suffer reduced accuracy scene particles leads low levels uniformly property applications object analyzed single propagation accuracy oversmoothing performance table three additional appearance sensors loss depth sensor motion accuracy improvement table depth observations structural estimation accuracy scene particle algorithm motion magnitude previous image plane directional estimation accuracy lower scene particles motion suffer low level noise terms motion noise generally low motion small absolute motion leads large probabilistic occlusion approach motion magnitude ieee transactions pattern analysis machine march 2014 table scene flow estimation appearance setup scene particles occlusion awareness compared discussion runtime computation single provided scene computation time provided basha standard scene particles approach operates faster previous techniques comparison previous regularization terms cost function approach matching function viewpoints single table analyze scene particle smoothing techniques discussed propagation scene particle algorithm magnitude faster previous analyze performance algorithm propagation time scene particles algorithm performance scene particles algorithm sequences varying single additional frame sequences reduction directional motion magnitude improvement additional frames motion hypotheses sets lower performance regions prior constant velocity motion model prior smoothing number approaches incorporating smoothness constraints scene flow estimation analysis error measurements sphere data set sequences experiments performed single appearance depth hadfield scene unregularized particle-based scene flow estimation table performance scene particles algorithm incorporating smoothness constraints performed scene appearance discussion patch-based matching post filtering improvement terms motion magnitude applying post magnitude directional accuracy cost computation bilateral post filtering techniques applied motion structural performance patch-based matching reduces structural estimation accuracy levels previous approaches table directional performance levels implies global regularization scheme local patch-based demonstrates smoothness constraints current motion estimation sampling sparsity property scene particle algorithm varying size scene particle population accuracy computation hypotheses time sampling probability maxima runtime varying number hypotheses ray performance sphere data set varying number single appearance depth three error function noise salt pepper noise gaussian noise performed single appearance depth sensor sphere data ieee transactions pattern analysis machine march 2014 frame discussion number hypotheses ray error measurements minutes directional accuracy estimation analyzed motion magnitude errors number relating minutes scene particles algorithm magnitude faster smoothing applied image scales reduces gaussian three-dimensional object tracking robustness analyze robustness input appearance depth images corrupted varying gaussian pixel corrupted gaussian performance analyzed standard deviation salt pepper noise varying number randomly set viewpoints algorithm well salt pepper error measurements corrupted scene global performance system gaussian noise generally absolute gaussian noise estimated motion salt pepper standard deviation values equivalent percent noise approach table agreement projection estimated trajectories system application accurate scene flow estimation object tracking scene particle cloud approach tracking allows tracking prior object approach applied tracking hands sign language sequences minutes sign language hands changes tracking motion field tracking previous approaches color assumptions provide segmentation color model reduce cost number scene particles estimate motion tracking algorithm applied large multiview sign language data provided data camera parameters estimated collection labeled technique degree robustness object projected input note tracking performed projected sensor performed table performance approach analyzed compared accurate frame labeled agreement estimated high level agreement tracking schemes demonstrates hadfield scene unregularized particle-based scene flow estimation note agreement tracking directly provide accuracy lower performance error tracking frames data set relating sign language traditional scene flow estimation application scale roughly applications motion scene particles approach scene flow estimation demonstrated provide accurate motion traditional reduced techniques demonstrated applying smoothness constraints motion field benefits multiple providing increased method estimating occlusion maps scene flow estimation cost increased analysis scene particle algorithm propagating time resolve estimating frame runtime accuracy varying number hypotheses robustness algorithm noise example scene flow estimation traditional vision application scene flow field location velocity approach applied hand tracking sign language demonstrated provide despite occlusions appearance example scene particles future work provide allow accurate scene flow estimation performed work presented european agreement flow estimation segmentation fitting ieee hadfield hand trajectories scene ninth image analysis recognition optical flow gesture ieee gesture recognition object structure motion motion ieee computer vision single ninth ieee computer vision multiple maps ieee scene ieee computer vision motion ieee computer vision pattern recognition scene ieee pattern analysis machine flow computer vision image flow varying 10th european computer vision flow varying algorithms ieee pattern analysis machine british machine vision dense scene flow sparse dense stereo 10th european computer vision scene flow computation motion computer estimation ninth ieee computer vision stereo reconstruction scene flow estimation global matching computer accurate motion field estimation stereo image sequences 11th european computer vision hadfield particle based scene flow depth ieee computer vision stereo computer motion video ieee computer vision pattern recognition accurate ninth computer vision scene flow tracking points ieee computer vision pattern recognition scene video motion ieee computer vision ieee transactions pattern analysis machine march 2014 zhang scene flow structure multiview image ieee computer vision pattern recognition motion disparity estimation computer vision scene flow stereo ieee workshop motion video zhang scene flow structure ieee computer vision pattern recognition zhang scene flow structure multiview image ieee scene flow stereo computer vision image scene flow multiple optical machine vision image processing motion scene ieee computer vision pattern recognition optical 10th european computer vision approach ieee signal density propagation computer particle resampling schemes particle image signal processing variational method scene flow estimation stereo 11th ieee computer vision scene flow variational computer tracking reconstruction motion video phd estimation motion structure stereo 11th european computer vision flow estimation pattern stereo depth maps ieee computer vision pattern recognition stereo motion synthetic stereo image vision optical matching variational motion ieee pattern analysis machine processing matching ieee pattern analysis machine tracking ieee language workshop processing sign sign language workshop gesture hadfield received meng degree computer phd degree university received meng performance meng university centre signal university motion gesture member received degree computer science university degree university phd degree computer vision university received phd computer vision machine university leads vision centre signal processing society senior computer vision tracking received british science science member british machine vision member senior member member ieee computer digital
ieee international conference robotics automation rgb-d dense 3-d motion estimation color depth 3-d motion estimation problem scene flow formulation assumptions scene object rgb-d provide computing dense 3-d flow work variational 2-d flow algorithms scene flow rgb-d depth previous variety dense 3-d flow rigid motion estimate motion person object object motion estimation assumptions scenes object objects scene tracking frame robust techniques problems person tracking tracking number rigid objects moving scene small moving objects scene 2-d motion robotics moving objects moving objects color features problems articulated objects point features techniques objects paper motion estimation general number moving introduce rgb-d scene flow algorithm dense depth color estimate 3-d motion point rgb-d rgb-d flow algorithms point features linearized flow constraints rgb-d low optical flow algorithms robust nonlinearized data constraints variational university computer optical flow dense 3-d motion estimation variety scenes human robot multiple moving algorithm dense motion handling small large motions motion small better previous dense flow rigid motion flow multiple objects vision robot paper introduce optical scene flow objective scene flow perform motion segmentation flow estimation dense motion estimation computer vision optical flow scene optical flow motion image scene flow translation estimated vector fields pixel optical flow problem brox variational optimization commonly flow estimation local approach bruhn brox local differential optical flow large variational optical flow rgb-d color depth term estimate 3-d flow color depth data estimating optical approximation depth introduce depth brightness constancy called range flow scene flow techniques add occlusion add local scene flow stereo sparse depth dense depth flow add color depth flow ieee dense depth computing flow color letouzey flow computation laplacian points 3-d image method large objects color optical flow approach 3-d estimate surface sophisticated surface variational approach 2-d robust avoid work regularization weights depth motion segmentation point color images sampling depth approach algorithm field perform data motion estimation object segmentation problems object estimating 3-d motion points scene rgb-d estimate motion optical flow scene flow optical flow estimates translation image pixel image motion vector measured scene flow estimates translation 3-d scene flow maps image measured image measured scene flow formulation scene making handling moving multiple moving objects well sparse features motion points motion segmentation dense tracking differential flow small large motions computing flow images flow formulation brox variational flow data image data images smoothness include data terms color depth well smoothness color consistency optical flow methods brightness color object pair images brightness constancy linearized form constraint linear called optical flow constraint equation optical flow nonlinearized brightness accurate flow linearized nonlinearized color consistency formulation optical flow quadratic penalty equation optical flow data optical flow data robust data term charbonnier penalty approximation better quadratic constancy brightness robust brightness values color data term depth consistency data term depth color measured depth point constraint change depth range flow constraint equation depth maps nonlinearized depth consistency term color consistency point moves depth motion change measured color robust charbonnier weights depth data term color data measure uncertainty measured depth stereo depth quadratic regularization regularization variational flow flow problem commonly optical flow constraint point moves flow algorithms accurate motion estimates local regularization term quadratic penalty smoothness term small flow motion large methods robust commonly regularization isotropic regularizer gradient flow charbonnier isotropic regularization term rgb-d data optical flow smoothness term scene flow depth term color frame depth maps gradient flow field laplacian flow image laplacian weights pixels isotropic regularizer work well optical flow find anisotropic smoothing takes local gradient image depth surface commonly anisotropic regularizer smoothing rgb-d flow method letouzey laplacian point weights depth color surface object pair points avoid smoothing regularizer motion segmentation regularizer points formulation color surface terms formulation regularization flow points points local include small quadratic penalty order large small find robust data smoothness large flow values objective color surface laplacian regularizer flow magnitude pixels variational rgb-d flow rgb-d low variational optical flow techniques brox euler-lagrange flow euler-lagrange equation 3-d points called methods optimize objective quadratic smoothness term pixels objective takes form laplacian flow gradient regularizer takes form making sor sor num denom num denom optimization magnitude derivatives depth general largest derivatives largest image image edges depth depth edges large derivatives smoothness assumptions depth data term methods data terms limiting magnitude depth slows convergence flow image work objects object general objects robot perform motion segmentation estimate object rigid object segmentation motion scene method sampling large number point fitting rigid motion scene candidate field scene add smoothness algorithm motion large number method small number candidate motions find candidate step motion segmentation large number motions seed points motion fitting point order avoid previous work point scene motion candidate limiting point pixel single point rigid point points algorithm motion estimation image laplacian bruhn laplacian terms euler-lagrange equation approximation sor step num denom num denom optimize optimization small change flow sparse linear smoothing image flow field making scene flow computation pixels small motion flow estimation takes convergence number smoothing slows flow magnitude penalty find large penalty slows optical flow scene moves segmentation work scene flow single candidate point optical scene accurate pixel problems flow smoothness small scene flow find point perform motion scene flow pixel scene flow algorithm rigid motion seed point step general fitting map single motion point smoothness scene flow method variety data scene flow algorithms videos frame pair comparison algorithm estimates scene flow estimating optical flow color scene flow sampling depth map image sophisticated scene flow method depth map depth sampling rgb-d algorithm method quadratic data smoothness method optical z-flow robust penalty flow low isotropic smoothing anisotropic smoothing algorithm isotropic smoothing method values column color column estimated flow middlebury color coding column images maximum include xy-flow fields middlebury frame pairs human comparison method letouzey data rgb-d low anisotropic smoothing method sophisticated regularization robust penalty rgb-d low anisotropic regularization frame pairs rigid number motions second scene method data color depth images depth images 2-d number pixels data depth depth uncertainty method better z-flow method videos quadratic flow anisotropic smoothing frame pair outline anisotropic isotropic second frame outline robot frame person flow frame pair anisotropic regularizer outline scenes rgb-d low frame pair smoothing optimize convergence number sparse linear algorithm sor limiting find provide smoothing videos scene flow second dense depth estimation scene 3-d translation scene scene flow motion objects scene flow estimation algorithm rgb-d largest problem algorithm occlusion pixels large estimated order frame frame frame frame frame frame color depth xy-flow z-flow xy-flow z-flow xy-flow z-flow rgb-d rgb-d isotropic smoothing rgb-d anisotropic smoothing xy-flow colors scene flow frame rigid objects rigid objects robot human pixels depth images column frame pair second column middlebury optical flow color coding z-flow colors maximum flow pixels maximum data frame pair pixels edges large formulation occlusion handling scene flow optimization rgb-d objective formulation uncertainty estimated flow sampling step rigid motion fitting seed points low flow work multiple measure uncertainty optical scene scene flow object segmentation objects differential motion estimation provide motion estimation previous work work computing rgb-d isotropic smoothing rgb-d anisotropic smoothing scene flow frame pair second middlebury optical flow color coding z-flow colors maximum flow pixels comparison include paper color rigid object segmentation frame pairs motions human recognition single depth ieee computer society conference computer vision pattern recognition object tracking object international journal robotics moving object motion ieee international conference robotics automation articulated international conference intelligence segmentation articulated objects ieee international conference robotics automation flow depth color machine vision conference flow tracking depth range flow international computing optical flow estimation conference computer vision optical image stereo image local flow international journal computer brox optical variational motion ieee pattern machine intelligence scene international conference computer vision flow variational method scene flow estimation stereo international conference computer vision scene flow estimation rigid motion international conference computer vision machine vision conference tracking point university scene flow depth international conference computer vision scene flow computation motion international journal computer object 3-d scene ieee international conference robotics automation smoothness constraints estimation vector fields image ieee pattern machine intelligence image local color segmentation motion dense method 3-d motion ieee computer society conference computer vision pattern recognition optical ieee computer society conference computer vision pattern recognition confidence ieee computer society conference computer vision pattern recognition bruhn confidence measure variational optical flow university confidence measure optical conference computer vision
computer dof scene flow rgb-d pairs microsoft approach computing dense scene flow pair consecutive rgb-d availability depth data patches pixels points inliers spheres contribution reasoning terms patches dof rigid body motions obtaining displacements large assumptions brightness constancy local surface output dense field rigid body contrast scene reasoning manner additionally allows carry occlusion handling dof consistency check flow computed patchwise silhouette check help reason occlusion promote flow fields intuitive local rigidity carry obtaining correspondence field -expansion occlusions flow challenging synthetic real-world assumptions brightness constancy local surface assumption corresponding pixels color assumption scene displacements alternative patches methods motion local surface patches geometry contrast scene flow availability points rgb-d data reasoning terms point reason points constituting patches inliers spheres patches dof rigid body local surface planarity patches non-planar surfaces traditional motion introduce dof consistency check flow recovered introduce patchwise silhouette check help reason occlusion introduce intuitive local rigidity prior promote approach output dense field dof rigid body translation vectors scene vedula introduced dense field translation vectors point scene flow techniques rgb images input recover structure rigid sparse structure dof motion structure motion rgb scene flow methods huguet devernay basha vogel basha point directly assumptions directly scene flow growing availability rgb-d microsoft kinect computer color depth computing dense motion field pair consecutive rgb-d frames traditional microsoft point figure point patches geometry hartley rigid body motion local planarity assumption non-planar surfaces large patch points penguin penguin brightness constancy assumption compute scene flow stereo pairs growing correspondence vogel carry regularization rigid motion field rigid motion introduce variational estimation directly vector field input images scene set surfel recover surfel parameters relative camera parameters devernay local surface planarity vogel assign image introduce range flow recover motion depth sparse motion rgb-d data sparse points help motion field estimation displacements dense normal flow estimation steps dense variational approach hadfield bowden brightness herbst compute rgb-d scene flow variational rigid motion quiroga variational local compute rgb-d bleyer detail step point patches matching cost aim detail second step correspondence fields handling occlusions dof point patches carry correspondence search reasoning terms point dof rigid body motions matching cost formulation image gradients gradients matching brightness point patch formulation allows alignment gradients non-planar surfaces traditional motion local surface planarity figure formulation allows compare depth manner traditional motion recognizing object boundaries encoded depth noisy additionally adaptive support weighting matching formulation number inlier points constituting point patch corresponding depth figure uniform matching denote dof rigid body motion input point patch denote set points encoded depth map radius point depth encoded pixel algorithm obtaining dense dof correspondence field second patchmatch algorithm depth map noisy compute color distance adaptive support weighting color denote nearest point set compute point similarity figure spatial point fixed sphere radius spatial function sphere radius function depth spheres expected uniform number inlier points fixed uniform matching camera hartley assign rigid body motion valid pixel source view destination pixel valid depth encoded input depth radius distance space points depth projecting neighboring pixels image space number points constituting point patch fixed radius expected depth figure matching order problem reasoning terms fixed radius radius pixel rpix denotes set points encoded depth map destination reasoning terms nearest point similarity allows handling pixels projection allows boundaries depth object boundaries encoded input depth maps compute final matching cost valid fixed dense dof matching patchmatch patchmatch carry dense dof patchmatch introduced depth introduced bleyer assign rigid body motion valid pixel point patch source view destination semi-random valid spatial additional semi-random view steps candidate rigid body motion equal lesser matching pixels upper left lower lower upper left contribution stereo scene promote view propagation pixels second view opposite steps detail semi-random valid pixel source point destination view maximum search radius translation vector candidate matching image dof computing rotation surface normal vector choosing random angular rpix fixed radius sphere equal viewpoint spheres expected uniform number inlier points fixed matching source destination color denote projection source destination compute similarity projection image form adaptive support weighting object boundaries spatial upper left lower rigid body motions adopt motion equal lesser matching lower upper spatial propagation dof rigid body corresponding pair point patches aim translation neighboring point destination view rotation destination normal rotation adopt candidate motion equal lesser matching reduce range view step pixel project nearest integer pixel adopt equal lesser matching carry patchmatch pixels opposite pixel view bleyer form view propagation figure dof consistency check project nearest integer pixel destination assigned motion centered point check space three carry steps consistency motion pixel consistency check disparity stereo bleyer project nearest integer pixel destination view fail check valid distance image space projection source view check distance space corresponding pixel displacement image space depth check additionally fail check vectors set zmed zmed depth alternative choosing geometry provided figure fail check order matching initial valid pixel passed consistency check assigned valid pixel failed consistency assign assigned pixel denotes set points set valid pixels passed consistency check view silhouette pixels destination view fail consistency matching cost function patches promote patchwise alignment depth silhouette check occlusion handling regularization dense correspondence search algorithm points expected fail occlusions introduce dof consistency check aim occlusion handling assign valid pixel failed check motion nearest point corresponding pixels carry regularization motion reduce labeling problem -expansion unary pairwise recognizing pixels failed consistency unary matching introduce silhouette check promote patchwise alignment viewpoint introduce intuitive local rigidity prior pairwise contrast destination view patchmatch matching cost check consistency check failed form glx silhouette check silhouette check pairwise pairwise reduce point computed motions glx glx source view figure silhouette check check passed points project pixel mask destination view corresponding inlier points sphere radius centered figure local rigidity pixel neighboring pixel rigid body motions glx glx corresponding labels prior space three passed points project pixel mask corresponding points opposite view inliers sphere radius centered figure mask order check noisy object regularization energy set fixed weight valid rpix zmed pixels corresponding points inliers sphere depth zmed manner alternative choosing maximum set formulation local motions glx glx expected points spatial geometry local rigidity prior figure energy -expansion algorithm compute labels random set pixels consistency check initial evaluation rgb-d scene quantitative evaluation huguet devernay color images ground truth disparity maps frames middlebury venus stereo data compare ground truth middlebury challenging synthetic real-world data denotes set pixels image set pixel denotes unary pairwise pixels set consistency check passed motion assigned dense matching form glx consistency check assigned motions consistency recognizing quantitative evaluation scene motion venus data sets motion camera translational camera motion matching problem occlusions geometry varying compare huguet devernay basha rgb images point error angular error optical flow vectors projecting output displacements image hadfield bowden additionally disparity error scene flow hadfield bowden quiroga compute numbers rgb optical flow techniques brox method scene flow quantitative numbers additional provided table bleyer fixed set kinect data large output sphere inlier pixel silhouette check pixels kinect data kinect depth labels computing dense dof scene flow pair consecutive rgb-d brightness constancy local surface planarity contribution reason terms patches points inliers dof rigid body middlebury venus data sets challenging synthetic real-world evaluation visualize recovered correspondence fields projecting displacements optical flow vectors manner figure intermediate final ground truth large displacement kinect data sets varying compare rgb-d scene flow techniques hadfield bowden herbst quiroga figure visualize manner vedula points frames final data set figure intermediate points visual recovered figure pairs data flow non-planar geometry large evaluation optical correspondence algorithm image patchmatch correspondence multi-view scene flow view centered variational patchmatch stereo stereo matching support energy brox large displacement optical matching variational motion multi-view scene surfel scene flow points estimation hadfield scene scene flow algorithm parameters radius rpix set set number kinect search radius set maximum displacement data function point weight set terms maximum relative maximum zmed depth relative weight set adaptive support weighting additional evaluation provided hadfield matching initial labeling final output ground truth matching initial labeling final output ground truth figure middlebury optical flow projection displacements image left numbers table left figure matching initial labeling final output herbst quiroga figure large displacement motion large algorithm quiroga fail motion herbst output quiroga matching initial labeling final output herbst quiroga figure large displacement motion motion pixels output quiroga matching initial labeling final output herbst quiroga figure large displacement motion three fail motion visualize figure visual recovered figure final kinect data set figure flow points encoded input frames point manner vedula rms-of brox basha huguet devernay hadfield bowden quiroga method rms-vz aae rms-of rms-vz aae rms-of venus rms-vz aae table quantitative evaluation rms-of point rms-vz aae angular methods rgb optical flow rgb scene flow compute translational flow depth three rgb-d scene flow computed valid disparity pixel middlebury occlusion rms-vz computed translational flow depth encoded points flow optical flow pixels recovered flow pixels disparity point depth hadfield additionally pixels flow recovered assumptions brightness constancy local surface figure large displacement synthetic data set planarity patch fail algorithm consecutive penguin pairs visualize viewpoint intermediate point points hartley view geometry computer second rgb-d dense motion estimation color depth rigid body huguet variational method scene flow estimation stereo random field scene flow depth color varying camera multi-view stereo scene flow estimation matching scene flow stereo depth maps range flow scene flow estimation growing correspondence scene scene flow estimation rigid motion rigid scene dense scene flow sparse dense stereo motion detail optical flow adaptive approach visual correspondence scene flow structure
variational method scene flow estimation stereo sequences paper method scene flow estimation stereo image scene flow 3-d displacement field scene optical flow scene flow propose scene flow optical flow estimation cameras dense stereo matching reducing image variational properly handle discontinuities observed surfaces 3-d displacement approach handles occlusions optical flow partial differential equations system optical flow solved previous variational methods 3-d reconstruction time scene flow method numerical synthetic data ground truth accuracy scene flow camera optical flow computation presented real stereo sequence large motion stereo code sample data evaluation parametrization scene flow optical flow disparity stereo image sequence scene flow disparity flow parametrization image problem optical flow estimation problem image variational methods compute optical flow work methods regularization term order discontinuities optical flow work reducing variational performance implementation terms accuracy brox avoid linearization energy terms variational formulation warping image time image time global energy inside energy data term method robust illumination handles discontinuities work variational formulation estimate dense disparity estimation scene flow variational method reconstruction scene flow estimation proposed scene flow estimation reconstruction 3-d motion energy account difference images computed 3-d propose joint estimation disparity motion method illumination occlusions work computes scene flow 3-d vector field surfaces motion 3-d point time optical flow optical flow 3-d scene flow vector methods propose scene flow observed optical flow cameras reconstruction cameras ieee disparity flow propose method computes scene flow joint estimation motion field image method account constraint images parametrization scene reference stereo disparity time optical disparity time 3-d scene flow computed set highly non-linear partial differential equations solved multi-resolution method linearization energy brox linearization optical flow constraint regularization terms handle discontinuities reconstruction motion paper mathematical formulation optical flow terms energy numerical global numerical synthetic real stereo sequences associated ground real stereo sequence associated scene large motion stereo time time displacement vector left image time time displacement time displacement time point corresponds reference functions time 3-d reconstruction scene point observed time reconstruction time scene flow computed difference time stereo left flow flow time stereo figure motion scene point time stereo associated global energy data term regularization edata esmooth variational formulation optical flow stereo estimate dense scene preserving surfaces motion zhang problem solved regularization stereo image image stereo disparity applied images order avoid numerical method numerical work brox changes illumination image gradient stereo optical flow occlusions regularization left image sequences optical flow left disparity maps regularization edata corresponding images edata est occluded pixels left optical flow functions occlusions associated edata difference illumination image terms edata est data term corresponding left optical corresponds optical vertical component left optical corresponds stereo matching left images time est corresponds stereo matching time pixels left image occluded three solve function robust corresponding function applied data pixels occluded optical gradient data terms energy robust illumination changes surfaces stereo terms highly images coming parameter set illumination expected disparity time previous scene flow estimation time time disparity data components scene 3-d reconstruction 3-d motion field regularization term esmooth scene esmooth typical expected 3-d scene typical size typical motion small respect size small euler-lagrange equations energy euler-lagrange euler-lagrange equations local multi-resolution approach equations computed variational image previous time reducing optical flow disparity global function preserving discontinuities functions data applied gradient discontinuities disparity optical flow disparity flow synthetic example regularization 3-d scene flow motion field respect parameter set properly optical flow disparity avoid cameras scene stereo parameter disparity flow discontinuities parameter initial disparity optical typical discontinuities terms observed computing data coming edata term three equations coming problem image system partial differential functions highly equations scene flow numerical solution energy optical flow constraint data term term euler lagrange problem gradient energy order solve highly non-linear differential multi-resolution iterations resolution method proposed brox solve optical flow stereo image computed pyramid multi-resolution approach global algorithm work performance data term euler lagrange image image computed respect reference image time warping three images time pyramid level image time solution computing data terms images equations pyramid level fixed point order euler lagrange equations inside fixed point iterations compute small solution images fixed point iterations full solution full compute fixed point iterations euler lagrange optical flow inside fixed point iterations method solve system system three system implementation image pixels order optical flow scene flow numerical fixed point iterations fixed point fixed point pyramid solution resolution full resolution occlusions computation occlusions computing functions fixed point account compute occlusions functions pixels occluded pixels occluded regularization term functions computed disparity image disparity map left image compute occlusion map disparity edata reference image full algorithm problem solve non-linear initialized order avoid local optical flow problem multi-resolution resolution small optical flow initialized optical flow small compared image image resolution optical flow global scene flow optical flow camera solve stereo problem left stereo problem optical stereo disparity image size size occluded multi-resolution stereo method scene flow algorithm functions stereo algorithm computes disparity resolution images disparity error stereo algorithm evaluated standard compute pyramid level disparity error compute pyramid level expected optical flow level solve optical flow problem terms left optical left images level level left optical flow optical flow initial disparity level level method terms stereo time initialized difference warping time algorithm scene flow estimation algorithm applied level level full scene flow estimation algorithm algorithm full scene flow estimation compute scene flow stereo pyramid left optical flow level optical flow level stereo disparity time level scene flow level evaluation datasets ground truth computer scene flow problem evaluation datasets scene optical flow standard benchmark optical flow sequence flow ground time camera camera occluded stereo datasets optical images images benchmark scene flow set cameras optical images stereo benchmark scene flow optical flow disparity maps algorithm images teddy cones datasets stereo pair time images stereo pair time ground truth disparity optical flow order algorithm scene synthetic images rotating sphere scene 3-d reconstruction scene sphere reconstruction rotating discontinuity scene method properly evaluation computing error maps optical flow maps evaluated disparity maps evaluated disparity disparity angular error optical flow components scene compared optical flow computed method maps discontinuity properly occlusion real stereo pair sequence large motion evaluation figure maps example vertical discontinuity reference left image time occlusion maps data terms corresponding left disparity disparity figure synthetic sample scene rotating image 3-d reconstruction 3-d motion scene scene flow discontinuity vertical teddy cones sphere method optical flow estimation dense stereo matching global method handles discontinuities 3-d 3-d motion vector robust changes handles occlusions optical flow method work brox optical flow numerical solution solve disparity map optical occlusions disparity size stereo proposed initial solution optical flow stereo initial solution scene flow estimation paper scene flow evaluation optic flow component scene flow optical flow method method handle real stereo sequences large motion stereo mathematical work work variational methods solve scene flow estimate function previous work formulation approach variational figure error pixels maps computed scene flow algorithm teddy cones sphere figure standard angular error optical flow component scene compared angular error optical flow computed implementation sphere datasets stereo sample code compute optical scene approach presented presented variational compute scene flow image code figure example real data time stereo pair illumination large motion motion discontinuity mouth pixels scene flow components sample set figure sample real vertical flow component scene flow mouth discontinuity image time stereo pair time left image time scene flow discontinuity mouth scene flow dense motion disparity estimation image optical flow local joint image ieee variational scene flow estimation ieee evaluation dense stereo computer optic flow variational method dense disparity approach large displacement optical flow occlusion scene ieee scene ieee zhang scene flow image ieee zhang scene flow ieee mathematical optical flow problem performance optical flow ieee optical accuracy optical flow estimation variational optical flow computation real ieee image discontinuity preserving computation variational optic flow estimation preserving variational computer disparity flow estimation variational method scene flow estimation stereo
int comput vis consistent binocular depth scene flow chained temporal profiles hung 2012 2012 2012 propose depth image scene flow estimation method input binocular temporal consistency making computation long sequences tackle number fundamental including motion structure consistency multiple longrange temporal constraint error depth scene flow estimation main motion link frame correspondences outliers temporal robust novel edge occurrence map introduction anisotropic smoothing priors proper video depth estimation consistent scene flow chained temporal profiles stereo matching introduction computer reliable depth motion estimation quality result supplementary supplementary hung computer university hong hong hung depth videos temporal challenging video well scene common number binocular videos difficult compute reliable consistent depth long multi-view stereo matching applied static global constraints established multi-view geometry zhang suitable videos moving correspondence multiple optical flow estimation correspondence consecutive frames optical flow depth variation jointly zhang kambhamettu huguet devernay wedel typically image scene scene flow constructed wedel methods compute motion depth tackle problem multiple stereoscopic video int comput vis local temporal constraints depth estimation video stereo matching methods object motion multiple frames suitable static scene optical flow methods black bruhn weickert temporal main estimation difficult perform long range erroneous estimates caused occasional sudden outliers frame influence effective measure reduce estimation errors reliable depth motion estimation multi-frame binocular videos temporal method based global multi-view geometry dynamic objects locally established temporal constraint major construct measure reduce estimation errors multiple propose motion trajectories link reliable corresponding points robust occasional noise build structure profiles considering multi-frame voting-like edges reliable multiple frames long-range temporal constraints based robust motion regression errors estimates propose anisotropic smoothing incorporating temporal edge boundary work depth motion estimation stereo images zhang wedel motion depth computed assuming depth previous frames improve depth motion jointly stereo zhang kambhamettu huguet devernay estimate motion fields constraints established temporal consistency wedel disparity motion field computation optimization scene flow field computed locally correspondence image scene constraints multi-view sequences applied filtering computed flow estimates proposed particle approach scene flow making depth image incorporated local constraint scene flow method temporal multi-frame longrange structure discontinuity zhang kambhamettu segmentation applied applied segmentation model optical flow edge-preserving regularizer image regularizer regularizer preserve motion depth color edges generally consistent making depth boundary optical common bruhn optical flow reported enforce temporal brox bruhn weickert bruhn constraints assuming flow vectors consecutive frames image enforced forward backward flow reject assuming motion black enforced temporal smoothness motion frame current methods consecutive effective errors frames multiple flow vectors noise global motion static construct chained motion particle chained trajectories constructed bidirectional check motion vectors based large displacement optical flow estimation quality trajectories flow estimate consecutive making methods noise estimation int comput vis based displacement frame flt written correspondence frt motion image scene flow correspondences frames temporal consistency depth zhang proposed data data term data locally window temporal video methods large motion frames suitable static propose binocular framework temporal consistency problem depth motion estimation long temporal multiple frames incorporated novel chained representation spatial depth variation correspondences denote displacement pixels depth correspondences fundamental constraints depth estimation neighboring frames scene flow problem introduction binocular method stereo frame dense motion framework stereo videos fundamental denote corresponding frames stereo sequences flt frt time pixel frame flt find corresponding pixels neighboring frames temporally motion estimation spatially stereo expressed correspondence indexes robust written reject matching stereo constraints motion constraints computation robust illumination compared considering image intensity estimating depth scene takes variational method devernay compute scene flow field constraints represent disparity point optical flow correspondence left sequence pixel int comput vis flow estimation corresponding frames binocular depth estimates joint method huguet devernay depth scene flow temporally consistent frames representation describe correspondences stereo frames binocular depth maps estimated joint method huguet devernay considering motion stereo temporally correspondences locally established occlusion illumination global constraint find errors temporal depth estimate computation steps errors long propose chained temporal constraints long-range estimation depth scene flow quality values visualize disparity maps visualize image scene optical flow color-coded motion intensity indicates flow scene flow vectors color coded algorithm method stereo sequence initialize motion fields disparity temporal build robust compute edge occurrence compute trajectory-based depth motion robust refine depth scene flow global temporal disparity maps scene flow approach method consistent depth scene flow binocular pixel correspondence multiple system algorithm main steps temporal constraint final depth joint scene flow initialization initialize depth apply variational method optical flow estimation optimization stereo estimating large int comput vis initial depth depth maps consecutive initial color-coded optical flow fields disparity initialization compute disparity frame esd defined image esd function preserve written esd regularization term preserve energy solve occlusion labeled check pixels left view mapped pixel smaller disparity set disparity maps consecutive frames boundaries inconsistent depth boundaries inconsistent depth values supplementary video depth optical flow initialization initialize motion variational forward backward flow vectors robust detailed compute flow frames flt flt frames figure motion computed describe estimation flow vector illustration function variational weight smoothness computed flow equation method brox estimated optical flow consecutive frames chained temporal priors motion trajectories link corresponding pixels structure system temporal robust motion trajectory estimation find correspondences build motion trajectories based optical flow estimate note motion vectors link mapped based motion vector correspondence frame frame motion vector generally spatial propose simple effective improve interpolation experiments simple bilinear erroneous figure regions motion point pixels bilinear variation preserve int comput vis illustration flow interpolation interpolation bilateral bilinear interpolation top bottom based occlusion labeled based produced considering errors larger long-range motion motion problem common sequences dynamic incorporating color interpolation spatial spatial bilateral filtering written frame indexes motion term fit points set comparison bilateral interpolation standard bilinear interpolation motion field dynamic object bilateral method motion interpolation link corresponding points motion estimation errors occlusion object outliers bidirectional flow vectors devernay mapped based motion vector sum vectors define map error set motion vectors set problematic flow vector motion temporal outliers caused occasional noise pixel color consistently frames pixel flow bidirectional consistency trajectory frame test break algorithm build motion trajectory detailed algorithm trajectories forward backward find series correspondences frames pixel frame motion vector correspondence frame expressed sum consecutive motion vectors trajectory frame figure motion fields produced algorithm matching black algorithm typically caused motion break motion occlusion trajectories demonstrate pixel occluded int comput vis algorithm robust trajectory motion demonstrate effectiveness flow comparison produced trajectory construction long-range flow black pixels flow field occasional noise estimation trajectory-based structure profile build system constraints applied multiple computation central motion disparity boundaries image structure edge motion depth flow edges illumination image sequences boundaries dynamic objects time common image gradient frames separately reliable consistent compute series temporally consistent spatially dynamic objects variation edge magnitude maps separately frame bilateral smoothing small magnitude compute illustration structure edge edges inconsistent structure profile temporally consistent trajectory-based structure profile construction setting pixels edge occurrence map smaller set indicates occurrence structure maps input computed dense motion structure profile map pixel frame edge occurrence values correspondences trajectories find consistent edge occurrence values voting-like corresponding point frame motion average occurrence trajectory expressed int comput vis number corresponding pixels statistics structure profiles occurrence strong edges multiple current pixel consistent edges typically large number frames consistently caused noise estimation errors correspondences current pixel occurrence edge point gather large occasional outliers consistent small voting-like consistent edges edge occurrence maps define edge figure edge occurrence inconsistent edges consistent profile construction process robust noise sudden illumination trajectory-based profile set profiles constructed based fact scene flow constraints proposed black bruhn weickert bruhn enforce temporal consistency motion locally weickert bruhn model fit motion reject outliers depth motion describe depth profile estimation motion profile computed adopt linear model fit depth values frames temporally based linear model parameters representing depth regression minimize energy computed maps frame check indicates weight regression check sum regression equation effective gather disparity multiple frames occlusion initial disparity values global robust regression process incorporated neighboring disparity parameters setting simple estimated linear depth frame written indexes frames trajectory weight frame corresponding depth frame sub-pixel point depth bilateral defined scene temporally consistent optimization regression reject outliers preserve average magnitude map smaller depth change motion profile apply linear weight window reduce influence frames current frame depth occlusion labeled motion profile pixel frame int comput vis algorithm depth scene flow computation depth profile motion profile frames temporal depth constraint frame scene flow based temporally consistent disparity maps scene flow motion occlusion estimated bidirectional occlusion generally forward correspondences pixel trajectory making find points gather statistics refine motion adopt small fact motion variation typically larger change scene depth scene flow describe central steps estimate depth image scene flow temporal find estimating depth scene flow pass fact depth scene flow variables disparity large values pixel scene flow object variation smaller variational optimization difficult perform local reported huguet devernay wedel joint procedure estimate depth scene flow takes long notable initial disparity values estimated sub-pixel estimation scene depth scene disparity sequence long-range temporal scene flow algorithm algorithm describe functions depth scene consistent depth estimation refine minimize data cost defined views time occlusion reduce influence occasional define numerical temporal depth data defined depth simple term method longrange temporal multiple local data structure profile incorporated depth regularization enforce structure consistency smoothness regions small depth discontinuity values edges enforce small smoothness pixels edge well anisotropic smoothness method provide constraints edge-preserving depth gradient image vector parallel frame intensity gradient propose function anisotropic smoothness robust defined smoothness enforced discontinuity gradient reliable strong large constraints note strong edges caused occasional outliers frame anisotropic compared temporally consistent edges anisotropic written diffusion int comput vis temporal defined effectiveness anisotropic depth estimate top bottom left input image depth anisotropic regularization depth refinement sub-pixel flow constraint defined diffusion defined smoothness term sudden change motion disparity frame diffusion anisotropic smoothing edge maps define vectors parallel equation minimized variational note solve large displacements variational framework initial estimate energy minimization depth map result comparison bottom left depth map smoothness large depth estimation problematic result anisotropic regularization scene flow estimation weight set final function scene flow estimation temporal constraints minimize variational method detailed energy minimization estimate motion depth variation temporal constraints image scene data term data cost scene flow defined disparity computed estimation step depth flow caused weight long-range temporal equations solved framework variational procedure variables well estimated optimization compute depth temporal depth profile jointly scene flow depth motion profiles provide energy minimization proper initialization estimation large displacements image perform series data term solve int comput vis div set estimate procedure final depth map computed depth set solve scheme brox estimate energy functions minimized corresponding euler-lagrange denote fdh frt fdh representing euler-lagrange equation fdh flv frv div flv div frh frt flt representing smoothness expressed depth system linear solved details appendix experiments evaluate perform quantitative evaluation sequences university auckland ground truth depth scene flow fields notable methods reported error statistics video sequences longrange temporal provide statistics detailed per-frame errors step proposed supplementary evaluate long-range temporal experiments separately test including longrange depth motion challenging sequences large dynamic set natural parameters quantitative evaluation error measure errors estimate ground truth disparity div brox bruhn weickert values previous step linear solved standard linear details appendix final scene flow denote flh flt flv flt flt flt frh frv frt euler-lagrange equations flh flh flv frh frv flh frh int comput vis per-frame error statistics sequences traffic scene traffic scene number pixels image scene adopt average angular error error evaluate angular table average mae including initialization robust regression final refinement scene flow aae errors traffic scene initial robust rmse disparity mae final errors traffic scene initial robust final rmse errors aae measure image occluded wedel traffic scene sequence frames auckland moving frames sequence supplementary initial disparity map erroneous occluded robust regression result depth occlusion estimation outliers temporal final variational refinement sub-pixel accuracy mae error pixels error pixels larger positive representation wedel scene flow fields stages angular error maps coded note window car dark regions consistently large large color corresponding statistics sequence table errors produced stages main steps detailed per-frame error int comput vis traffic scene left views time ground truth disparity pixels disparity maps robust final error images dark pixels positive visualize scene flow fields corresponding angular error maps scene flow table multi-pass depth scene flow estimation errors pass scene flow aae pass rmse disparity mae int comput vis table error statistics long-range temporal constraints scene flow aae temporal constraint temporal constraint rmse disparity mae table error comparison structure profiles scene flow aae structure map single-frame structure map temporal structure map rmse disparity mae disparity maps produced single-frame edge temporal structure profile temporally consistent edges image car boundary problematic edges compared bottom corresponding edge maps long-range temporal constraint evaluation evaluate constraint long-range temporal depth motion experiments robust temporal depth motion parameters temporal constraint perform variational minimization setting comparison frames computed temporal constraints moving car estimates temporal constraints pixels table corresponding errors long-range temporal challenging natural video apply method natural video sequences multiple dynamic objects motion final balloons fish sequences input initial depth trajectory-based edge profiles depth maps robust final depth sub-pixel final scene flow fields figure dynamic scene moving depth maps scene flow fields second note consistent depth scene flow estimation sequences large dynamic objects challenging illumination final temporal consistent frames traffic scene frames ground truth depth stages errors coded scene flow vectors errors corresponding errors table per-frame statistics second visual quantitative effectiveness multi-pass flow estimation scene flow depth fields motion trajectories refine experiments running system result previous pass scene flow fields computed construct motion error statistics traffic scene table notable multi-pass estimation improve result temporal well incorporated algorithm structure profile evaluation evaluate structure table statistics traffic scene profile errors profile setting constructed statistics edge map single frame structure temporal structure profile structure profiles int comput vis traffic scene left views time ground truth disparity pixels disparity maps robust final error images dark pixels positive visualize scene flow fields corresponding angular error maps scene flow int comput vis single-frame edge temporal structure profile depth method standard videos demonstrate accuracy consistency depth view sequences views produced initial depth visual final continuous variation depth spatially temporally object effective multi-frame profile robust supplementary cost method long-range temporal cost compared single-frame trajectory-based optimization procedure frames parallel binocular video traffic scene frame takes table running time initialization system single temporal consistency estimating depth scene flow maps video disparity result comparison temporal depth profile temporal depth profile table time data process single system running running time initial final frame running time constraints multiple frames previous joint estimation methods generally long range scene flow estimation wedel tackle problem temporal consistency point view approach temporal constraints chained temporal major proposed approach chained temporal priors depth scene flow estimation binocular work range int comput vis balloons sequence 1st input 2nd initial depth trajectory-based structure depth maps temporal depth maps sub-pixel final color-coded scene flow fields int comput vis fish sequence 1st input 2nd initial depth trajectory-based structure depth maps temporal depth maps sub-pixel continuous final color-coded scene flow fields int comput vis sequence top input depth scene flow fields frames balloons 1st initial 2nd depth sub-pixel visual final boundaries major long-range temporal improve depth scene flow consistency robust estimation method motion trajectory construction find reliable estimates break occlusion consistently multiple occasional noise novel edge occurrence maps constructed incorporating multiple average scheme errors multiple frames propose anisotropic smoothing scheme provide proper regularization pixels based structure work method depth initial depth estimation consistently refinement improve work videos int comput vis frames fish 1st initial 2nd depth sub-pixel continuous optimization time work hong appendix details euler-lagrange equation fdh 2hh div applied anisotropic diffusion smoothness term neighboring represent current point apply central second function expressed 2hh defined apply gauss-seidel represent anisotropic computed int comput vis flh frh set neighboring defined flh flv frv flv flv frv flv frh 2hh frh adopt standard non-linear numerical scheme weickert gauss-seidel applied functions defined appendix non-linear numerical scheme compute appendix dense optical flow estimation international journal computer evaluation optical international journal computer multi-view scene flow view variational cvpr non-linear estimation flow eccv accuracy optical flow estimation based eccv large displacement optical cvpr motion accuracy iccv local global flow international journal computer scene flow estimation correspondence cvpr robust cvpr particle based scene flow depth iccv appendix linear equations variables gauss-seidel written flh frh flh variational method scene flow estimation stereo iccv multi-frame correspondence estimation international journal computer energy functions minimized ieee transactions pattern analysis machine edge-preserving joint joint disparity motion field estimation stereoscopic image international pattern motion field estimation stereo image sequences eccv stereo matching eccv particle long-range motion estimation point cvpr particle long-range motion estimation point international journal computer evaluation dense stereo correspondence international journal computer transactions optical eccv dense point trajectories large displacement optical eccv bilateral filtering color iccv university auckland image sequence analysis test joint estimation structure geometry stereo eccv int comput vis stereo motion stereo international image scene ieee transactions pattern analysis machine scene flow estimation motion iccv dense scene flow dense stereo eccv stereoscopic scene flow computation motion international journal computer bilateral optical flow estimation occlusion eccv segmentation based variational model optical flow eccv motion optical flow cvpr approach correspondence ieee transactions pattern analysis machine estimation displacements frames ieee transactions pattern analysis machine scene flow structure cvpr dynamic cvpr consistent depth maps video ieee transactions pattern analysis machine
dense motion disparity estimation loopy belief propagation isard maccormick computing dense estimate motion stereo video sequence moving contrast previous motion disparity estimated single coherent probabilistic model correctly depth motion estimation motion disparity estimating scene previous work stereo problem estimating disparity motion fields video sequence moving objects pair stereo standard temporal stereo motion problem estimating disparity motion consecutive frames stereo stereo paper solution two-frame stereo explains solution stereo form dense image stereo sequence stereo dense disparity estimation single stereo pair regions foreground object boundaries visible single depth estimated objects temporal stereo sequence previous work motion estimates average stationary estimate dense motion disparity single coherent probabilistic two-frame motion estimation stereo regions scene visible single filtering time depth scene patches occlusions non-reference dense motion disparity estimation loopy belief propagation work temporal based image dense disparity motion examples approach example approach segmentation estimate assuming single motion objects motions approach paper single probabilistic model motions objects work large dense stereo belief propagation graph approaches motion temporal stereo motion probabilistic approach two-frame stereo motion single nodes pixels reference labels occlusion map estimate mrf loopy belief work disparity motion estimation approach previous approaches temporal stereo motion three estimates contrast approaches employ single coherent probabilistic contrast segmentation approaches correctly occlusions paper stereo motion work occlusions occlusion previous work stereo likelihood disparity point computed point visible occluded object figure example stereo motion work data mrf computed mrf label reference image pixel occlusion status figure non-reference image patches data contrast previous work stereo patches corresponding occluded solution multi-frame temporal stereo motion problem two-frame problem isard maccormick left previous image previous image left current image current image motion disparity non-reference foreground objects disparities moving stationary pixel reference image non-reference images visible pixel visible left previous images left current image pixels visible three non-reference images pixels visible reference image filtering previous frames extra term mrf data describes mrf two-frame stereo explains multi-frame loopy belief propagation map estimates describes mrf two-frame stereo motion input two-frame stereo motion algorithm left0 right0 left1 right1 left stereo views previous current frames stereo video stereo pairs corresponding pairs output disparity motion fields graphical model map estimate disparity motion graphical model form standard input nodes dense motion disparity estimation loopy belief propagation input current image right1 reference state node motion disparity estimated pixel right1 state node models point object pixel reference camera scene note object previous current note visible three non-reference state write described disparity current frame horizontal components motion difference disparity previous frame current occlusion status three visible non-reference state non-reference assuming definitions left1 left0 right0 graphical model states pairs neighboring negative log log log term data term continuity data cost difference patches centered image image isard maccormick patch rgb space rgb image pixel location average rgb image patch centered nssd computed centered small standard nssd patch centered computed image gradient values nssd expected small patches derived views assuming nssd distributed patches negative log values log log data derived physical described technical report data cost graph node state left1 left0 right0 definitions node models point patches reference centered non-reference centered location visible non-reference nssd distributed nssd distributed definitions visible log based distribution assuming nssd equivalent log data cost previous work data cost log likelihood linear function region data cost terms log likelihood linear function dense motion disparity estimation loopy belief propagation continuity cost neighboring nodes graphical states continuity cost assume components state image equivalent adopting form continuity terms based expected scene image stereo camera model terms continuity assume difference distributed negative log distribution function linear technical report describes values based physical graphical disparity motion fields object object boundaries image gradient reference image location corresponding nodes average image gradient reference note nodes occlusion status horizontal occlusion status object object boundaries neighboring mrf nodes occlusion example horizontal object visible contrast model pairs neighboring temporal filtering stereo motion previous described model computing disparity motion fields consecutive frames stereo video model pair consecutive frames disparity motion fields estimates temporal extra computational isard maccormick adopting filtering model time output time mrf time nodes labels output filtering algorithm time estimated labels filtering model equivalent extra term data cost temporal compatibility form temporal compatibility function function derived write label terms occlusion motion occlusion status three non-reference point visible location reference physical visible image rightt-1 location disparity adopting motion assume time note image rightt-1 reference image stereo motion computation assuming map estimate write temporal compatibility function disparity motion expected cost small standard negative log distribution function components cost function linear previous point visible rightt-1 temporal compatibility function form components technical report explains stereo motion estimate map mrf described previous loopy belief propagation form model computational belief propagation large images large dense motion disparity estimation loopy belief propagation disparities motions approach note approaches employ pixel graph full state space algorithm states algorithm employ images stereo motion algorithm computational algorithm stereo examples pixel region pixels full stereo motion computation label space values labels small image disparity motion computational approach figure foreground object left stereo computation estimate foreground disparity filtered stereo motion algorithm correctly previous disparity output left previous image previous image left current image current image disparity disparity estimated estimated filtered stereo stereo motion estimates disparity left current image stereo computation estimate disparity filtered algorithm previous isard maccormick left previous image previous image left current image current image disparity disparity estimated estimated filtered two-frame stereo stereo motion motion disparity estimates multiple foreground person large left occlusion two-frame stereo computation disparities occluded region large filtered estimate disparity estimates previous frames figure temporal image foreground person disparity two-frame stereo motion estimate disparity occluded left foreground person disparity estimate filtering algorithm estimate full filtering algorithm examples frame computation small image patch state algorithm temporal stereo motion algorithm dense disparity motion estimates coherent probabilistic occlusion approach models two-frame stereo motion problem single multi-frame temporal filtering mrf dense stereo motion stereo estimates stationary moving objects image regions stereo stereo motion disparity motion fields dense motion disparity estimation loopy belief propagation work computational object binocular image estimating disparity occlusions stereo video temporal sequence computer vision image understanding stereo three motion multiple moving objects binocular image computer vision image understanding stereo belief computer computing occlusions graph computer computer approach dense multiple camera views image vision scene computer approach binocular computer vision dense two-frame stereo computer vision vision dense motion disparity estimation loopy belief technical segmentation binocular stereo understanding belief propagation belief propagation
tensor voting approach multi-view scene flow estimation refinement korea korea introduce framework estimate refine scene flow connects structures scene previous approaches compute scene flow connects depth stereo image sequence depth approach takes advantage reconstruction computes scene flow connects point clouds multi-view stereo approach standard multi-view stereo optical flow algorithm compute initial scene refinement process scene flow direction magnitude scene flow direction refined neighbor smoothness defined tensor magnitude scene flow refined connecting implicit consecutive point estimated scene flow temporally approach model error outlier approach synthetic experimental approach previous algorithms synthetic improves reconstructed model refined point cloud scene tensor scene flow dense motion vector field motion approaches scene flow based stereoscopic inputs estimated scene flow defined depth map scene flow depth map estimated scene flow easily introduce scene flow algorithm point cloud multi-view estimated scene motion vector field approach multi-view reconstruction optical flow multi-view works reconstruct accurate model error eccv heidelberg tensor voting approach standard multi-view stereo dataset well occlusion optical flow algorithms average error average angular error standard optical flow dataset scene flow estimation estimated optical flow estimated model accuracy estimated scene flow approach easily errors estimated model estimated optical optical flow estimated individually image errors view point easily estimated scene approaches estimate model scene flow mesh deformation points approaches data structures handle reconstructed surface accurate dense points approach computes scene flow point clouds model data approach initial scene flow optical flow view points point cloud work closed form tensor voting reject outliers estimate surface explicit computation surface cftv handle scene flow cftv scene flow magnitude scene utilize implicit surface estimate scene flow magnitude connecting scene flow point cloud time implicit surface point cloud time major advantage approach computation number direction magnitude estimation scene flow process cftv framework scene flow direction scene flow refinement algorithm advantage outlier error point experimental approach accuracy estimated scene flow scene flow refinement algorithm scene flow estimation algorithms enhance work scene flow estimation introduced scene flow estimation algorithms introduced scene flow estimation algorithm optical flow model consistent input images estimated scene scene park flow estimation multi-view camera motion optical flow stereo view scene flow estimate scene flow scene reconstruction matching images time images time refine scene flow stereo image sequences stereo matching algorithm estimate depth map computes scene flow estimated depth approaches estimation framework approaches estimate depth map scene flow simultaneously camera huguet optical flow camera dense stereo time implementation variational optical flow algorithm temporal basha structure scene flow simultaneously variational approach based depth map scene improves work basha rigid motion prior function scene flow compute scene flow video cameras stereoscopic video depth estimate camera scene introduce scene flow depth map based deformation points video sequence estimate scene represent surfel small square deformation surfel video estimate surfel scene flow surfel deformation mesh projected motion mesh scene optical flow scene flow mesh projected optical approaches scene flow estimation represent scene flow surfel encodes motion small region scene point cloud point mesh simultaneously estimate shape motion scene comparing approach previous approach regularization estimate refine scene major approach previous approaches process algorithm point cloud previous algorithms depth map image comparing point clouds depth point tensor voting approach initial inputs data synchronized cameras uniformly outdoor point cloud reconstructed pmvs estimated optical flow cameras method note errors point cloud optical flow clouds scene flow point neighborhood regularization point clouds mesh reconstruction neighborhood regularization scene flow depth map neighborhood well defined well image cftv neighborhood based based location normal work unstructured point cloud model deformation scene flow estimation comparing previous works data structure unstructured point cloud approach data system data consists cameras uniformly cameras synchronized image system well capture data outdoor figure initial inputs initial scene flow estimation approach initial estimation scene flow multi-view stereo optical flow based multi-view stereo initial park point cloud frame compute optical flow image sequence camera image plane optical flow image scene flow camera projection represent i-th point point represent -th camera system represent time input image sequences total number points point total number total number order initial scene flow point cloud optical utilize approach square errors scene flow projected image plane point point cloud individually image i-th point projected -th optical flow i-th point image plane -th second -th camera projection closed form tensor voting approach scene flow refinement closed form tensor voting cftv improves reconstruction accuracy noisy surface normal tensor voting field normal neighborhood define neighborhood point point neighborhood points tensor vector direction function encodes normal defined connecting tensor voting approach normal decay function saliency decay function equation decay function surface normal evaluate neighborhood larger weight point larger surface better normal decay function neighborhood points votes refined normal direction largest note cftv reconstruct mesh model order estimate refine normal major scene flow refinement unstructured point tensor cftv outliers votes point location normal direction point cftv second accuracy point cloud pmvs scene flow estimation scene flow refinement scene flow direction cftv framework handle scene flow define scene flow scene flow direction neighborhood structures points explicit mesh reconstruction order data structure scene flow refinement process scene flow normal direction propagation enhance outlier error define scene flow tensor scene flow direction structure aware decay function defined surface saliency second largest structure park tensor equation configurations normal points physically normal configurations points match small weight propagation weight physically closed normal configurations figure larger weight point surface normal direction weight point surface normal second order scene flow scene flow direction direction largest angular refined scene flow direction initial scene flow scene flow cftv estimate scene flow magnitude scene flow magnitude scene flow second order propagate scene flow magnitude individually scene flow direction estimated scene flow magnitude physically estimated scene flow surface model scene flow magnitude estimated scene flow estimate scene flow physical property scene flow connects surface model consecutive function defined point location time evaluated surface saliency point cloud time direction connecting surface data term equation smoothness term second equation estimated locations votes evaluate surface evaluated surface saliency note estimation individually explicit surface equation initial estimation scene flow magnitude tensor voting approach virtual decay estimate magnitude scene flow physical property scene flow connects structures consecutive approach mesh compute surface saliency tensor votes scene flow scene flow magnitude virtual surface structure aware neighbor smoothness regularization defined equation note function equation define neighborhood smoothness term normal neighborhood smoothness prior defined cftv figure process scene flow magnitude connects virtual surface point clouds time reject outliers scene flow saliency equation temporally consistent scene flow process scene flow point cloud frame order utilize consecutive temporal estimated scene flow nearest neighbor temporal frame temporal propagate scene flow voting field voting direction direction defined scene flow direction points time scene flow tensor temporal neighborhood defined point neighbor time function encodes scene flow tensor scene flow defined decay function equation temporal neighborhood scene flow direction propagation temporal estimated scene flow temporally scene flow well structure aware scene flow propagation equation equation refinement park sphere points sphere translated initial noisy scene refined scene flow ground truth scene table square error figure approach magnitude noisy scene scene flow magnitude regularization magnitude regularization described temporal neighborhood time temporal scene flow experimental consists synthetic figure sphere ground truth scene flow uniformly noisy scene flow optical flow ground truth locations points approach refine noisy scene figure approach refine noisy scene points refined scene shape sphere table quantitative evaluation square error scene flow comparing ground direction scene flow magnitude refinement scene flow magnitude regularization huguet basha ball dataset figure ground dataset consists images represent view points time sphere plane mask region discontinuity region baseline performance approach mesh deformation approach tensor voting approach table quantitative evaluation synthetic ball huguet basha estimated scene flow evaluation nrms errors estimated point location nrms errors estimated scene flow estimated scene flow row row second row errors discontinuities mask occlusion mask row pixels quantitative nrms nrms discontinuities huguet occlusions pixels discontinuities basha occlusions pixels discontinuities occlusions pixels discontinuities occlusions pixels method synthetic ball dataset synthetic images discontinuity map occlusion estimated estimated scene flow scene flow initial mesh scene flow matching time baseline initial point location cftv refinement location point location closed input scene flow implicit estimated scene flow refined scene temporal consistent scene flow refinement described evaluation term park scene flow refinement outdoor reconstructed structure better scene flow scene flow refinement square error estimated point locations estimated scene angular error estimated scene table quantitative baseline table comparing refinement approach works point points outlier better performance pixels discontinuity regions approach better performance pixels evaluated cftv handle outliers discontinuities smoothness occlusion note approach scene flow refinement point stereoscopic inputs huguet basha handle stereoscopic tensor voting approach approach improves reconstruction input second reconstructed model point cloud pmvs reconstructed model refined point locations point evaluate approach outdoor dataset figure dataset consists points average algorithm pmvs takes minutes reconstruct point cloud optical flow implementation takes seconds compute optical flow image implementation nearest neighbor data structure takes minutes data takes minutes direction refinement seconds magnitude refinement points neighbor points frame implementation takes seconds time temporal takes better regions figure scene flow refinement approach accuracy scene flow scene flow direction refinement scene flow magnitude approach reject outliers locations outliers reconstructed model surface reconstruction figure translated points time time estimated scene flow figure figure translated points match points park point cloud match point cloud estimated scene regions points well points estimated scene flow framework refine scene flow estimated multi-view stereo optical flow projection images approach cftv framework handle scene flow data normal estimation cftv estimate scene flow physical property scene flow implicit surface point clouds consecutive introduced scene flow temporal neighborhood described propagate scene flow direction temporal approach model handle outliers noisy scene motion rigid motion prior enhance performance estimated scene flow data video view korea multi-view tensor voting approach scene ieee pami multi-view scene flow view variational cvpr variational method scene flow estimation stereo iccv stereo ijcv accurate motion field estimation stereo image sequences eccv heidelberg dense scene flow dense stereo eccv heidelberg scene flow structure multiview image ieee scene flow estimation rigid motion iccv estimation structure geometry stereo eccv heidelberg based scene flow depth iccv stereoscopic scene flow computation motion ijcv multiview ieee pami evaluation multi-view stereo reconstruction cvpr optical flow estimation cvpr evaluation optical ijcv scene flow optical image processing scene flow points cvpr dense motion capture synchronized video cvpr reconstruction multiview cvpr multi-view stereo reconstruction scene flow estimation matching ijcv park surface multi-view scene capture surfel video shape ijcv reconstruction geometry point geometry processing dense accurate multi-view heidelberg tensor ieee pami nearest neighbor surface geometry processing
international conference vision motion smooth rigid scene flow rgb-d images propose joint segmentation approach estimate scene flow rgb-d scene number independent labels capture transitions rigid parts velocity point computed combination estimated rigid better traditional sharp piecewise variational smooth segments scene corresponding rigid segmentation number regions optimization process capture number moving evaluate approach synthetic real rgb-d images large experiments method estimates scene flow accurately meaningful segmentation scene based frame second frame motion segmentation warped labels figure proposed method based motion interpolation allows smooth transitions segments motion convex combination rigid motions scene flow estimation body articulated scene scene rigid articulated methods work scene flow estimate motion rigid rigid methods segment scene regularization regions segmentations ieee motions scene scene depth scene piecewise planar scene addressed rigid motions parts body moving transitions rigid rigid motions articulated sharp segmentation estimate motion smooth transitions problem smooth segmentation motion estimates rigid rigid assigned independent solve non-convex optimization problem coordinate descent motion estimation step visual variational quadratic regularizer total variation smooth transitions motion models approach traditional evaluate motion scene flow algorithm synthetic real rgb-d image approach evaluation combination convex labelling quadratic regularizer sharp traditional segmentation piecewise method meaningful soft segmentations rigid parts figure work scene flow estimation stereo computer vision proposed methods based optical range flow approach regularize flow field quadratic regularization flow scene flow estimation real-time stereo approach image planar estimates rigid-body motion planar motion regularization constraint scene flow rgb-d scene flow estimation depth images high data term optical range flow constraint real-time dense scene flow rgb-d regularization flow regularization term scene flow observed scene flow estimate 6-dof motion regularize flow field 6-dof model rigid propose corresponding points range traditional planar motion segmentation computer vision variational method motion segmentation optical flow proposed work motion motion segments best motion method motions model occlusions label optimization constraint data methods 6-dof motion estimate segmentation dense rgb-d propose variational rigid-body motion segmentation reconstruction method variational plane step method method approach estimate motion segmentation rigid rgb-d motion segment assigned rigid-body approach interpolate motions proposed approach segmentation estimate motion rgb-d regularize estimation process rigid-body motion approach model smooth transitions motions allows small motion field mc-flow algorithm approach joint scene flow estimation scene moving regions underlying algorithm estimates motion based smooth piecewise soft segmentation regularizer interpolate rigid motions order moving parts underlying number rigid parts segmented mc-flow state-of-the-art rgb-d scene flow algorithms problem formulation assume scene segmented motion label rigid parts rigid motion smooth segmentation figure rgb-d depth images defined image segments rigid motions associated labelling function rigid motions number moving label function moving scene segment velocity note order label functions label assignment problems underlying binary general problem motion segmentation motion estimation optimization function photometric geometric order regularize labels imposing term note problem labels non-convex best problem formulation labels optimization process label assignment function data term independent rigid function geometric photometric rgb-d images combination rigid-body optimization problems labels models motions interpolated evaluate residuals residuals computed independent rigid motion interpolated binary interpolation motions residuals models solve motion interpolation model model regularization term crucial estimate accurate interpolated motions transitions rigid optimization models rigid optimization problem convex global joint propose coordinate descent strategy motions set labels labels algorithm motions computed visual warping function coordinates observed points pixel function coordinates rigid odometry scene rigid labels solved model convex note label number labels solve algorithm algorithm coordinate descent optimization joint motion estimation segmentation initialize arg arg residuals weighting high minimization problem solved associated weighting function motion estimation set iteration algorithm estimate rigid-body motions associated label problem visual odometry general scene moving assume rigid motions motion point solution estimate motion segments robust dense visual odometry solution photometric geometric defined solve motion estimation problem minimization photometric geometric residuals allows estimate motion segments geometric crucial segments considerably small photometric geometric data solve problem label optimization motion iteration label assignment function second step optimization problem set motions convex solved labelling function primal-dual algorithm algorithm addressed problem total variation quadratic data rgb-d provide regularize labels real defined function note residuals defined motion interpolation model large process minimization applied residuals order provide accurate motion robust function residuals arg regularization strategy total variation regularization total variation work image regularizer solution applied general reconstruction problems image image function considerably better image segmentation order regularization labels associated small label small pixels outlier label label quadratic regularization regularization sharp label segmentation smooth smooth label transitions quadratic occlusions outlier label capture pixels depth pixels high residuals velocity associated label image plane outlier label process number label pixels assigned assigned label motion estimate assigned outlier motion estimation assigned best label label optimization detect occlusions evaluation pixels second rgb-d occlusions binary occluded points segmented motion estimated regularization allows provide segmentation scene points areas occluded order detect pixels pixel second frame temporal depth function pixels frame warped pixel second frame estimated function points second observed points higher pixels frame warped pixel second function pixels occlusion warping estimated motion function quadratic regularization provide geometric weighting estimate labels soft transitions rigid parts number labels strategy number labels represent number independent rigid motions non-convex crucial initial set labels allows converge global algorithm number labels variational formulation propose initialize labels meaningful observed scene labels motion initial k-means segmentation based coordinates points initial number labels set number independent rigid motions scene k-means figure label iteration algorithm image k-means figure initialize algorithm k-means coordinates image optical flow geometric point occluded function detect occlusions temporal functions detect occluded areas scene imposing strategy functions variational formulation imposing regularization occlusion work label figure sintel observed sharp labels pixels quadratic regularization smooth segmentation pixels interpolated velocity rigid-body figure real rgb-d images number labels method regularization higher number labels interpolate motions labels observed sintel-4 segmentations represent accurately rigid parts scene flow evaluation scene flow compared state-of-the-art primal-dual flow flow flow photometric geometric residuals computed warping depth images estimated note occluded pixels high residuals motion accurately estimated considerably error provide rmse scene flow assume occlusion layer computed approach accurate pd-flow sr-flow detect represent method applied occluded pixels error compared methods table frame observed method accurate estimates quadratic transitions labels number labels converge sharp segmentation motion quadratic regularization smooth transitions labels areas interpolated converge higher number capture best quadratic figure experiments evaluate scene segmentation scene flow evaluation process scene flow ground truth segmentation set synthetic real rgb-d frame pairs approach tested sequences sintel dataset dataset large optical flow ground truth scene flow joint segmentation motion estimation rgb-d image pairs rgb-d method corresponding regularization strategies label optimization total variation quadratic regularization images rgb-d sintel depth set total image sintel dataset real image pairs scene segmentation motion segmentation method tested occlusion layer sequences segmentation occlusion inputs sintel-1 segmentation segmentation sintel-5 inputs segmentation segmentation sintel-2 sintel-3 figure segmentation estimated approach sequences sintel dataset independent associated rigid outlier inputs segmentation segmentation occlusion inputs segmentation segmentation occlusion sintel-4 figure segmentation occlusion layer estimated approach image pairs rgb-d independent associated rigid outlier flow flow mc-flow mc-flow figure comparison motion estimated initial frame point frame point scene flow comparison approach accurate estimate scene motion field compared methods estimate work estimate large motions capture motion body sr-flow better real motion approach estimates motion field quadratic regularization sintel image sintel-8 sintel-7 sintel-6 scene flow image plane optical flow ground truth sintel evaluate error average error average error computed comparison methods occlusions provide estimates occluded approach photometric rmse geometric rmse pd-flow sr-flow layered-flow mc-tv mc-quad pd-flow sr-flow layered-flow mc-tv mc-quad sintel-1 sintel-2 sintel-3 sintel-4 sintel-5 sintel-6 sintel-7 sintel-8 average table photometric geometric residuals warping image pairs estimated scene optical flow optical flow pd-flow sr-flow layered-flow mc-tv mc-quad pd-flow sr-flow layered-flow mc-tv mc-quad sintel-1 sintel-2 sintel-3 sintel-4 sintel-5 sintel-6 sintel-7 sintel-8 average table average optical flow computed estimated scene flow image quadratic regularization motion estimate accurate sr-flow method second seconds seconds seconds optimization addressed problem joint segmentation scene flow estimation rgb-d optimization problem solved coordinate descent method motion estimation label number labels real number independent rigid motions regularization strategies labels sharp smooth method tested synthetic real rgb-d image experiments joint segmentation motion estimation accurate state-of-the-art scene flow algorithms rgb-d regularization strategies quadratic regularization estimates motion accurately smooth label transitions rigid models scene motion work rgb-d temporal regularization european convex vision evaluation optical international journal computer scene flow variational international journal computer variational motion segmentation european conference computer vision computer optical flow european conference computer vision convex approach journal variational image image note motion variational approach piecewise motion international journal computer rgb-d dense motion estimation conference robotics automation scene flow rgb-d conference computer vision pattern recognition variational method scene flow estimation stereo conference computer vision visual odometry range ieee primal-dual real-time dense rgb-d scene conference robotics automation robust odometry estimation rgb-d conference robotics automation algorithms global image segmentation journal applied order primal-dual algorithms convex conference computer vision dense scene flow estimation european conference computer vision dense motion estimation reconstruction ieee total variation based dense rigid-body motion segmentation estimation rgb-d international journal computer scene flow conference computer vision pattern recognition joint motion estimation segmentation label occlusion conference computer vision pattern recognition scene conference computer vision scene flow estimation rigid motion conference computer vision piecewise rigid scene conference computer vision scene flow motion international journal computer stereo conference computer vision scene flow conference computer vision pattern recognition
ieee international conference robotics primal-dual real-time dense rgb-d scene flow paper compute dense scene flow real-time rgb-d based variational formulation brightness constancy geometric consistency depth data provided rgb-d regularization flow field imposed surface set observed scene image leading consistent minimization problem efficiently primal-dual algorithm implemented temporal compare approach work quantitative qualitative set experiments work estimate motion accuracy rgb-d estimate heterogeneous non-rigid motions high frame estimating motion objects scene point scene flow defined dense non-rigid motion field scene observed instants conversely optical scene motion image scene flow motion field optical flow stereo camera allow estimation scene commonly compute scene rgb-d cameras directly provide depth images high frame rate advantageous implementation fast scene flow applications scene flow field robotics estimation motion highly temporal performance high frame existing approaches ranging compute scene flow computer vision computer paper dense real-time scene flow algorithm rgb-d variational highly primal-dual algorithm proposed solve real-time algorithm total variation regularizer dual approximations fast implemented variational optical flow range flow constraint equations applied scheme allow displacements points consecutive regularization imposed surface order smooth flow field close points real space pixels image experiments evaluate performance approach compare work flow approach achieves accuracy magnitude quantitative qualitative primal-dual scene flow estimate heterogeneous non-rigid motions work scene flow computed data stereo camera term proposed compute optical flow apply range flow constraint equation local solution geometric data estimate optical global variational approach presented optical flow depth flow estimated quadratic allow motion total variation variational formulation compute motion field authors advantageous algorithms efficiently solve global gpu scene flow leading real-time stereo cameras realistic image motion rigid ieee local formulation accurate standard rgb-d cameras field robotics computer scene flow scene flow algorithms rgb-d cameras presented variational approach quadratic data regularization terms variational work weighted quadratic regularizer set large regularization weights functions depth regularization pixels depth values rgb-d flow presented achieves good optical range flow constraint equations weighted weighting function color surface work presented achieves accurate formulation adaptive weighted regularizer weights presented points large variational provide dense motion presented algorithm compute scene flow stereo represents good local global respect accuracy hadfield proposed particle filter estimate motion field depth intensity images rgbd work range rgb-d depth data image advantageous large paper scene flow algorithm rgb-d cameras primal-dual algorithm applied time solve variational formulation scene flow algorithm iterative solver efficiently implemented regularization standard adaptive represents flow field observed geometric data provided camera accurate approximation image well filter coarse-to-fine scheme variational problem estimating dense motion field scene instants time color depth images provided rgbd motion field defined image domain respect camera frame expressed alternative representation commonly expressed terms optical flow range flow depth equation directly values coordinates observed estimating optical range estimating motion implementation optical range flow constraint equations apply directly image order compute motion field minimization problem geometric consistency imposed well data term represents depth pairs compute regularization term smooth flow field solution data term intensity images depth images instants data term brightness constancy geometric consistency commonly existing optical flow scene flow approaches point brightness intensity coordinates image constancy intensity gradients approach high frame brightness constancy accurate depth time difference depth image second image optical conversely norm approximation minimize norm function weights geometric consistency brightness norm outliers norm primal-dual algorithm solve minimization problem minimize approximation details data term energy local order close global coarse-to-fine scheme image pyramid solution computed camera representation camera scene image image surface curve regularization smooth points close image image domain curve depth values color derivatives respect space solution computed equations optical flow range flow constraint equations global pyramid regularization term regularization term problem associated optical range flow well provide smooth flow paper regularizer flow field based total variation geometry standard image domain real proposed regularization observed surface scene curve motion field curve respect total variation defined vector vector curve computed function gradients coordinates image weighted geometry scene regularization pixels distant points space values geometric data conversely standard regularizer regularization close points distant points close points object scene distant points objects motion equation leading dxdy regularization term variational formulation flow dxdy dxdy associated optical flow term temporal performance primal-dual problem dual variables associated details associated implementation primal-dual scene image filter applied pyramid weighting parameters image optical scene flow difference approximation centered apply applied compute depth image gradients high values real gradients surface set observed estimated range flow high values object borders approximation gradients adaptive approximation image gradients consistent geometry observed weighted weighting functions capture geometry derivatives weights energy formulation based data convex regularizer formulation algorithm convex energy order solver primal-dual formulation dxdy dxdy dxdy dxdy dual variables regularizer dual data term iterative solver real-time implementation scene flow dual efficiently computed convex respect variables respect dual variables compute global primal-dual data term range flow minimize optical flow term details primal-dual alternative represents good fast formulation dual left derivatives terms left approximations presented derivatives borders approximation capture real surface pixels object borders terms standard centered approximations image commonly solution variational problem outliers outliers coarse-to-fine scheme pyramid flow median filter applied flow estimate filter motions objects applied weighted median pixels weighted local median weighting function defined kdt depth difference represents temporal depth kdt weighting function apply median filter flow fields time pixels high temporal details parameters weighting functions variational function presented weights geometric data term defined geometric consistency areas depth gradients depth gradients null depth parameters scene flow set kdt experiments evaluate approach test scene flow rgb-d authors stereo depth groundtruth intensity images rgb-d set images real rgb-d realistic large areas depth created test scene flow algorithms artificial rgbd images rgb-d frame motion quantitative qualitative qualitative real rgb-d real-time motion field evaluation data compare approach work terms accuracy temporal procedure evaluate estimated real motion field rgbd procedure capture rgb-d frame rgb-d artificial motion field consistent geometry apply motion field intensity depth images second rgb-d frame created real rgb-d image motion intensity depth images real respect rgbd pixels image intensity depth values set intensity values pixels null depth measurements second scene flow pixels null depth measurements pairs rgb-d images images generated procedure distinct motion fields maximum displacements ranging images realistic close distant objects error measurements error expressed error magnitude maximum magnitude motion field quantitative presented table flow standard tvg rgb-d flow qualitative rgb-d pairs generated evaluation motion rgb-d flow accurate approach optical flow range quantitative pd-flow regularization surface flow accurate rgb-d flow estimating norm motion field accurate temporal performance cpu gpu work test standard cpu gpu table cpu implementation pd-flow order magnitude faster rgb-d flow rgb-d flow gpu time intensity images depth images optical flow groundtruth optical flow flow tvg optical flow rgb-d flow range flow groundtruth range flow flow tvg range flow rgb-d flow color representation optical range frames frames frames color scheme range flow flow field areas null depth table quantitative evaluation scene flow distinct rgb-d frame nrms-v pd-flow tvg nrms-v pd-flow nrms-v rgb-d flow aae pd-flow tvg aae pd-flow aae rgb-d flow dual scene flow faster rgb-d maximum frame rate primal-dual solver accurate presented table table rgb-d flow flow cpu flow gpu ball energy distinct representation scene flow real movements point generated consecutive depth red vector field represents magnitude estimated motion images evaluation real data real-time qualitative scene flow high frame rate created alternative flow field performance scene flow computed fast heterogeneous non-rigid motion fields associated objects temporal sequence color objects norm red fast observed motion field ball red scene ball frames sequence maximum representation estimated motion field non-rigid movements point red flow temporal sequence magnitude motion field ranging red primal-dual scene flow estimate movements movements left images existing displacements approach high frame consecutive images motion frame rate maximum frames table movements faster second estimated scene flow algorithm rgb-d cameras variational geometric data depth images accurate regularization applied observed surface proposed image gradients geometry order minimize highly primal-dual solver proposed implemented gpu real-time magnitude faster work scene flow rgb-d applications real-time quantitative qualitative presented accuracy authors computer vision local global flow international journal computer dense motion estimation color robotics scene conference computer vision iterative image stereo international conference artificial scene flow conference computer vision pattern recognition scene flow centered variational international journal computer variational scene flow estimation stereo conference computer vision scene flow motion international journal computer scene flow estimation rigid motion conference computer vision rigid scene range flow flow depth color vision conference scene flow conference image flow estimation conference computer vision pattern recognition hadfield particle based scene flow depth conference computer vision hadfield particle based scene flow ieee pattern scene flow rgb-d conference computer vision pattern recognition algorithm optical approaches motion accuracy optical flow estimation based conference computer vision flow computer vision image algorithm smooth conference computer vision primal-dual algorithm convex applications journal time weighted median stereo conference computer vision
odometry scene flow rgb-d based geometric clustering paper solution jointly estimate camera motion scene flow rgb-d two-fold segmentation geometric clusters static moving dynamic scene set rigid clusters motion static dynamic parts separate camera motion rest motions observed method robustly motion rgb-d camera dynamic average milliseconds joint estimation motion camera motion objects problem computer vision mapping dynamic computationally problem solved visual odometry static methods number pixels observing non-static parts scene flow scene estimated non-rigid field observed points camera approach camera motion points scene treated static non-static regions camera scene flow estimation computationally existing approaches require seconds align pair paper method estimate motion rgb-d camera scene approach two-fold segmentation scene divided geometric clusters running k-means coordinates observed systems computer clusters treated rigid exploited scene flow computational cost scene segmented static moving static regions camera motion scene flow estimated moving propagate background segmentation static moving parts scene consistent image evaluation methods visual odometry scene flow approach estimates camera motion scene highly second scene flow main milliseconds running multiple cpu faster scene flow applied feature existing approaches visual odometry scene flow estimation highly joint typically visual odometry approaches exploited sparse feature correspondences estimate camera motion large require optimization multiple frames accurate camera methods provide dense scene estimate motion multiple rigid objects require feature points objects small regions rgb-d provide dense depth high dense methods cost penalize intensity error depth error error error feature space main methods require compute approaches multiple cost robust cauchy strategy works well scene static images observe moving fails moving parts scene flow estimated systems rgb-d approaches proposed compute scene flow rgb-d image regularization presented real-time good estimates small semi-rigid scene flow proposed representation flow camera motion best accuracy scene flow presented approach segmentation estimation process motion small motion field high align pair smooth flow proposed accurate flow estimates jointly scene moving parts computationally approaches estimation large motion field points range sparse correspondences points geometric flow optimization methods provide works non-rigid estimate camera motion flow objects estimates camera motion moving estimate set sparse rigid align moving object dense camera motion sparse color correspondences dense field associated object provide camera motions problem multiple static motion field estimated method proposed method jointly estimate camera scene motions rgb-d sequences pair frames intensity depth images defined image frame segmented geometric clusters k-means coordinates scene cluster considered rigid scene flow estimation motion estimated number associated cluster odometry computed minimizing photometric geometric residuals robust rigid motion scene dynamic camera motion estimate segment scene static parts moving images warped average residuals cluster clusters moving low segmented clusters high residuals warping motion moving require set sharp threshold representation define cluster moving segmentations details background segmentation segmentation motion estimation process separate clusters background precise odometry non-static piecewise rigid scene flow estimated rest cluster rigid background segmentation odometry warped moving objects moving iteration segmentations consistent estimation motion field cluster scene points treated rigid existing strategy well faster scene flow iteration robust odometry background segmentation scene flow estimation propagate segmentation k-means clustering background odometry representation main estimation process pair rgb-d process background segmentation frame k-means presented compute k-means clusters based coordinates observed strategy close points space rigid representation image clusters computed computed rest spatial coordinates k-means consistent clustering image propagate frame cluster clusters spatial clusters points rigid person close clusters points person estimated motion clusters motion solved number cluster motion higher computational robust dometry odometry computed minimizing photometric geometric residuals rgb-d geometric residuals photometric residuals defined graph clusters contiguous space graph exploited background segmentation contiguous clusters segmented contiguous clusters sharp motion scene flow estimation number clusters leads large scene regions parts scene clusters points motion robustly clusters leads clusters image number cluster regions high main approach cluster rigid pixel image function points image associated warping function formulate dense optimization problem camera compute cauchy m-estimator min number pixels cauchy m-estimator good robust parameter weights photometric geometric parameter estimation robust high pre-weighting applied camera pre-weighting two-fold pixels clusters segmented moving objects pixel moving previous distant points observe static parts highly motion solved residuals image solution background segmentation estimate odometry scene separate static parts scene moving camera camera scene motion static non-static objects robust odometry pattern motion evaluation pixels cluster rigid rgb-d frames warped clusters background low photometric geometric residuals associated moving objects segment scene static non-static process residuals good metric precise image intensity depth images pixels background object tend color clusters perfectly high residuals color occluded pixels high residuals images perfectly depth error geometric residuals distant clusters tend higher clusters close background segmentation divided compute robust metric residuals cluster formulate minimization problem segmentation clusters based average previous segmentations computed robust average residuals computed number occluded pixels cluster average depth occluded pixels considered geometric residuals pixel occluded zocc formulate minimization problem background dataterm clusters segmented background residuals define mapping low high residuals define dataterm function dataterm residuals clusters background moving regularization term clusters defined term smooth clusters sharp provide better temporal regularization static dynamic parts scene moving penalize smooth regions optimization process precise term background clusters fact moving objects tend distant optimization problem solution parameters introduced scene low dometry scene motion estimation separate clusters segmented background considered rigid rigid motions moving clusters computed scene flow associated cluster range separate static moving threshold three cluster assumed static utilized odometry cluster assumed moving utilized scene flow cluster uncertain utilized odometry scene flow clusters segmented odometry estimation compute rigid motions moving clusters minimizing min weights penalize pixels spatial temporal provide strategy better robust minimization details details algorithm set values parameters introduced cauchy m-estimator parameter robustly residuals parameter set average photometric geometric residuals iteration robust odometry scene flow odometry parameters segmentation compute robust residuals associated clusters clusters moving objects typically residuals higher set observed average residuals motion camera camera odometry computed minimizing background optimization pre-weighting scene clusters residuals low threshold assumed background high threshold assumed moving rest parameters introduced set set photometric geometric residuals threshold zocc set geometric background segmentation set depth scene divided weights segmentation set set values presented paper provide good typically rgb-d values method applied table pixels wrongly segmented background uncertain regions sequences moving objects uncertain divided main evaluation odometry evaluation scene flow rgb-d images cpu presented method odometry evaluation accuracy algorithm sequences selected sequences moving objects moving objects percentage pixels null depth tested method difodo dvo semi-rigid scene flow odometry scene accuracy method translational rotational proposed pixels valid depth estimate camera compute average percentage pixels valid percentage pixel valid depth frames rgb-d image number null depth table noticed static difodo translational approach rotational provided average times accurate dynamic sequences best high fact number points static sequences valid depth number non-static rgb-d frames valid depth fact observing moving odometry problem percentage pixels approach provide rgb-d pairs low valid depth leads higher rms segmentations provided method static compute percentage pixels wrongly segmented considered uncertain low three sequences scene flow evaluation approach three works scene flow scene flow semi-rigid scene flow smooth piecewise rigid flow presented rgb-d evaluation scene selected set rgb-d pairs observing objects tested images previous works half rgbd pairs moving camera accuracy methods warping rgb-d pairs estimated scene flow rms residuals associated fact occluded pixels high residuals images perfectly error method pixels geometric residuals pixel zocc rms residuals metric precise image segmentations estimated observed segmentations precise pixels segmented clusters points objects hand segmented moving object small presented table noticed best rgb-d strategy estimate clusters precise table dometry translational rotational second static average valid depth min valid depth translational difodo dvo sr-flow rotational difodo dvo sr-flow segmentations rgb-d frames tested clusters segmented background moving objects noticed moving parts segmented segmented perfectly image geometric method moving objects clusters wrongly segmented moving clusters observe points depth odometry estimation clusters segmented half image null depth pixels valid half observing static method fails segment scene fails estimate camera computational average methods milliseconds seconds seconds cpu seconds running cpu milliseconds multiple cpu approach second best scene flow estimates mc-flow higher accurate times faster mc-flow times faster sr-flow pd-flow faster paper presented method estimate odometry scene flow rgb-d method better odometry scene estimate method joint estimation problem accurate camera motion accurate scene well times main approach accurate low best method precise odometry scene flow frame high k-means clustering objects leads scene flow color segmentation process clusters temporal background segmentation consistent algorithm odometry estimation rgb-d conference robotics automation visual odometry conference computer vision visual odometry range conference computer vision pattern recognition conference robotics automation visual odometry dense rgb-d conference computer vision table flow rms photometric geometric residuals rgb-d pair person hand moving person camera moving rms photometric residuals pd-flow sr-flow mc-flow rms geometric pd-flow sr-flow residuals mc-flow selected images scene segmentation provided clusters segmented background moving objects observed moving objects segmented main rgb-d hand segmented moving object small observed small parts wrongly segmented moving objects motion depth accurate photometric geometric error rgb-d dense visual conference robotics automation real-time dense mapping depth color dense scene mapping rgb-d systems real-time visual odometry dense rgb-d conference robotics automation scene flow motion computer flow depth color vision conference dense motion estimation color conference robotics automation scene flow conference image real-time dense rgb-d scene conference robotics automation scene flow estimation rgbd conference computer vision rgbd scene flow conference computer vision pattern recognition smooth rigid scene flow rgb-d vision scene flow rgb-d conference computer vision pattern recognition large scene flow graph conference pattern non-rigid conference computer vision pattern recognition real-time non-rigid scene flow estimation piecewise rigid scene computer evaluation rgb-d conference systems
scene flow propagation semantic mapping object discovery dynamic street scenes scene understanding vehicles robots dynamic urban street robots persistent model static dynamic propose method stereo frame observations temporally consistent semantic contrast previous approach scene flow propagate dynamic objects method persistent occupancy well semantic belief static well moving objects noisy single-frame observations develop novel approach discover object instances based temporally consistent semantic cues evaluate approaches dynamic semantic mapping object discovery popular kitti benchmark demonstrate compared single-frame stereo frame depth scene flow visual odometry semantics semantic mapping previous map update spatial consistency vehicles urban street scene understanding traffic motion map static moving objects semantics scene object instances propose novel approach semantic mapping stereo explicitly takes motion scene account method maps occupancy static scene moving objects dynamic map stereo depth scene probabilistically filter image-based semantic maps order temporally spatially consistent semantic based persistent semantic representation dynamic propose object discovery approach finds object instances based motion cues contrast previous approaches semantic mapping urban street scenes approach filters probabilistic belief occupancy semantics static vision visual map dynamic environments occupancy grid maps stereo visual scene image-based semantic segmentation additionally temporally semantic maps spatially consistent dense scene motion dynamic objects estimated scene well dynamic potential semantic segmentation objects persistent dynamic method depth scene flow observed single stereo object discovery semantic segmentation objects demonstrate approach semantic mapping object discovery dynamic urban traffic evaluate method popular kitti benchmark compare approach single-frame methods demonstrate improvements semantic segmentation quality object main work propose method occupancy mapping dynamic environments based state-of-the-art methods stereo scene semantic fuse depth measurements semantic segmentation individual stereo frames dynamic maps temporal propose probabilistic filtering occupancy semantics belief map observed scene dense crf voxel grid order enforce spatial propose approach object discovery based aggregated motion semantic cues work autonomous vehicles robots semantic mapping scene mapping urban street scenes vision current state-of-the-art methods demonstrate consistent large-scale methods number moving objects slam static slam methods proposed explicitly static parts environment dynamic objects indoor rgb-d slam proposed separate dynamic parts static tracking propose slam method takes motion scene account map shape model methods scene semantics building scene approaches semantic segmentation integrated temporally spatially consistent global semantic mapping indoor label points aggregated point cloud map random field model points appearance label point cloud map rgb-d visual segments individual rgb-d images random forest classifier probabilistically filters labelling random forest dense random field spatial consistency semantic labels point cloud semantic slam approach filters semantic segmentation classifier slam based map approaches assume environment static semantic mapping semantic mapping street scenes stereo images object classes crf second crf image-based semantic segmentation ground plane fuse crf semantic labeling stereo images point cloud crf order enforce consistent semantic labelling points individual image-based semantic enforce temporal consistency bayesian filtering spatial consistency dense crf semantic crf image-based appearance contrast semantic mapping methods assume scene static apply random forest segmentation individual stereo images fuse stereo depth distance representation voxel voxels annotated object integrated measurements map observations static explicitly estimated motion objects account propagate occupancy belief map scene probabilistically filter semantic labelling static parts well moving demonstrate features temporally integrated map representation object previous approaches object discovery cues methods discover objects motion cues multiple frames approach filters semantics motion cues consistent map subsequent apply clustering temporally integrated map order discover approach takes stereo image semantic map map voxel grid occupancy semantic categories contained determine depth stereo images estimate camera visual odometry order camera integrate depth global map street scenes moving update voxels motion moving objects occupancy belief compute scene motion stereo image order motion accumulate flow voxels propagate occupancy semantic belief dynamic objects propose efficient grid takes flow measurements time extract semantic stereo images filter map compute semantic segmentation probabilistic label label bayesian update semantic category corresponding method map integrate multiple frames main integration time enforce temporal consistency semantic labeling additionally employ probabilistic model enforce spatial consistency semantic note filter distribution semantic labels temporal apply crf accumulated voxel grid accumulated measurements frames object proposal method semantic method clustering algorithm aggregated maps voxels objects proposals based motion semantic emantic dynamic stereo depth motion based stereo frames estimate visual image depth scene semantic mapping stereo visual odometry proposed method stereo street scenes stereo time scene flow methods estimate motion pixel state-of-the-art scene flow method pixel images method depth flow scene set segments rigid motion stereo frame note camera visual odometry semantic stereo image segmentation semantic segmentation mapping pixels stereo images category labels road apply semantic segmentation approach proposed based random forest probabilistic label temporally label probabilistic spatial crf image point cloud compute features supervoxel random forest classifier appearance features color compute color cielab color determine covariance computed shape points supervoxel computed ground plane point cloud grid grid grid compute second shape features covariance points compute surface normal supervoxel surface normal ground plane supervoxel scene distance ground distance point mapping dynamic scenes algorithm mapping occupancy semantics moving stereo camera dynamic main mapping method account motion time step algorithm current stereo frame extract depth map semantic segmentation semantic segmentation distribution labelling image pixel single-frame visual odometry camera frame time piece-wise rigid scene flow estimated frame current frame camera visual odometry visual odometry scene flow estimates time observations voxel map corresponding image kitti voxel map stereo average color image pixels average voxel flow voxel map map voxels voxel voxel occupancy semantic labelling probabilistic stereo images observations voxel occupancy semantic camera frame frame visual odometry scene observed scene occupancy semantic belief voxel bayesian filtering t-1 voxel-age frames kitti tracking voxel-age number frames voxel scene flow voxels dynamic objects static parts dynamic map prediction determine distribution t-1 t-1 t-1 visual odometry estimates filter prediction prediction step applies state transition model voxel map based scene subsequent update step depth semantic observations efficient integration stereo image-based accumulate scene flow measurements local local map grid temporally integrated local measurement map stereo occupancy label voxels time step based current scene flow note model occupancy label belief separate bayesian estimated scene flow model state transition occupancy label state transition model model t-1 t-1 occupancy label distribution propagation separate propagate belief previous frame scene flow distribution t-1 t-1 t-1 second step applies smoothing number state apply smoothing occupancy semantic approximate scene flow propagation particle propagation t-1 t-1 t-1 measurements voxels accumulated local measurements occupancy employ method model approximate occupancy belief voxel local measurement map point voxel generate set particles particles sampled distribution current flow measurement set particles voxel voxel flow particle number particles sampled scene flow estimate particle positions voxel map distribution scene piece-wise rigid scene flow approximate normal distribution scene flow covariance stereo depth estimates corresponding pixels subsequent stereo covariance stereo depth estimate pixel distribution scene flow normal distribution particles sampled voxel positions voxels normal voxels moving particles sampled occupancy semantic belief belief number particle efficient belief propagation occupancy semantics measurement integrate measurements global voxel map bayesian t-1 number measurements voxel local measurement map parameters voxels global map measurements local measurement map occupancy computed voxels accumulate semantic image segmentation local measurement map label distribution pixels set pixels voxel update semantic belief integrated map spatially consistent semantics spatial pixels voxels map semantic image-based classifier well bayesian mapping approach pixels voxels enforce spatial consistency voxel map dense random field belief label distribution voxels semantic maps model spatial features voxel positions average cielab parameters potential set large-scale mapping mapping current camera temporally integrated order large-scale fuse integrated map large-scale global voxel global map set belief integrated voxels average scene flow order static voxels voxel-age frames note determine voxels stereo frame apply occupancy dynamic emantic object proposals semantic maps individual images potential semantic time integration occupancy time noisy stereo depth single integration time map generate proposals objects individual propose employ density-based spatial clustering features semantic map points based distance clustering points number points points points points features voxel class average color cielab color average scene extract proposals multiple range values additionally occupancy voxels clustering values proposals bounding image proposals number evaluate approaches semantic mapping object discovery popular kitti benchmark kitti odometry evaluate semantic mapping based dense annotations training set random forest classifier evaluate segmentation method object discovery tracking ground annotations bounding objects training parameters method training values crf parameters grid map semantic segmentation frames training semantic dynamic mapping evaluate quality semantic labelling maps image-based ground annotations kitti odometry generate semantic labelling stereo frames belief contained semantic semantic labelling image pixel depth measurement object road building pixel class pixel class crf recall iou semantic map recall iou emantic segmentation kitti odometry corresponding voxel depth mapping approach road dynamic objects well static parts large-scale semantic segmentation finds consistent labelling traffic segments surface categories road segmentation quality compare approach image-based semantic segmentation method applies spatial smoothing dense crf note pixels pixel class label object recall iou average semantic contained maps clearly semantic segmentation baseline method improvements recall iou segmentation object classes classes image-based segmentation noisy stereo depth consistent integration map improvements persistent maps compared range object discovery proposal method temporally integrated semantic kitti tracking training set annotations object bounding note semantic segmentation set semantic labels set parameters object building object road large-scale semantic mapping kitti tracking large-scale second large-scale semantics temporally integrated object instances discovered temporally integrated object discovery categories camera camera object proposal kitti tracking semantic segmentation maps temporally integrated previous object proposals discovered maps temporally integrated previous object discovery three annotated categories three depth discovery algorithm object proposals bounding compare method density-based approach state-of-the-art object proposal method order depth maps piece-wise rigid scene flow baseline methods method gop method semantic segmentation stereo frames semantic object category recall number proposals depth demonstrate improvements compared previous work camera assume temporal integration method depth semantic approach takes motion appearance method three annotated categories kitti tracking method clearly gop baseline detection individual kitti scene flow occupancy single method baseline clearly main method gop temporal method proposals range objects traffic finds traffic proposed novel approach semantic mapping object discovery dynamic street scene flow propagate occupancy semantic belief maps temporally consistent semantic belief static parts environment previous dynamic based map develop object discovery approach noisy observations single stereo develop method scene understanding camera potential tracking discovered objects time occupancy voxel work semantic stereo ieee dense semantic stereo large-scale semantic scene ieee autonomous kitti vision benchmark ieee slam ieee large-scale slam stereo mapping detection tracking moving ground urban ieee mapping dynamic environments autonomous grid tracking based ieee real-time dense surface mapping reconstruction dynamic scenes vision reconstruction tracking scenes ieee labeling point indoor semantic mapping indoor scenes rgb-d ieee mapping semantics rgb-d real-time image dense visual semantic mapping temporally consistent semantic segmentation street ieee based semantic indoor ieee tracking discovery scenes shape ieee object discovery rgb-d ieee segmentation dynamic ieee model real-time semantic scene dense reconstruction vehicles rigid scene ieee vision object object tracking street ieee image object segmentation point random ieee reconstruction voxel density-based algorithm spatial discovery visual object classes object detection autonomous ieee
scene flow depth color images scene flow depth color images consider problem motion field multiple depth camera color cameras sensors geometric depth combined intensity variations color images order estimate dense motion handle large motions estimation linear problem solved existing scene flow takes geometric depth camera surface photometric constraints experiments real synthetic data provide demonstrate interest motion feature real scene artificial vision interest motion depth time-of-flight directly additional consider recover dense motion fields sensors vision directly provide sparse motion moving consider intensity images temporal variations estimates dense motion motion fields estimated optical flow multiple cameras allows motion scene flow intensity variations sufficient estimate motion additional constraints smoothness scene flow depth color images depth cameras provide geometric additional smoothness constraints depth cameras estimation motion instantaneous intensity variations depth motion build scheme proposed case multiple color depth scheme allows motion estimated temporal correspondences easily simple dense instantaneous motion moving depth approach work recover motion fields photometric consider motion fields consecutive images estimate optical flow normal flow constraints image intensity stereo images motion fields estimation estimation temporal consistency case multiple image structure motion estimated combined configuration considered structure consecutive images estimated motion scene optical flow fields estimated images directly normal flow constraints coming images interest work scheme photometric normal flow combined rigid deformation work configuration depth camera providing structure structure estimate motion field depth camera temporal intensity method proposed approach directly estimates motion field surface photometric takes input depth color images coming calibrated associated camera configuration single depth camera combined color consider depth camera color extension color cameras scene flow depth color images work surface flow proposed approach surface takes depth map consider simple configuration depth camera single color kinect step color image depth map cameras calibrated projection map points points color image depth image depth map yields points easily build mesh surface depth connected input approach estimates dense vector field set instantaneous motion vector figure optical flow projection estimated scene flow color image figure projection surface color image depth color order estimate flow problem data terms corresponding photometric consistency constraints combined regularization term motion data terms small large displacements regularization term deformation local explained scene flow depth color images photometric constraints work optical flow photometric large small constraints data term dense normal flow optical flow consistency consecutive point image context small normal flow equation surface --i --i jpv associated displacement point scene motion projection matrix equation yields data term low jpv sufficient estimate motion constraints motion normal valid small problem projection valid small displacements sparse feature correspondences order handle large displacements scene consider photometric consistency set sparse feature correspondences consecutive color images motion field easily images feature points features points motion error regularization features strategy features directly optical scene flow features associated points consecutive displacement associated data term set points corresponding scene flow depth color images motion field regularization normal flow constraints sufficient estimate providing additional features sparse local order estimate dense motion additional motion optical local global considered global regularization extension method global regularization term global smoothness estimated vector laplacian term rigid mesh order handle discontinuities explained smoothness constraints geometric depth solving terms term involves involves set linear constraints point solving linear laplacian matrix surface mesh weights point weights motion constraints coming data linear sparse solved existing equation solved scheme linear point terms equation solving low --i jpv implementation implementation weights mesh large small displacements scene flow depth color images laplacian weights weights matrix weights computer context mesh image points image connected mesh yields mesh corresponding depth order handle discontinuities regularization weights providing operator mesh regularization term operator surface laplacian matrix explained normal flow valid large optical flow existing strategy large strategy context involves image image regularization sparse feature correspondences estimation solving equation low step allows surface estimation surface color camera color image surface normal flow large small motion second resolution equation second sparse features displacement points motion field experiments large motions step second local scene flow depth color images order acquired synthetic data real recover depth color images experiments synthetic data synthetic data moving sphere moving sequence depth build depth map figure depth map resolution meshed surface figure sequence sphere moving cameras moving extension color cameras constraints equation figure synthetic color image depth map meshed surface approach method scene method method easily presented figure motion displacement camera color figure error surface error error proposed method error error norm error synthetic data comparison method proposed proposed method depth discontinuities sphere considered connected meshed surface regularization scene flow depth color images displacement norm color displacement color figure synthetic data comparison method proposed method error displacement norm error displacement figure error synthetic data comparison method proposed method experiments synthetic data normal flow deformation rigid feature correspondences color cameras features images real data proposed method real data acquired time-of-flight camera resolution color calibrated kinect color data depth resolution camera calibrated color cameras work presented order demonstrate proposed method handle large motions preserving motion discontinuities acquired sequence scene flow depth color images sequence figure input color image meshed surface displacement field time-of-flight figure input color image meshed surface displacement field kinect data displacement demonstrate interest method real data dense motion cameras time-of-flight motion meshed surface kinect resolution acquired data yields mesh linear case implementation data presented approach scene flow depth interest photometric scene flow depth color images directly simple camera sparse dense image approach large displacement preserving motion term work extension depth step depth coming sensors single surface dense motion fields acquired motion optical flow international journal computer scene flow computer vision pattern conference computer operator meshed computer vision pattern optical artificial method scene flow estimation stereo international conference computer scene flow stereo computer vision image image features international journal computer image stereo international conference artificial stereo international journal computer surface flow stereo scene flow estimation global international journal computer scene flow depth color images motion field estimation stereo image conference computer scene pattern motion preserving optical flow computer vision pattern
multi-scale scene flow binocular stereo sequences scene flow methods estimate three-dimensional motion field points video methods motion estimation describes dense scene flow estimation cameras stereo optical flow estimation problems estimation multi-scale method adaptive combined approach preserves discontinuities problems commonly basic multi-scale framework distributions optical flow uncertainty reliable estimation scene flow standard stereo optical flow methods experiments synthetic real data demonstrate effectiveness represents scene motion dense defined surface multi-scale estimation framework estimation errors scene flow regions contrast framework basic multi-scale distributions optical flow scene method order reliable combined approach preserves discontinuities problems commonly basic multi-scale improved framework good binocular work area motion work based rigid monocular techniques recover motion scene structure monocular image scene assumed rigid rigid non-rigid motion techniques non-rigid monocular techniques estimate non-rigid motion monocular image method motion deviation rigid approaches model motion motion stereo motion combined recover techniques rigid non-rigid approach approach dense motion methods estimate motion scene video applications estimation non-rigid motion number estimation motion problem estimation motion noise small number cameras problems estimation errors regions contrast regions surface improved algorithm computation nonrigid scene flow binocular video three-dimensional scene flow proceedings ieee workshop motion video computing ieee left sequence sequence distribution disparity distribution optical flow distribution optical flow scene flow figure system approach non-rigid dense scene flow optical linear algorithm compute scene flow optical scene flow scene dynamic scene structure zhang scene flow estimation problem scene flow computed fitting motion model image algorithm improved discontinuities approaches recover shape dynamic correspondence time improve accuracy correspondence estimate method pixels represents image represents image caused time error function optical flow stereo vision problem optical flow constant motion images captured time based approaches proposed approach distributions optical flow image approach probabilistic tracking motion approach improved proposed approach problem preserves distribution motion approach estimate disparity distributions optical flow compute scene flow integrated algorithm weighted squares described distributions flow uncertainty optical flow computation described gaussian noise system figure binocular camera images captured distributions optical flow disparity computed based approach described scene flow computed distribution flow disparity described approach optical flow disparity problems corresponding points optical flow correspondence time disparity correspondence function image pixel caused time image intensity function image coordinates time image temporal derivative image random describes error second random describes errors temporal derivative distribution gaussian distribution small covariance intensity image constant small number pixels optical flow distribution covariance defined flow proceedings ieee workshop motion video computing ieee pixel estimation flow distribution uncertainty model scale resolution finer scale resolution algorithm basic solution equation estimated flow field solution problem window adaptive local image multi-scale problem caused level level solve parametric model flow regions image commonly assumed motion pixels region parametric denoted algorithm flow scale finer linear scale flow field finer random represents uncertainty flow field flow assumed equation defined based standard framework time scale derived estimate set scale derivative scale temporal derivative process parametric model fitting model corresponds model corresponds motion model corresponds weighted squares equation estimate model region error uncertainty model approach pixels region reliable flow fitting pixels pixels regions fitting weighted function pixels corresponding flow field covariance computed approach solution window size window size level level combined image segmentation based performed proceedings ieee workshop motion video computing ieee input image input image image pyramid segmentation pyramid flow figure flow computation sequence input image input image image pyramid segmentation pyramid disparity figure disparity computation data set resolution level image order parametric model fitting adaptive resolution region size fitting residual order model order model improve fitting residual error fitting model region size region region flow field model fitting performed region small error residual figure process computing optical flow sequence row projection camera distribution disparity algorithm optical flow computation computing disparity input image captured stereo time view left view view binocular stereo corresponding variances optical flow disparity computation constant disparity multi-scale based problem optical figure process computing disparity disparity data set solve scene flow cameras system xv1 yv1 xv1 yv1 xv1 yv1 computing scene flow camera camera scene flow defined motion field points optical flow motion field points optical flow projection scene flow image point image point view denoted solution squares error scene flow optical xvn yvn xvn yvn xvn yvn xv1 yv1 xvn yvn integrated approach correspondence problem time compute scene flow optical number cameras solve proceedings ieee workshop motion video computing ieee improve accuracy covariances derived computation optical flow covariances disparity optical flow linear system scene flow small number estimated scene flow covariances applications probabilistic tracking motion structure stereo disparity corresponding image coordinates indicates left view indicates denote denote cameras assumed equation image coordinates left cameras pixel disparity left solve scene angular error standard deviation angular error method method method method method method variance gaussian noise magnitude error variance gaussian noise standard deviation magnitude error method method method method method method variance gaussian noise variance gaussian noise covariance covariance algorithm algorithm computing scene flow small fvl fvr compute compute model fitting described compute set solve solve figure angular error magnitude error synthetic data gaussian fvr fvr pyramid images image resolution level image resolution optical flow computed level pyramid binocular denoted disparity denoted algorithm describes integrated method computing optical disparity scene experiments experiments demonstrate effectiveness weighted model synthetic data effectiveness weighted squares points surface scene optical flow disparity scene flow gaussian point magnitude corresponds non-rigid gaussian noise variances optical flow methods accuracy computed scene flow compute scene flow frames stereo video fvl denote left video fvr denote video image fvl fvl proceedings ieee workshop motion video computing ieee average angular error average magnitude computed scene flow standard deviation angular magnitude error estimated scene flow based average method incorporating covariance method covariance optical flow method covariance optical flow variance disparity figure standard deviation angular magnitude incorporating covariance optical flow variance scene flow estimated weighted multi-scale integrated algorithm scene flow computation proposed covariances variances probabilistic framework optical flow disparity computation combined estimate scene experiments synthetic real data demonstrate good framework covariances estimated scene covariances derived image data reliable estimated flow covariances good model based tracking work incorporating framework tracking applications video analysis scene non-rigid linear optical flow images parametric local analysis method moving framework model image optical dynamic structure stereo vision image stereo motion structure estimation stereo shape nonrigid motion estimation recovery nonrigid motion optical flow field approach motion analysis sequence stereo multi-scale optical vision real scene algorithm experiments performed real scene row figure frames binocular video sequence captured sequences captured binocular stereo camera frame stereo sequence resolution scene flow algorithm experiments dense scene flow computed frame sequences binocular video sequences estimates optical flow disparity motion scene hand left second row figure projection left left described variance velocity reliable variance variance image local image contrast cameras binocular camera figure sequence row figure covariances proceedings ieee workshop motion video computing ieee input left frame left frame frame frame estimates variances flow projection flow project velocity variance velocity input left frame left frame frame frame estimates variances flow projection flow project velocity variance velocity estimates variances flow projection flow project velocity figure real velocity intensity area represents hand moving area indicates hand moving row second sequence distributions optical recovery 3-d shape nonrigid three-dimensional scene binocular image 3-d motion estimation sequence stereo shape recovery dynamic zhang integrated scene flow structure recovery image zhang scene flow structure zhang estimation 3-d frames proceedings ieee workshop motion video computing ieee
online computer vision image understanding 110 multi-scale scene flow binocular stereo sequences sclaroff computer 2007 online 2007 scene flow methods estimate motion field points multi-camera video methods combine reconstruction motion paper describes formulation dense scene flow estimation reliable cameras stereo optical flow estimation single proposed algorithm distributions optical flow account uncertainty allows reliable estimation scene flow previous methods handle problems estimation optical flow multi-scale method region-based combined approach preserves discontinuities problems multi-scale experiments synthetic real test data demonstrate proposed 2007 scene optical methods estimate motion scene video streams multi-camera estimates scene flow applications model reconstruction motion video applications non-rigid scene flow estimation motion estimation problem estimation motion image estimation motion generally image order corresponding sclaroff 2007 reliable estimate large cameras stereo problems estimation errors regions low contrast areas surface estimation algorithm handle problems principled paper method account errors non-rigid scene flow estimates image measurement contrast scene flow methods image measurement errors propagate optical flow disparity scene flow method explicitly models uncertainty optical flow disparity uncertainty estimation scene experiments data ground truth incorporating angular magnitude errors estimated scene flow standard deviation smaller sclaroff computer vision image understanding 110 paper unified multi-scale algorithm estimation optical flow compared proposed region-based algorithm preserves optical flow disparity region parametric model fitting contrast region model fitting process uncertainty require parameters robust error algorithm estimates optical flow disparity regions image making adjacent regions valid estimates optical flow demonstrate improved accuracy optical flow disparity standard data sets case estimating dense scene flow setup cameras stereo approaches require cameras gain reasonable estimate scene reported experimental algorithm two-camera setup three cameras two-camera case covariances flow disparity estimation explicitly discontinuities region-based algorithm includes cameras estimating multi-scale scene flow error covariance cameras captured image streams image pyramid compute distributions optical flow regions image segmentation displacements optical flow region-based approach described estimates displacements error covariance weighted squares formulation estimate scene flow displacements scene flow propagated pyramid estimates pyramid combined algorithm described integrated approach allows principled propagate multi-scale work work category includes methods motion estimation point correspondence explicitly time camera compute distributions optical flow disparity region based algorithm compute scene flow error covariance distributions propagate estimates covariance level pyramid level camera image pyramid image pyramid sclaroff computer vision image understanding 110 recovered second category includes methods dynamic depth map estimation point motion estimation large area motion work based setup rigid monocular sequence techniques recover motion scene structure monocular image scene generally assumed rigid rigid non-rigid motion techniques non-rigid monocular sequence making techniques estimate non-rigid motion monocular image deformable model motion recovered estimating parameters work motion minimizes deviation rigid non-rigid objects rigid component weighted combination shape shape bases component analysis shape points stereo tracking set shape bases online model-based monocular method proposed estimate shape motion time method parameters data points projection set proposed eliminate shape method provide solution projection methods feature points multiple video frames three methods motion assumption linear combination shape bases capture shape motion stereo multiple stereo motion combined recover techniques category scene non-rigid algorithms deformable model-based approach approach provide dense motion approach deformable model non-rigid approach dense scene flow optical linear algorithm compute scene flow optical flow fields video streams multiple scene flow initial scene dynamic scene structure scene flow estimation problem terms energy scene flow computed fitting affine motion model image global smoothness algorithm improved discontinuities occlusions depth occluded area simply scene flow case simply estimated multiple view optical flow computation optical images surface depth recovery scene flow energy minimization energy minimization global local occlusions handled case energy minimization approaches category regularization terms determined dynamic depth map approaches recover dynamic depth map time making motion output scene scene assumed motion depth map time algorithm minimizes image matching approaches category recover shape dynamic correspondence space time approaches methods multiple frames space time techniques online require improve accuracy correspondence three output dynamic depth correspondence computed scene flow proposed method compute dense scene flow fields multiple camera setup optical flow sclaroff computer vision image understanding 110 assumption scene motion eliminate uncertainty computation scene incorporate propagate uncertainty principled consistent solve scene incorporating uncertainty improve accuracy propagating uncertainty propagated uncertainty applications tracking motion proposed algorithm global method local method multi-scale framework discontinuities fill-in areas images low occlusions handled region merging occluded regions valid estimates adjacent experiments synthetic incorporating covariance optical flow estimation errors terms magnitude estimated scene flow standard deviation smaller experiments real two-camera setup presented three cameras approach optical flow disparity estimation problems corresponding points optical flow estimation images disparity images captured cameras techniques estimate optical flow disparity techniques solve problems techniques generally methods phase methods function regularization variational framework solve displacements flow methods methods optical image methods optical flow disparity handle discontinuities estimating flow methods image combination local region model fitting computing optical flow stereo methods uncertainty formulation optical flow disparity previous global approach simoncelli optical flow estimation problem global approach multi-scale approach coarse-to-fine local motion model fitting method image model fitting process weighted squares output motion fitting process linear map estimates region-based approach problem disparity function pixel image camera pixel displacement caused change time camera represent represents spatial image represents change image caused time equation optical flow stereo vision problem single linear equation regularization formulation pixel neighborhood smoothness pixel displacements neighborhood smoothness large displacement pixels neighborhood images alleviate based approaches simoncelli proposed approach computes distributions optical flow approach computes covariance flow estimates pixel based contrast local neighborhood multiple approach improved variance estimates provide error scale estimates estimation proposed approach takes problem preserves estimate flow distribution approach estimate disparity distributions distributions optical flow compute scene flow integrated algorithm weighted described sclaroff computer vision image understanding 110 flow disparity distribution formulation scene flow estimation yields case compared region-based approach proposed algorithm system parameters yields good estimation distributions optical flow uncertainty optical flow computation described gaussian noise solution state equation estimated flow field image intensity function image coordinates time image temporal derivative image random describes error assumption constant small second random describes errors temporal derivative based constant brightness assumption map estimate prior distribution optical flow considered distribution flow covariance scale level image linear scale flow fields finer-scale flow state matrix standard kalman random represents uncertainty finer-scale flow fields flow assumed measurement equation defined based standard kalman filter framework kalman filter time scale optimal derived estimate scale set scale derivative pixel neighborhood refers window gaussian filter size weighted coarse-to-fine estimation flow distribution constant brightness assumption large motion alleviate problem propagate uncertainty levels finer-scale levels simoncelli coarse-to-fine algorithm kalman propagate kalman gain process temporal derivative warped kalman filter weighted squares propagate covariance previous spatial scale current spatial region-based parametric model fitting approach solution window size neighborhood solution problem window size window local image multi-scale problem caused solve making parametric model fit flow regions image motion estimation assumed motion pixels sclaroff computer vision image understanding 110 region parametric motion model fitting denoted models algorithm fit flow model corresponds model corresponds affine model corresponds models weighted squares equation yields estimate model parameters region formulation robust error uncertainty model explicitly models local image intensity pixels flow computation yosemite sequence sclaroff computer vision image understanding 110 table evaluation proposed approach yosemite sequence method simoncelli method average angular error standard deviation function optimal solution pixels optimal flow field corresponding covariance k0v computed flow computed area error k0v region reliable estimates fitting pixels pixels regions fitting combined evaluated combined image segmentation based disparity computation teddy data set sclaroff computer vision image understanding 110 table evaluation proposed approach stereo data set evaluation nocc disc nocc disc teddy nocc disc nocc disc represent pixels pixel disparity error error acceptable disparity indicates average algorithm algorithms errors reported column errors evaluated areas errors reported column errors evaluated image errors reported errors evaluated regions depth discontinuities depth discontinuities resolution level image segmentation shift order parametric model fitting set defined order parametric model resolution region size fitting residual model model error residual error fitting model region size region shift region flow field model fitting step region small error residual process computing optical flow flow field yosemite sequence table angular data sets column left image stereo output disparity maps second error pixels disparity error pixels pixels third disparity error maps sclaroff computer vision image understanding 110 error indicates combined angular error terms standard deviation compared method produced error state method ground truth yosemite sequence making difficult error reported performance unified framework disparity scene flow distributions disparity region model fitting algorithms depth map computation integrated algorithm optical flow computation compute disparity image captured stereo time view refers left view refers view binocular stereo displacement corresponding variances estimation disparity optical method estimate optical flow combine consistent scene flow process computing disparity standard teddy data set performance algorithm evaluation data set provided algorithm average data set provided table evaluation parameters provided online evaluation system error acceptable disparity estimated disparity considered ground truth disparity estimated disparity considered evaluation system allows table method acceptable disparity derivative displacement method data sets algorithm unified algorithm computing optical flow disparity construct image pyramids levels images pyramids compute compute image current level pyramid shift region segmentation fit parametric model current region process described model compute pixels single consistent algorithm computing optical flow disparity algorithm algorithm represent optical flow disparity pixel displacements images proposed unified algorithm presented algorithm estimation displacements constant brightness proposed algorithm problems estimating objects scene surface case large baseline second region model fitting process algorithm alleviate large baseline experiments real video data captured large baseline stereo cameras displacement pixels stereo proposed algorithm produce reasonable estimates region merging step fill-in takes algorithm good estimate optical flow accurate algorithms solve optical flow combine solve scene point estimating optical flow fields explicitly model optical flow disparity based image gain estimates distributions optical flow propagate uncertainty computation scene principled propagating uncertainty estimates scene flow propagated error covariance computing scene flow unified formulation optical flow computation scene cameras change sclaroff computer vision image understanding 110 scene flow defined motion field points optical flow motion field points optical flow simply projection scene flow image point image point view denoted cameras assumed equation image coordinates left pixel disparity left solve scene projection matrix camera motion determined binocular derived disparity flow field estimates optical flow covariance error covariance scene flow solve scene flow cameras setup system simply oxcn oxcn oxcn oxcn solution minimizes squares error scene flow optical integrated approach image correspondence problem time difficult estimate scene flow optical flow reported cameras solve improve accuracy incorporate covariances derived computation optical flow covariances disparity optical flow linear system produce reasonable scene flow small estimated scene flow covariances applications probabilistic tracking motion structure stereo disparity corresponding image coordinates indicates left view indicates denote baseline denote algorithm describes single integrated method computing optical disparity scene compute scene flow frames stereo video denote left video denote video image pyramids pyramid images image resolution level image resolution refers pyramid image level image construct pyramid captured left camera time optical flow fields computed level pyramid binocular denoted disparity denoted experiments sets experiments conducted demonstrate effectiveness weighted squares method performance algorithm algorithm computing scene flow construct image pyramids levels images compute compute sclaroff computer vision image understanding 110 region segmentation fit parametric model flow computed disparity pixels region compute k0v pixel region segmentation fit parametric model flow computed pixels region compute k0v pixels set solve initial solve angular error standard deviation angular error method method method method method method variance gaussian noise magnitude error variance gaussian noise standard deviation magnitude error method method method method method method variance gaussian noise variance gaussian noise angular error magnitude error synthetic data gaussian sclaroff computer vision image understanding 110 synthetic data effectiveness weighted squares points surface scene optical flow disparity motion points surface deforming gaussian point corresponds non-rigid gaussian noise variances optical flow noise variances average displacements optical flow three methods propagating noise estimated computing optical flow accuracy computed scene flow average angular error average magnitude computed scene flow standard deviation angular magnitude error estimated scene flow reported based average experiments noise method incorporating covariance method covariance optical flow method covariance optical flow variance disparity standard deviation angular magnitude error frames based recovered scene gain experimental noise yields reliable scene flow estimating scene flow real image optical flow disparity camera noise image image regions texture image regions low contrast motion capture image accurate optical flow disparity image evaluated algorithms account errors estimating scene improve accuracy effectiveness algorithm real video sequences real algorithm experiments conducted real scene experiments error assumption displacements constant small error computing temporal frames synthetic data level surface deformation estimated scene surface deformation estimated method surface deformation estimated method surface deformation estimated method sclaroff computer vision image understanding 110 covariance matrix initial represents prior distribution image pyramids test parameters determined three video sequences presented sequences captured binocular stereo camera capture stereo sequence resolution scene flow algorithm experiments conducted dense scene flow experimental three real presented sequence moving second sequence motion hands moving deforming sponge third rows demonstrate estimation estimated scene flow left third column velocity intensity darker area represents hand moving brighter area indicates hand moving column variances velocity darker areas represent estimates velocity reliable brighter areas represent velocity estimates sclaroff computer vision image understanding 110 estimation three video third rows algorithm rows computed sequences binocular video sequences estimates optical flow disparity sequence movement hand movement left second projection estimated left left hands described second hands moving motion flow demonstrate recover non-rigid captured sequence deforming sponge texture sponge motion sponge projected recovered variances column variances darker areas variance brighter areas represent variance image local image sclaroff computer vision image understanding 110 contrast texture proposed movement captured proposed sequences displacement stereo pixels occluded areas captured produce reasonable estimates region merging step fill-in takes approach algorithm covariances ground truth difficult error image warped image image projected flow fields smaller warped image three test proposed method produced smaller warped image errors compared proposed method produced projected flow three test three cameras binocular stereo multi-scale integrated algorithm scene flow computation region-based probabilistic algorithm compute distributions optical flow covariances variances probabilistic multi-scale framework optical flow disparity computation combined estimate scene occlusions large displacement handled region merging process allows occluded regions valid estimates adjacent experiments synthetic real data demonstrate performance cameras compared performance uncertainty computation region proposed method computes covariances estimated scene covariances propagated image data provide reliable estimated scene flow covariances provide good tracking proposed approach multi-camera setup improved estimation scene approach multi-camera setup camera incorporate proposed algorithm tracking applications applications stereo video analysis scene work includes formulation prior shape eliminate dense disparity map estimation image based visual image reliable estimation dense optical flow fields large ijcv non-rigid linear reliable performance optical flow ijcv estimating optical flow images parametric models local pami stereo matching image vision computing variational motion segmentation level robust approach feature space pami method moving ijcv framework depth computing image computation component image velocity local phase ijcv stereo algorithm dense depth maps preserves image vision applications model monocular experiments feature based stereo pami model image optical tracking moving objects based local surface structure local phase image understanding optical local surface structure local phase image understanding techniques disparity image understanding region matching motion estimation image understanding image vision computing reconstruction dynamic structure objects stereo vision applications sclaroff computer vision image understanding 110 distributions optical dynamic depth recovery multiple video non-rigid shape visual recovery 3-d shape nonrigid scene moving images image binocular image pami image phase based disparity solution non-rigid shape motion 3-d motion estimation sequence stereo pami shape recovery dynamic integrated scene flow structure recovery image scene flow structure dynamic scene estimation displacements 3-d frames pami consistent segmentation optical flow video view occlusions pami optical flow local image matching method image stereo model-based motion structure estimation stereo shape nonrigid motion estimation pami region based video flow segmentation affine region visual image smoothness temporal estimation accurate flow computation ijcv recovery nonrigid motion pami variational scene flow estimation dense depth map minimization regularization approach preserves evaluation dense ijcv evaluation dense stereo correspondence ijcv model disparity unified optical flow field approach motion analysis sequence stereo multi-scale optical computer vision
colour flow addition colour computation flow improve accuracy applied additional optical flow constraints aligned colour image combining constraints velocity displacement synthetic real depth depth benefits depth data capture computation dense velocity surface scene multiple orthographic application optical-flow techniques motion deformation flow space stereo techniques data range estimates observed optical-flow depth multiple optical-flow surface motion image computed raw stereo intensity depth flow estimates error approach raw range data stereo derive optical flow technique applied estimation approach directly estimation techniques range-flow computed directly original surface localised estimates directly basis flow scene range-flow capture motion deformation depth dense stereo aligned intensity colour directly initial improve reduce aperture density flow estimates case incorporating intensity optical-flow techniques colour work applied range data benefits colour basis additional colour channels actual surface intensity surface invariant colour benefits localised motion case deformation motion surface stereo standard optical flow techniques colour improvement simply adding intensity work channels adding colour optical-flow adding intensity range-flow work directly colour approach incorporating multiple aligned channels based squares range-flow adding colour accuracy aperture colour flow channels original work depth surface range flow depth local aligned intensity colour channel observed flow colour intensity range channels case sensor aligned orthographic adding additional constraints aperture optical range velocity case case localised motion combining multiple estimates channels range aperture combined constraints lead improvement colour cie initial input colour space effects rgb values nrgb raw rgb colour cie directly intensity space combining additional channel depth depth raw colour depth cie lab depth depth invariant colour depth combining constraints optical-flow estimate motion assume squares flow assume orthographic technique approach channels combined weighting basis intensity depth data adding multiple channels constraints simply robust invariant estimation contribution channel combined assume channels sensor directly channels depth initial sensor simply channels local channels noise aperture based squares approach weighting local neighbourhood contribution squares contribution weighting channel neighbourhood channel greater estimation neighbourhood flow flow estimate pixel aligned input channels robust estimation provide best applied channels smoothing smoothing flow estimate computed combined weighting neighbourhood synthetic data benefits additional channels accuracy displacement synthetic pattern 100 100 figure figure synthetic slope splaid depth rgb splaid surface generated space colour depth values generated observed displacement depth motion colour values pattern aperture colour generated figure noise standard input channel sensor figure colour data accuracy standard magnitude error angular error estimate fest displacement fcor fest fcor fcor fcor fest fcor fest 180 angular error zlab zrgb znrgb 100 magnitude error zlab zrgb znrgb 100 magnitude error angular error displacement displacement figure translation slope error flow effects translation slope dataset figure pixel derive estimate based aperture slope combining additional channels estimates rgb nrgb provide best addition lab robust addition estimation colour effects translation surface splaid dataset figure channel local surface estimate estimation combined rgb nrgb improvement addition lab colour robust channels 180 angular error zlab zrgb znrgb 100 magnitude error zlab zrgb znrgb 100 magnitude error angular error displacement displacement figure translation splaid addition noise slope dataset nrgb rgb figure magnitude error slope dataset combined intensity lab noise smoothing combining channel rgb input intensity channel intensity noise 100 magnitude error zlab zrgb znrgb 100 magnitude error zlab zrgb znrgb magnitude error magnitude error displacement displacement figure noise real data accuracy colour work scene flow motion stereo capture capture frames pixel dense stereo data stereo original image aligned colour data figure reduce figure real data squares approach derive range flow estimate frames incorporating additional channels figure 180 270 360 180 270 360 180 270 360 180 270 360 180 270 360 180 270 360 figure magnitude estimates frames zrgb additional channels lead density estimation channel zlab capture best motion actual surface displacement original frames depth displacement range-flow zlab znrgb zrgb actual addition colour constraints range-flow improve estimation depth aperture combined raw rgb channels reduce error simply combining colour provide robust greater channels lead greater density accuracy data capture work case optical image optical flow pattern optical optical flow computer computation optical motion computer image image technique application stereo range flow application image image dense range flow depth intensity range flow computer image scene pattern
continuous optimization approach efficient accurate scene flow georgia usa georgia usa usa propose continuous optimization method solving dense scene flow stereo represent dynamic scene moving planar scene flow problem joint estimation normal rigid motion parameters complex optimization propose continuous formulation solved fine superpixel segmentation fixed propose factor graph formulation problem initialize initialization geometry motion global nonlinear evaluate method challenging kitti scene flow faster top scene flow optimization stereo optical flow factor graph geometry motion urban stereo problem autonomous urban scene supplementary supplementary international eccv estimate scene flow reference image stereo image pair temporal image pair image assign motion hypothesis superpixel initialization optimize factor graph accurate global flow map reference frame scene motion background figure analysis dynamic reconstruction geometry pixels image scene flow problem motion points scene scene flow stereo work explicitly reasoning scene flow stereo optical flow scene flow directly displacement stereo problem disparity flow motion stereo joint optimization solve solving scene camera motion estimation disparity motion field step optimization solve problem semi-dense scene flow achieved efficient accurate estimation scene flow dense stereo optical flow challenging reasoning scene cope problem scene stereo camera scene motion depth continuous optimization approach scene flow motion scene large displacement optical flow based rigid moving planar scene assumption achieved scene planar scene flow problem optimization problem pixel planar continuous rigid motion parameters view scene flow discrete assign best label plane set moving plane temporal achieve depth motion approach problem discrete optimization joint inference complex multiple planar assuming set moving objects solve motion objects continuous discrete optimization assign label object assumption scenes non-rigid continuous planar assumption achieves state-of-art optical flow planar better solve scene flow continuous rigid planar representation solve high reasoning discrete fine superpixel segmentation fixed robust nonlinear approach cope depth motion assumption fine superpixel segmentation optimize superpixel segmentation scene flow piecewise continuous initialization deepmatching achieve fast inference sparse nonlinear discrete census cost fast robust cost evaluation continuous differentiable approach work propose formulation scene flow problem sparse directly manifold continuous compared representation achieve better accuracy faster directly solving propose geometry motion helps cope nonlinear initialization nonlinear optimization deepmatching algorithm semi-dense set feature initialize motion planar initialize planes set motion optimize continuous cope non-rigid objects scene flow analysis follow assuming locally smooth rigid set rigid planes moving parameters plane normal motion plane superpixels geometry locally problem planes images set normal plane plane reference point equation optimization manifold plane rigid plane transform superpixel assume superpixel reference frame boundary adjacent superpixels induced moving planes representation point reference corresponding point observed frame transform reference frame observed image frame t01 plane motion reference frame camera matrix homography transform induced t01 stereo frames planes homography reference frame frame continuous optimization approach scene flow proposed factor graph scene flow unary factors set based homography transform factors set based locally smooth rigid geometry factors incorporating temporal factors represent transform reference frame stereo t01 reference frame planes factor graph formulation scene flow images reference image observed estimate planes raw image assume set point assume fixed superpixel segmentation measurements parameters assume superpixels joint factor graphs scene flow prior measurements constraints assume factor least-square error gaussian represent measurements constraints multiple factors unary point homography transform stereo point image pixels superpixel photometric factor fpho fpho t01 census error noise homography transform sparse matches estimate error corresponding plane t01 pairwise pairwise factors parameters based fsmoothb describes locally smooth assumption adjacent planes share boundary fsmoothb d-1 d-1 factor d-1 represents depth pixel describes distance points boundary plane expect boundary d-1 d-1 fsmoothb smooth motion expect adjacent superpixels share motion continuous optimization approach scene flow factor gaussian noise unary factor robust cost noise factor robust helps cope depth motion continuous optimization factor graph manifold factor graph estimated maximum non-linear square solved non-linear factors retraction retraction choice plane manifold optimization motion retraction factor graph sparse pairwise factors adjacent sparse matrix solve problem follow sparse matrix continuous census transform census distance bilinear distance census cost bilinear interpolation equation differentiable image census distance set helps achieve better scene flow estimation general step detailed initialize superpixels reference stereo estimate depth map plane initialized depth map figure bilinear interpolation achieve differentiable cost census census distance bilinear interpolation evaluate census cost planar graph solve factor graph factors estimation plane geometry parameter reference estimation motion estimate semi-dense matching reference frame temporal frame estimated plane set ransac find set motion ransac find motion hypothesis errors set motion hypotheses local motion graph initialize motion superpixels set motion superpixels motion estimate motion incorporating factors plane global graph set estimated factors initialization superpixels reference frame initialized superpixels urban scene complex initialized superpixel large cope superpixels plane find superpixels superpixel stereo method proposed stereo initialize planes ransac plane initialized ransac inlier continuous optimization approach scene flow plane homography transform plane camera robust matches disparity set matching factor census transform share maximum distance disparity planar graph optimization factors stereo factor estimate planes motion gaussian noise maximum factor graph least-square fpho rpho rmatch fsmoothb rsmoothb solve equation robust choice compared semi-dense matching ransac state-of-art matching method semi-dense matching large image estimate motion ransac inliers based small hypotheses hypotheses inliers step compared hypotheses ransac hypotheses complex high small moving motion objects matches inliers cope superpixels prior evaluate inlier superpixels inlier feature matches feature matches motion motion transform scene induced camera estimate camera transform hypothesis solved motion hypothesis optical flow scene motion flow camera motion explicitly scene motion image multiple motion hypotheses ransac smooth motion non-rigid estimated continuous figure local motion estimation estimation plane initialize motion individual plane set motion raw image measurements pair estimated depth frames sparse field estimate hypothesis individual assume set label superpixel assuming motion hypothesis prior corresponding map estimation equation presented edepth ephotometric ecluster edepth represents depth error depth superpixel ephotometric represents photometric error superpixel ecluster represents error edepth ephotometric ecluster homography transform depth pixel describes depth pixel feature point hypothesis continuous optimization approach scene flow local motion optimization hypothesis incorporating factors planes rpho rmatch rsmoothb prior factor prior matrix prior factor small general accuracy speed global optimization estimate global factor set reference factors set measurements reference rpho rmatch rsmoothb factors optimization algorithm fast stereo prior deepmatching method noise robust gaussian factors based training images optimization individual factor evaluation kitti evaluate algorithm challenging kitti scene flow benchmark benchmark kitti method scene flow test faster kitti optical flow test stereo test explicitly quantitative scene flow table table top scene flow proposed scene general errors disparity flow evaluation disparity flow estimation pixels scene flow error pixel table quantitative kitti scene flow test disparity errors reference frame frame flow error scene flow test images errors background pixels occ errors noc errors method occlusion error prsm osf prsf method 300 time min 150 min min min time error prsm osf prsf 300 min 150 min min min occlusion time method achieves top accuracy figure stereo frame disparity images flow figure method achieves state-of-art small occlusion directly discrete follow representation achieved better pixel errors faster continuous optimization approach scene flow compared method detailed test presented supplementary table quantitative kitti optical flow errors background error pixels noc error occ errors methods stereo method prsm osf prsf occ error noc error time 300 min 150 min min min table method compared state-of-art optical flow methods stereo methods deepmatching method best parameter table evaluate choice factor motion multi-scale census best choice factor detailed parameter presented approach solve scene flow problem continuous high accuracy kitti scene flow benchmark large faster inference non-linear least-square factor graph initialization multi-scale differentiable cost table evaluation error images kitti training corresponding factors stereo error flow error factors factors census matching census matching census census raw census multi-scale matching census piecewise motion census raw images reference view estimated disparity ground truth error estimated flow ground truth error disparity flow estimation ground truth kitti scene flow training continuous optimization approach scene flow optimize geometry motion global analysis fast accurate scene flow proposed method achieved speed challenging points cope photometric scenes expect constraints unary prior better local achieved optimization accuracy speed autonomous work scene flow view evaluation optical heidelberg factor graphs georgia square square scene flow ieee conference computer vision pattern recognition june method scene flow estimation stereo international conference computer vision ieee depth scene flow temporal dense motion disparity estimation heidelberg factor graphs ieee object scene flow autonomous ieee conference computer vision pattern recognition discrete optimization optical heidelberg reconstruction non-rigid scenes ieee conference computer vision pattern recognition june efficient representation scenes dynamic ieee june stereo reconstruction scene flow estimation global matching accurate motion field estimation stereo image eccv heidelberg interpolation optical ieee conference computer vision pattern recognition optical flow segmentation ieee conference computer vision pattern recognition quantitative analysis optical flow estimation joint estimation geometry stereo eccv heidelberg scene international conference computer vision scene ieee pattern scene flow estimation multiple eccv heidelberg piecewise rigid scene international conference computer vision scene flow estimation piecewise rigid scene scene flow motion efficient dense scene flow sparse dense stereo eccv heidelberg large displacement optical flow international conference computer vision continuous optimization approach scene flow efficient joint occlusion stereo flow eccv heidelberg accurate optical flow estimation piecewise ieee conference computer vision pattern recognition estimation frames ieee pattern object eccv heidelberg
object scene flow autonomous vehicles paper novel model dataset scene flow estimation autonomous advantage fact outdoor scenes small number independently moving scene rigid motion parameters superpixel plane well corresponding crf data term decomposes pairwise superpixels model scene dynamic performance model existing benchmarks well novel realistic dataset scene flow ground dataset 400 dynamic scenes kitti raw data collection detailed cad models vehicles experiments novel challenges handled existing figure scene flow proposed estimated moving objects background object flow flow ground include background motion moving camera realistic benchmarks scene flow ground quantitative evaluations synthetic images static scenes provided middlebury stereo benchmark kitti stereo optical flow evaluation paper scene flow model objects shape models model structure scene collection planar motion small number rigidly moving objects optimize parameter method scene independently moving novel realistic dataset quantitative scene flow 400 dynamic scenes kitti raw dataset visual rigid estimation dense motion scene dense motion scene well decomposition individually moving include robotics autonomous driving object motion input scene scene flow estimation focus autonomous number methods demonstrated performance advantage fact scenes small collection independently moving objects ieee scene flow ground disparity maps laser scans accurate cad models moving point clouds optical flow ground provide evaluation metric depth motion performance proposed method individual demonstrated respect set state-of-the-art baselines well proposed kitti stereo optical flow benchmark synthetic sequence experiments proposed benchmark novel challenges handled existing scene flow optical flow motion objects plane parameters superpixel shape motion proposals optimize model jointly object objects scene flow model parameters superpixel geometry object well small number parameters moving quantitative evaluation scene flow methods reference optical flow ground truth synthetic scenes quantitative methods middlebury benchmark set stereo challenging kitti benchmark evaluation benchmarks provide scene flow ground truth rigid scenes independently moving scene flow evaluations middlebury stereo dataset motions image evaluations kitti flow stereo problem realistic benchmark dataset dynamic objects ground truth evaluation scene flow optical flow advantage kitti raw data realistic challenging scene flow benchmark independently moving objects annotated ground 200 training 200 test scenes data collection paper scene approach details ground truth annotation dataset quantitative qualitative method state-of-the-art baselines three datasets work work work scene flow estimation existing datasets scene flow scene flow defined flow motion point work problem variational setting optimization smoothness depth rgb-d depth well paper outdoor scene flow estimation autonomous driving focus optical flow stereo proposed model pixel image rigidly moving plane optimization problem performance demonstrated challenging well kitti stereo optical flow benchmarks method scene advantage fact scenes small number rigidly moving objects jointly estimate decomposition well object scene flow focus scene flow setting input consecutive stereo image camera location flow pixel reference structure scene set planar superpixels number moving objects data term decomposes pairwise denotes data term superpixel plane parameters rigid body motion data term flow cross reference view image images illustrated stereo flow cross figure data superpixel reference view rigidly moving plane image matching superpixels associated plane variable object rigid note static elements scene handled object elements rigidly respect set superpixels superpixel associated image random variable plane object superpixel associated object associated random variable rigid body motion note superpixel associated object rigid motion parameters combination plane parameters scene flow pixel inside input images consecutive frames geometry superpixel objects rigid body motion object crf terms energy function terms defined matching pixels inside superpixel matching computed pixel associated object denotes camera maps point reference point camera coordinate system camera rigid motion matching pixel location reference image pixel location advantage dense well image dense sparse dense leverage distance truncated cmax cmax image parameter cmax flow cross sparse matching term defined sparse data smoothness denotes set superpixels data term data term models corresponding points images superpixel denotes pixel set sparse feature set pixels reference image correspondences denotes truncated function details sparse feature correspondences smoothness term smoothness term superpixels terms smoothness decomposes depth motion depth motion sgm disparity map optimization annotation denotes disparity plane pixel reference set pixels superpixel superpixel robust defined defined motion points optimization optimization crf respect superpixels objects problem leverage shape particles motion particles object iterations motion particles shape particles map shape particles proposed plane parameters combination superpixels stereoslic algorithm rigid body motions extracting motion estimates sparse scene flow estimate rigid body motions algorithm large number reader supplementary figure figure cad model model well corresponding disparity map minimizing flow disparity ground truth consecutive ground truth challenging individually moving objects laser data annotation static background scene dynamic objects dynamic objects fitting detailed cad models point clouds static scene elements dense point cloud static scene laser scans camera motion individual laser system kitti fitting point clouds yields accurate motion estimates optimization accurate individual scans coordinate scene flow dataset annotation datasets annotated 400 dynamic scenes kitti raw dataset points moving objects bounding provided kitti moving objects dynamic elements scene laser measurements leverage detailed cad models note stereo cad models required vehicles model set vehicles supplementary point cloud points cad point cloud fitting cad model frames sequence measurements illustrated dynamic object estimate model frame well rigid body motion parameter leverage three points moving object frames annotated bounding disparity estimates computed matching sgm estimates optimize small number parameters including term manually annotated correspondences cad model corresponding image including correspondences object accurate optical flow ground parameters minimizing energy function sgm sgm measurements error respect correspondences frame optimization minimizing respect terms disparity optical flow maps cad model images estimated objects leverage moving objects regions laser scans manually flow disparity maps visual manually annotated ground truth pixels large motions image ground scene flow evaluation metric error quantitative qualitative analysis model proposed scene flow kitti evaluation well synthetic sphere sequence huguet input leverage sparse optical flow feature sgm disparity maps sparse cross computed optical flow valid sgm superpixels stereoslic rigid motion parameters objects scene extracting rigid body motion algorithm model parameters coordinate training details estimated parameter values reader supplementary evaluation kitti benchmark evaluation provide stereo optical flow error proposed scene flow annotated total 200 training 200 test scenes kitti raw dataset method evaluate disparity errors flow valid ground truth pixel reference errors disparity flow pixels combination evaluation frame energy terms corresponding denotes truncated distance laser points inside bounding sgm cad truncated distance disparity map cad model occ occ occ occ occ occ number objects number iterations figure figure scene flow errors method proposed dataset respect number object proposals mp-pbp combination pairwise terms yields scene flow performance full model respect object set number object model outdoor scenes well small number rigidly moving performance method respect number mp-pbp iterations experiments iterations performance table method baselines novel scene flow sparse methods kitti variational approach huguet sparse scene flow method baselines state-of-the-art optical flow algorithms disparity estimates frames matching input rgb-d algorithms sphere flow provided required depth valid pixels sgm disparity maps include rigid scene flow approach note proposed approach baselines qualitative qualitative scenes proposed dataset color kitti note error scene flow metric error method disparity flow challenging objects rigid ground truth failure method illustrated scenes challenging moving objects algorithms evaluation kitti model static scenes kitti stereo optical flow benchmark methods pixels evaluation error rmse flow rmse disparity rmse scene flow table errors sequence respect annotation errors ground methods provide second disparity estimate second frame leverage estimated optical flow disparity values corresponding pixels frame background pixel reference values evaluate scene disparity values second flow evaluate combination three scene flow metric pixels flow evaluate errors image regions well regions image individual term energy proposed scene flow table regions supplementary errors terms disparity flow scene flow provide terms background foreground well combination table model including unary unary pairwise terms full model feature sparse optical flow combination combination individually terms scene flow experiments term pairwise error huguet sgm sgm sgm sphere flow unary unary unary unary pair unary pair unary pair unary pair unary pair unary pair unary pair table quantitative proposed scene flow table disparity flow scene flow errors 200 test provide errors background foreground objects well pixels image errors stereo errors optical favorably respect details full provided supplementary provide method synthetic dataset huguet dataset random object stereoslic algorithm dense optical flow disparity provide illustrated table method well fact scene 200 planar qualitative error maps supplementary mp-pbp yields total processing scene number shape motion particles setting number mp-pbp iterations total algorithm table full method favorably respect state-of-the-art supplementary reader supplementary analysis performance respect quantitative qualitative demonstrated dynamic outdoor scenes collection rigid jointly decomposition well geometry motion small number objects scene proposed model accurate dense scene flow favorably realistic scene flow dataset ground truth static dynamic objects kitti benchmark dynamic objects scene flow well novel challenges existing optical flow scene flow methods motions moving objects second failure stereo matching optical flow estimation required challenges dataset figure disparity optical flow ground truth reference disparity map optical flow map estimated error images color scenes failure scene flow view variational international journal computer vision extracting object proposals depth stereo european computer vision object stereo joint stereo matching object ieee computer vision pattern recognition large optical matching variational motion ieee pattern analysis machine intelligence scene flow estimation ieee computer vision pattern recognition robust ieee pattern analysis machine intelligence novel model dense variational scene flow rgb-d british machine vision stereo motion robust symposium pattern recognition scene ieee pattern analysis machine intelligence vision kitti international journal robotics autonomous kitti vision benchmark ieee computer vision pattern recognition dense ieee intelligent vehicles symposium scene flow estimation intelligent british machine vision rgb-d dense motion estimation color ieee international robotics stereo processing matching ieee pattern analysis machine intelligence scene flow rgb-d ieee computer vision pattern recognition huguet variational method scene flow estimation stereo ieee international computer vision energy ieee pattern analysis machine intelligence processing visual variational optical international journal computer vision international machine stereo scene flow estimation matching international journal computer vision dense scene flow estimation rgb-d european computer vision accurate motion estimation stereo image european computer vision ieee computer vision pattern recognition evaluation dense stereo international journal computer vision driving system ieee intelligent vehicles symposium quantitative analysis optical flow estimation international journal computer vision model foreground background ieee computer vision pattern recognition stereo vision british machine vision joint estimation structure geometry stereo european computer vision scene ieee computer vision pattern recognition scene ieee pattern analysis machine intelligence scene flow estimation european computer vision scene flow estimation rigid motion ieee international computer vision rigid scene ieee international computer vision scene flow motion international journal computer vision dense scene flow sparse dense stereo european computer vision european computer vision robust flow ieee computer vision pattern recognition joint stereo flow european computer vision visual european computer vision
edge-preserving simultaneous joint estimation image propose energy-based joint motion disparity estimation algorithm anisotropic diffusion operator correct displacement model left motions order increase euler-lagrange equation equation method method initial disparity current frame joint estimation disparity energy proposed algorithm accurate discontinuities boundaries performed field edge-preserving regularization method edge-preserving smoothing term compute optical flow model improve edge-preserving smoothing performance propose energy model correspondence estimation stereo image model energy reduce improve estimate disparity current frame joint estimation constraints initial disparity energy propose simultaneous joint energy model estimate left motions energy acquire correct displacement vectors reduce number displacement vectors motion flow image data image sequences data fields motions motions motion stereo image sequences compute motion constraints motion disparity stereoscopic image motion estimation corresponding points motion disparity motion flow consists motion number correspondence order smooth disparity fields discontinuities propose energy model simultaneous estimation regularization joint estimation constraint joint estimation accurate estimate motion disparity stereo image motion disparity stereo sequences vectors motion vectors disparity frame energy-based motion disparity estimation motion disparity fields smooth discontinuities boundaries order discontinuities regularization term proposed estimate motion disparity fields 18th international conference pattern recognition 2006 energy functional propose simultaneous joint energy model estimate left motions simultaneous joint estimation model figure simultaneous estimation model energy consists regularization dxdy il2 dxdy energy functional motion image weighting anisotropic regularized energy model diffusion operator fields boundaries energy functional disparity previous current energy functional motion left image sequences refers energy functional left motions simultaneous joint estimation functional consists data term smoothing data term refers data term left data term joint estimation smoothing term refers smoothing term left smoothing term smoothing term motion corresponding points points simultaneous joint estimation corresponding joint estimation acquire correct estimation disparity vector previous frame disparity vector estimated joint estimation constraint disparity current frame simultaneous joint estimation model joint constraint energy propose simultaneous joint estimation model acquire correct vectors reduce number vectors model simultaneous joint estimation constraints motion disparity energy data term ir2 dxdy energy functional point estimated left motion point estimated motion correct joint local energy-based order local initial field estimation performed proposed estimation motion estimation initial motion disparity field previous frame initial disparity field current proposed energy simultaneous joint estimation performed compute initial motion disparity vectors previous estimate disparity vector previous frame initial disparity estimate left motion vectors initial motion vector regularized disparity 18th international conference pattern recognition 2006 compute initial disparity vector current frame estimate disparity current frame initial disparity yields euler-lagrange equation euler-lagrange equation disparity left div div il2 ir2 div il2 ir2 div ir2 il2 solution solution figure sequences previous frame joint estimation weighting number model yields correct disparity map motion model yields correct motion map initial disparity vector motion regularization energy regularization term disparity motion convergence regularization term solution local convergence motion model regularization proposed energy-based joint correspondence estimation algorithm stereo image performance model stereo image anisotropic diffusion operator improve edge-preserving increase estimated motion disparity joint model initial disparity proposed joint data term increase estimated motion vectors left sequences proposed method edge-preserving performance convergence performed stereoscopic stereoscopic sequences point motion disparity weighting motion disparity disparity fields previous current motions simultaneous model smooth displacement initial disparity vector accurate disparity motion initial disparity vector joint estimation solution 18th international conference pattern recognition 2006 figure disparity motion disparity previous motion motions left image sequences initial disparity regularized disparity current frame figure energy regularization term disparity motion optical constraints estimation displacement vector fields image ieee ieee disparity map estimation image image estimation energy-based optical optical ieee ieee flow image ieee stereo correspondence 18th international conference pattern recognition 2006
evaluation scene flow estimation mordohai state evaluating performance scene flow estimation points generating ground truth dynamic evaluation methods estimate motion multi-view sequences shape criteria reconstruction computer models scenes including accurate models images reconstruction dynamic scenes despite applications static models virtual dynamic scenes applications include video markerless motion capture dynamic problem estimating shape motion images scene flow estimation vedula estimated scene growing progress problem models number estimated dierent consider methods consider methods space-time stereo long images mordohai dierent time depth taxonomy methods shape number criteria evaluating methods computer generating ground truth scene flow estimation generating progress binocular multi-view stereo optical flow taxonomy methods dense scene flow methods methods pixel depth three motion method point three three depth methods frames captured dierent estimate depth pixels methods estimate evaluating state space-time stereo motion small methods methods optical flow temporal correspondences frames matching depth time motion shape dynamic spatio-temporal form temporally consistent shape estimation based surfaces time frames methods sequences motion surfaces temporal correspondences frames images dierent evaluation scene flow estimation depth motion pixel methods estimate shape motion degrees pixel work based motion models image number degrees estimated state including variational shape stereo matching estimate shape motion methods space dynamic growing correspondence authors depth motion estimation depth temporal correspondences methods scene flow estimates shape motion methods vedula three algorithms scene vedula space space joint shape motion estimation include surface patches surfaces points rigid estimate scene flow single mesh degrees estimated capture shape motion estimation estimating motion frame motion estimates shape variational method prediction error shape motion surface patches long mesh point variational stereo scene flow motion shape methods estimate motion algorithms consider shape estimates input temporal methods tracking multi-view stereo frame mordohai dense correspondence surface patches methods estimate shape time frame temporal accuracy single evaluation criteria performance scene flow estimation ground truth depth scene flow estimation evaluated accuracy depth measured temporal correspondences performance algorithm static sizintsev wildes experiments cameras scenes ground truth experiments temporal reconstruction authors experiments synthetic data generating ground truth lidar depth depth pixels single lidar points lidar despite lidar markers algorithm evaluation ground truth motion motion texture scene generate ground truth optical flow optical flow texture measured motion errors real ground truth small number long quantitative optical flow synthetic data captured virtual cameras generating data ground truth motion requires eort form points points algorithms eort generate evaluation scene flow estimation frames camera ground truth data captured sizintsev wildes surfaces markers synthetic huguet devernay texture ground truth shape motion camera ground truth scene flow sizintsev wildes quantitative evaluation dense scene flow generate ground markers moving surface motion estimation accuracy algorithms markers evaluated accuracy algorithm long sequences small number points images time accuracy approach scene flow estimation algorithms include quantitative synthetic huguet devernay authors include synthetic synthetic scenes image prediction cameras images test errors evaluating view approach prediction error view rendering dierent real images virtual camera surface test input quality work technique synchronized mordohai image prediction background evaluation approach image prediction rendering depth dierent frame sequence depth pixels approach approach depth videos input temporal consistency criterion consider prediction dierent images time estimated scene evaluation requires eort mesh tracking evaluation shape despite errors criterion applications view temporal consistency background dierent criterion temporal consistency static scene pixels static sequence estimated small algorithm consistent depth background accurate depth sequence method pixels background methods require consistency ground truth videos sequences measured consistency scene flow estimates frames evaluation scene flow estimation shape estimates motion technique requires input data scenes ground truth depth rigid motion technique generate test sequences videos moving camera static scenes ground truth ground truth depth lidar depth cameras motion frames estimated point algorithm structure depth ground truth dense scene flow rigid points long algorithm evaluation global motion evaluation form criterion huguet devernay data images scene flow evaluated binocular motion prediction errors technique eort generate data image accuracy technique binocular methods moving camera data methods require state scene flow estimation problem progress evaluation synthetic video sequences rendering data ground sequences capture stereo matching motion estimation synthetic data number including camera evaluating criteria image prediction require eort view scenes ground truth depth rigid require eort generate capture scene flow consistency technique work mordohai silhouette stereo cviu multi-view stereo reconstruction scene flow estimation global matching ijcv multi-view accuracy dense multi-view pami scene multi-view scene capture video non-rigid shape ijcv spatio-temporal stereo ijcv shape animation acm graphics time applications markerless motion ijcv surface capture computer graphics applications mesh animation multi-view acm graphics spacetime shape recovery dynamic spacetime depth pami taxonomy evaluation dense stereo correspondence ijcv evaluation multi-view stereo reconstruction camera multi-view stereo evaluation optical ijcv approach dense evaluation scene flow estimation estimating disparity stereo video real-time spatiotemporal stereo matching temporally consistent disparity optical flow spatio-temporal spatiotemporal spacetime temporally consistent reconstruction multiple video real-time based disparity estimation temporal consistent depth recovery video space-time temporally space-time surface reconstruction acm graphics spatio-temporal shape silhouette correspondence surface dynamic depth recovery multiple synchronized video scene flow structure recovery image pami scene flow binocular stereo cviu dense motion disparity estimation variational method scene flow estimation stereo disparity estimation stereo sequences scene multi-view scene flow view variational joint estimation structure stereo scene flow estimation rigid motion real-time joint disparity disparity flow estimation graphics cviu scene flow estimation growing correspondence accurate motion estimation stereo image sequences mordohai optical flow motion estimation real scene flow motion ijcv spatiotemporal stereo scene flow pami shape motion real video computer graphics flow silhouette temporally consistent dense motion capture synchronized video dense motion capture dense accurate spatiotemporal multi-view estimation motion markerless shape motion capture video video dense correspondence animation reconstruction temporal surface tracking mesh dense non-rigid surface global temporal multiple non-rigid surface mesh evaluation optical prediction error quality motion quality video image video view acm graphics
variational stereovision scene flow estimation statistical similarity measures variational framework dense depth recovery dense three-dimensional motion field estimation multiple video robust camera spectral sensitivity illumination image matching problem backprojecting input images suitable solve matching problem case statistical similarity criteria image intensities method good problem problem structure motion solve ambiguous methods image intensities assumption stereo problem brightness constancy assumption optical correlation techniques cope affine changes image stereo problem optical flow matching techniques surface scene stereo camera scene plane windows order depth recovery three-dimensional motion estimation multiple video sequences image matching problem backprojecting input images suitable shape approximation plane approximation matching window window input images correlation smooth gaussian window surfaces backprojected volume order cope intensity statistical similarity criteria multimodal image computational framework energy well-posedness minimization three-dimensional structure motion estimation multiple video sequences problem dense motion scene flow multiple video sequences techniques spatio-temporal derivatives input scene flow derivatives regularization flow scene flow vector problem optical flow spatio-temporal derivatives order scene problem flow tikhonov smoothness brightness constancy local spatio-temporal methods shape scene flow estimated proceedings ninth ieee international conference computer vision 2003 ieee approach brightness constancy high computational enforce smoothness recovered shape techniques previous optical flow smoothness constraints optical flow methods recovered scene method scene flow estimation previous optical flow ambiguous spatio-temporal image vector field intensity changes robust illumination changes statistical similarity multi-resolution case framework case statistical criteria defines statistical criteria stereovision method scene flow variance local similarity integration local cross correlation mutual variational stereovision statistical measures approach surface backprojected images matching statistical measures defined surface minimization energy functional gradient descent model scene function defined bounded domain smooth dense note image camera intensity point image camera function gradient gradient image camera note pair surface image surface note point statistical intensity similarity criteria consider similarity criteria image cross correlation affine mutual measures intensity images criteria global computed local computed cope joint probability computed global criteria global joint probability density estimated window method gaussian window images variance defined bounded domain joint probability density function variational stereo problem minimization functional measures statistical dissimilarity backprojected defines images constraints note backprojecting method intensities surfaces correlation techniques approximation local similarity measures smooth gaussian windows backprojected volume windows input classical regularization term local criteria proceedings ninth ieee international conference computer vision 2003 ieee function smoothness assumptions classical tikhonov regularization order depth equation solve gradient descent guess solve evolution problem function global classical equation function equation gradient dissimilarity term pattern gradient global criteria convolution intensity equation gradient integration domain similarity criteria surface criteria assumption approach case depth well-posedness minimization assumptions bounded equation global classical consider tikhonov image enforce consider high convolution depth gaussian variance gaussian multi-resolution energy functional gradient descent local initial guess order local multi-resolution flow equation set volume images initial guess resolution plane suitable depth convolution global joint probability backprojected images local criteria gradient experiments statistical criteria time resolution figure stereo pair experiments reconstructed surfaces statistical proceedings ninth ieee international conference computer vision 2003 ieee figure method camera spectral sensitivity intensities initial stereo pair local cross correlation global cross correlation local cross correlation local cross correlation global mutual global mutual local mutual figure stereo pair reconstructed surface statistical figure stereo pair reconstructed surface local cross correlation global mutual joint probability backprojected images initial surface global mutual global mutual good figure evolution joint probability backprojected images mutual joint probability recovered global cross correlation global affine stereo local cross correlation global mutual good local mutual matching images local mutual criteria figure high resolution local cross correlation views backprojected variational scene flow estimation statistical measures vector field defined estimation surface backprojected image time instant backprojected image surface time instant enforce estimated scene flow figure views reconstructed proceedings ninth ieee international conference computer vision 2003 ieee previous optical flow ambiguous spatio-temporal image vector field evolution minimization energy functional gradient descent experiments figure input stereo sequence computed scene flow head estimated surface time instant model function scene flow function dense surface time order surface scene flow estimation robust surface estimation note image camera time instant note point vector variational consider minimization functional measures global local statistical dissimilarity backprojected images time instant surface images time instant defines constraints smoothness assumptions consider tikhonov smoothness term global criteria gradient figure input sequence views estimated scene flow recovered flow order computed scene sequence initial surface scene note figure defined images equation gradient local criteria figure assumptions well-posedness minimization variational framework depth recovery scene flow estimation multiple video method proceedings ninth ieee international conference computer vision 2003 ieee backprojecting input images suitable surfaces statistical similarity criteria camera spectral sensitivity illumination framework surfaces defined set shape motion order stereo scene flow estimation computational estimation dense optical flow international journal computer optical flow international journal computer scene video shape proceedings international conference computer ieee computer ieee computer stereo motion proceedings international conference computer vision pattern volume june ieee computer variational surface set methods stereo ieee transactions image journal variational methods multimodal image dense image matching global local statistical variational proceedings stereo matching ieee transactions pattern image mutual ieee transactions model joint motion structure estimation stereo computer vision image spatio-temporal stereo multi-resolution international journal computer estimation probability density variational stereovision motion estimation statistical dense depth minimization regularization approach proceedings conference computer correlation similarity multimodal image image computer stereo matching international journal computer june optical flow field approach motion sequence stereo pattern motion stereo integration depth proceedings conference computer three-dimensional scene proceedings international conference computer volume ieee computer ieee computer shape motion proceedings international conference computer vision pattern volume head june ieee computer mutual international journal computer three-dimensional motion structure multiple image computer vision image scene flow structure recovery image proceedings international conference computer vision pattern head june ieee computer scene flow structure proceedings proceedings ninth ieee international conference computer vision 2003 ieee
international journal computer vision multi-view stereo reconstruction scene flow estimation global image-based matching score keriven faugeras april april april variational method multi-view stereovision non-rigid three-dimensional motion estimation multiple video method prediction error shape motion problems generic image registration global measure image imaging conditions scene integrating matching measure computed independently surface approach computes global image-based matching score input images predicted matching process fully handles projective distortion partial neighborhood well global intensity robustness appearance changes non-lambertian approximation motion approach implementation existing computation time datasets standard method hardware implementation graphics processor stereovision algorithm good datasets including motion estimation algorithm challenging multi-view video sequence non-rigid non-rigid scene prediction reprojection variational global image-based matching cross mutual non-lambertian level problem problems computer authors dense non-rigid three-dimensional motion field called scene multiple video input data two-dimensional scene images keriven faugeras multi-view stereovision dataset time video sequence captured stereovision scene flow allows spatio-temporal model model novel views scene space time stereovision scene flow estimation match images points cameras corresponding correspondence problem shape three-dimensional motion scene correspondence problem difficult computer vision scene points view existing stereovision scene flow algorithms assumptions common photometric geometric assumptions shape motion estimation assumption photometric scene brightness corresponding pixels photometric calibration assumption popular stereovision photo-consistency measure voxel space carving deformable surfaces methods lhuillier variational formulation relies intensity authors model intensity brightness constancy assumption methods scene flow derivatives input images carceroni neumann brightness constancy assumption local spatio-temporal differential methods scenes constant robustness noise imaging matching measures embedded stereovision scene flow algorithms neighborhood intensity geometric distortion views time stereovision methods fixed matching assumption called parallel cameras scene parallel assumption work andez esteban projective authors compute stereo scene cameras approximation camera methods account tangent plane object jin duan ucke adaptive matching windows scharstein robustness photometric conditions approximation shape motion computation matching robustness matching process scene fixed matching windows stereo correspondence depth tangent plane approximation compute matching measure faugeras keriven jin duan ucke magnor tangent plane points regions curvature previous work multi-view complete stereovision complete stereovision methods complete reconstruction scene number input methods depth multi-view stereo reconstruction scene flow estimation partial methods handle visibility graph cuts method method scharstein good dense stereo correspondence focus multi-view complete stereovision methods space carving framework deformable surfaces space space carving framework scene three-dimensional voxel algorithm voxels volume voxel input order visibility voxels account method called voxel constraint algorithm space carving handles arbitrary camera space carving framework hard voxel voxel voxels probabilistic space carving method space carving brightness constancy choice global color photometric robustness calibration kutulakos representation hard space carving noise typically deformable methods active contour method space carving formulation geometric scene two-dimensional scene reconstruction energy initial driven partial differential minimizing energy work level set stereovision method faugeras keriven stereovision problem minimal surface active method energy integral surface data normalized cross correlation image surface evolution implemented level set framework representation numerical ability handle changes approach implementation points data lhuillier andez esteban lhuillier spatio-temporal scenes ucke method proposed jin cope non-lambertian method estimate shape non-lambertian geometric photometric model allows predict appearance novel surface driven minimization radiance deformable surfaces methods faugeras keriven duan lhuillier andez esteban ucke magnor jin matching measure computed independently surface matching measure point relies local approximation plane andez esteban tangent plane keriven faugeras curvature derivative matching measure respect tangent plane intrinsic computation term authors previous work scene flow estimation jin duan ucke visibility neighborhood faugeras keriven cross correlation matching windows computed account partial assumptions typically depth assumptions robustness matching process data methods minimal surface approach data regularization difficult good soatto authors numerical jin integrating matching measure images choice matching photo-consistency lhuillier normalized cross correlation ucke andez esteban radiance matching measures method cope imaging dependency matching measure surface normal complex handle matching windows tangent surface complex minimizing flow derivatives matching score ucke energy denotes surface matching gradient writes three-dimensional motion estimation multiple video sequences scenes problem computing dense non-rigid three-dimensional motion field multiple video sequences methods scene flow family methods carceroni neumann relies spatio-temporal derivatives input scene flow derivatives regularization associated normal flow scene flow vector parallel contour problem optical flow carceroni kutulakos neumann derivatives order scene problem normal flow constraint brightness constancy local spatio-temporal differential methods scenes constant second family methods optical flow computed independently scene approach corresponding optical noise optical scene flow multi-view stereo reconstruction scene flow estimation approach common variational framework multi-view complete stereovision scene flow estimation metric framework ability predict input views input view estimated shape proposed quality motion estimation stereo correspondence prediction error estimation popular duan lhuillier andez esteban ucke jin compute matching measure independently surface point image domain soatto approach computes global image-based matching score input images predicted matching process fully handles projective distortion partial approximate tangent plane neighborhood well global intensity robustness appearance approximation motion formulation image similarity measure quality normalized cross statistical measures correlation ratio mutual wells estimation camera non-lambertian constraint method computes global matching images projective distortion semi-occluded regions complex handle matching windows tangent pixels computation matching score minimizing flow faugeras keriven jin duan ucke magnor involves derivatives matching scene flow method existing object vector field input images captured variational formulation complete stereovision non-rigid motion similarity measures normalized cross correlation mutual implementation level graphics hardware minimizing prediction error method respect shape similarity input view predicted images coming input images compute predicted projective computational graphics hardware reprojection image camera surface cameras predict appearance shape estimate reprojected images corresponding input calibration appearance changes semi-occluded common stereovision shape motion shape motion estimation generic image registration stereovision optical multi-view stereo arbitrary camera novel registration keriven faugeras global measure image imaging conditions scene measure function images scalar images measure neighborhood well global intensity similarity measure regularization term energy regularization term problem shape motion focus matching term regularization minimization energy number graph cuts method energy minimization method allows global strong local method applied problems computer including stereovision image applied arbitrary energy function graph cuts energy gradient embedded multi-resolution probability local stereovision note generic measure similarity matching term sum input view reprojected images coming cameras compute similarity reprojection camera domain semi-occluded soatto minimal surface energy sum matching term computed images regularization approach matching process global soatto matching measure computed independently surface tangent plane image compute variation matching term respect vector figure camera variation visibility term surface term assumption stereovision surface model shape note image captured camera images color projection camera denoted method account visibility surface visible image reprojection camera surface denoted projection image camera surface writes figure camera multi-view stereo reconstruction scene flow estimation jin duan lhuillier gradient matching term image denotes surface reprojected image variation matching term involves derivative similarity measure respect second denoted images derivative reprojected well similarity measures change minimizing motion surface reprojected surface point gradient regions visible note term scalar regularization term typically associated minimizing flow curvature evolution surface driven denotes curvature scene flow vector camera surface normal integral image integral surface change depth camera model shape scene iit image captured camera time vector field motion scene matching term sum cameras images time corresponding images warped time scene iit iit gradient writes keriven faugeras regularization term typically energy flow corresponding minimizing flow intrinsic based intrinsic called evolution scene flow driven cross correlation time denotes similarity measures similarity measures normalized cross correlation mutual wells cross correlation local dependency mutual cope statistical measures family statistical proposed multimodal image scalar images measures vector images note shape domain input image associated prediction semi-occluded regions minimizing derivative similarity measure respect second denoted derivative images note function image variation cross correlation popular stereovision matching methods fixed correlation choice window size difficult match depth projective distortion authors problem adaptive windows scharstein match size matching window shape neighborhood assumption dependency scenes reduce size correlation estimation noise hard gaussian formulation problem implemented neighborhood gaussian kernel standard deviation local cross correlation images function shape constant numerical constant parzen gaussian kernel estimate local probability images local cross correlation similarity measure corresponding multi-view stereo reconstruction scene flow estimation minimizing flow method includes derivative similarity measure respect second writes minimizing change iterations reduce computational mutual function implementation mutual based probability estimated parzen window method gaussian kernel standard deviation note measure mutual implemented method level set framework numerical ability handle changes method surface implementation predicted images computed graphics visibility surface points cameras depth compute reprojection image camera surface projective texture implementation computation similarity involves graphics processor based implementation framework regularization similarity measure embedded method size correlation window cross standard deviation parzen kernel mutual matching window standard deviation pixels cross parzen kernel cross correlation mutual stereovision derivative respect second image writes faugeras table stereovision datasets datasets color images images kutulakos keriven faugeras table stereovision datasets image size 600 measure level set size 1283 1283 1283 1283 time cross correlation mutual well complex challenging scene includes strong camera pairs cameras scene visible pairs number camera pairs table number iterations 600 computation time visible driven curvature 600 shape minimal curvature initial surface approximate initial shape multi-resolution level set size datasets input ground truth views estimated views texture coming note representation texture method image-based shape figure stereo multi-view stereo reconstruction scene flow estimation figure images dataset figure images dataset keriven faugeras figure images ground truth fully number images visible strong intensity multi-resolution evolution surface table method non-lambertian stereovision method jin method jin method error error figure images ground truth table non-lambertian stereovision method jin shape error measure ratio volume estimated shape shape volume dataset method multi-view stereo reconstruction scene flow estimation figure multi-resolution shape evolution computation time stereovision scene flow scene flow algorithm challenging multi-view video sequence non-rigid sequence datasets neumann sequence captured cameras cameras focus sequences method handle applied stereovision scene flow shape compute motion scene flow good estimate initial stereovision algorithm iterations compute backward motion time cross correlation level set size 1283 number figure input sequence estimation shape forward motion corresponding figure images sequence dataset keriven faugeras figure time-interpolated sequence time-interpolated sequences images time previous shape texture warped forward shape texture warped backward forward backward images forward backward sequence note video images including algorithm motion motion non-rigid scene time-interpolated work includes hardware implementation stereovision method graphics processor reduce computation shape motion order kutulakos neumann note work novel method multi-view stereovision scene flow estimation prediction error global image-based matching input views images image similarity neighborhood global intensity approximation motion visibility matching implemented stereovision method level set framework complex non-lambertian scene flow optical two-dimensional motion field points optical flow projection scene flow image plane optical flow international journal computer computing minimal surfaces graph international conference computer probabilistic framework space international conference computer multi-view stereo reconstruction scene flow estimation multi-view scene video non-rigid shape international journal computer active international journal computer shape reconstruction data deformable european conference computer multimodal image registration journal applied variational surface level set methods stereo ieee transactions image evolution international conference computer vision pattern minimal computer european conference computer variational methods multimodal image international journal computer andez stereo object computer vision image multi-view stereo reconstruction dense shape complex international journal computer stereo matching algorithm adaptive ieee transactions pattern analysis machine active contour international journal computer scene reconstruction graph european conference computer energy graph ieee transactions pattern analysis machine approximate european conference computer shape space international journal computer surface reconstruction integrating data multiple international conference computer spatio-temporal stereo multi-resolution international journal computer algorithms based journal computational estimation probability function correlation ratio similarity metric multimodal image image computing stereo matching international journal computer dense stereo correspondence international journal computer van texture computer scene reconstruction voxel international journal computer shape radiance multi-view international conference computer geometric formulation gradient variational problems international conference space methods computer van stereo multiple probabilistic international conference computer vision pattern van dense matching multiple international conference computer prediction error quality metric motion international conference computer stereo european conference computer spatio-temporal view three-dimensional scene ieee transactions pattern analysis machine wells mutual international journal computer regions space carving novel photo-consistency international conference computer scene flow international conference computer vision pattern
multi-frame scene-flow estimation multi-frame scene-flow estimation patch model smooth motion prior computer paper problem estimating dense motion scene frames set motion estimation techniques estimating motion single prior model scene estimating motion general scene difficult complex surfaces scene prior scene experimental proposed method estimates dense scene-flow multiple accuracy proposed method demonstrated estimated motion ground estimating motion scene computer vision estimating motion estimating motion general scene estimation demonstrated motions single frame multiple carried frame approach estimating scene-flow general scene difficult scene surfaces complex prior tracking model constrain general smooth approach optical-flow techniques smooth motion scene-flow estimated motions regularized images translations regularized surface motion estimates accuracy multi-frame scene-flow estimation figure fitted patches frame patches tracked accurately positions frame visual hull frame representing representing paper approach estimating motion model surface patch model figure planar patches fitted surfaces frame local reconstruction tracked frames smooth motion prior paper scene-flow translations estimated point scene motion neighbouring surfaces points accurately predicted estimates translation measurement covariance surface motion regularization carried surface patches motion estimates constrain neighbouring patches proposed method estimate scene-flow multiple demonstrated multi-camera sequences ground truth paper work proposed experimental multi-camera work estimate motion scene motion estimates cameras optical techniques scene flow estimates motion regularization camera approach motion regularization multi-frame scene-flow estimation methods estimate motion scene points scene approach estimating motion energy meshes sequence reconstruction algorithm estimate complex motion set surface single frame estimate motion set patches model motion prior multi-camera performance capture methods sequence consistent mesh frame constrain mesh carried feature-based methods multi-view frame final methods surface points meshes frames starck field order estimate correspondences frames captured set features mesh main problem approach scene surfaces surface features estimating correspondences difficult work surface patch-based approach scene-flow estimation main difference smooth motion motion estimates motions estimated regularized problem optical-flow estimation patches fitted proposed algorithm dense motion feature-based stereo algorithm frame smooth tracking planar patches planar patch motion model order non-rigid motion surfaces set planar planar patch three patch three patch patches fitted scene surfaces local surface reconstruction method patch time patch texture model function patch image camera projected patch function point patch surface image camera projected image camera patch problem estimate motion patch time time patch time motion patch multi-frame scene-flow estimation order patch rotation motion energy model tracking problem patch motions local motion energy term smoothness edata esmooth smoothness estimating motion patch single patch texture consistent set input images function motion patch texture input images projected edata set cameras set smoothness energy term difference neighbouring example approach work smoothness energy term esmooth flow flow smoothness term patch tracking problem esmooth energy order function equation order projected image patch motion predicted set motion function edata energy edata esmooth multi-frame scene-flow estimation time difference neighbouring motion image neighbouring set iteration neighbouring image iteration model equation patch tracked equation covariances measurement neighbouring estimate measurement covariance motion covariance patch iteration estimating patch patch covariances distance patch j2n j2n estimated motion patch motion patch function covariance multi-frame scene-flow estimation distance patches estimation local rotation patch rotation predicted translation noise distance covariance measurement noise measurement noise image example patches figure estimated measurement covariances three translation three patches three patches texture patches regularization advantage approach patches texture constrain motion patches texture figure representing set example patches covariances estimated equation motion field estimation tracked patches final method mesh provide motion multi-frame scene-flow estimation field estimates motion neighbouring motion point j2n j2n predicted motion point motion patch covariance motion patch function equation frame frame frame frame frame frame frame frame patch frames figure input images frames sequence patches fitted frame tracked patches frames tracked patch frames average motion estimation multi-frame scene-flow estimation experimental evaluation proposed method demonstrated sequences performance capture sequence frames captured frames sequence captured frames sequences scene sequence surfaces texture sequence captured cameras motion cameras surface point scene-flow visual hull provide cameras frame cameras tracking patches figures views sequence camera sequence surfaces texture figure patches fitted scene surfaces frame figures tracked patches frames figure patch frames tracked motion smooth motion surface accurately tracked advantage regularized texture model approach feature-based estimate correspondences figures views sequence frames figures patches fitted frame tracked frame figures visual hull mesh evaluation order accuracy input positions feature time ground truth tracks order ground truth tracks error views motion field ground truth points positions frame final figure average error estimated motion ground truth motion final average error frames error scene-flow proposed method estimate scene-flow multiple frames single frame proposed method motion prior multi-frame scene-flow estimation frame frame frame frame frame frame frame frame frame frame frame frame figure input images frames sequence patches fitted frame tracked patches frames visual hull mesh frame frames scene surfaces optical-flow problem proposed method demonstrated sequence scene-flow sequences proposed method consistent meshes techniques multi-view reconstruction frame proposed method reconstruction provide surface point tracks time motion feature-based techniques feature-based methods correspondences frames proposed approach correspondences multi-frame scene-flow estimation time advantage proposed method features constrain motion scene figure patch-based scene-flow estimation method proposed estimate motion proposed method patch-based translations estimated point local motion prior measurement measurement covariance neighbouring main multi-view stereo evaluation optical ieee international conference computer surface ieee transactions computer transactions multi-view scene capture non-rigid international computer performance capture multi-view international conference computer mesh tracking motion ieee international conference computer vision pattern non-rigid surface feature visual conference motion capture tracking surface ieee conference computer vision pattern multi-frame scene-flow estimation approach multi-view stereo ieee conference computer vision pattern optical visual hull image ieee transactions pattern machine motion capture computer vision image surfaces multi-view stereo reconstruction scene flow estimation international computer surface features surface machine vision starck multiple reconstruction ieee international conference computer starck surface ieee international conference computer scene ieee transactions pattern machine dense scene flow dense stereo conference computer surface feature mesh ieee computer vision pattern
computer vision image understanding 121 computer vision image understanding estimating scene flow interconnected patch surface model popham computer article presents novel method estimating dense three-dimensional motion scene multiple method interconnected patch model scene interconnected nature model incorporate prior knowledge neighbouring scene motions markov random whilst patch-based nature model efficient techniques estimating local motion aspect work method fact local surface texture motion estimated simple cost better method based optical flow robust cost functions energy minimisation 2014 article 2014 2014 motion tracking markov random stereo scene flow dense non-rigid three-dimensional motion field optical flow motions images estimating scene flow scene surfaces texture move problem prior knowledge form solution scene flow problem similarities optical flow difference multiple cameras required order estimate scene flow single camera provide sufficient current approaches estimating scene flow methods local prior introduced global prior algorithms surface patches surfels local prior method estimating optical flow techniques global prior image regularisation similarities optical flow method article presents method local global constraints estimating scene achieved paper interconnected surface patch local prior provided surface whilst interconnected nature model global interconnected patch model markov random field techniques patch motions local motion observation aspect presented method combination local global energy minimisation single local motion estimates patch provided markov random field inferences belief optical local approaches robust whilst global approaches dense motion field combination local global approaches optical flow estimation proposed approach estimating scene flow method presented extension method scene flow combination local global estimate scene flow multiple frames surfaces order method estimating scene flow confidence patch introduced patches constrain neighbouring patches confidence measure covariance matrix belief propagation messages 2014 popham computer vision image understanding 121 ellipsoids representing uncertainty position set example patches katy covariances estimated note patches texture tend large ellipsoids patches texture tend smaller simple gaussian implementation obtaining confidence optical flow well feature tracking confidence measure performance computer vision image selected scene flow finding confidence measure arbitrary patch scene set sparse patches selected feature proposed technique image gradients surface patch respect rigid-body existing methods motion estimation hessian matrix image gradients respect rigid-body covariance matrix ellipsoids patch earlier work conference article framework presented scene flow method method based state-of-the-art optical flow article existing approaches scene flow local global accurately estimate scene experimental proposed motion tracking multiple scene flow scene flow estimates feature detection feature detector work image regularization approaches paper scene vedula estimate motion point surface model combining optical flow approach state-of-the-art optical flow techniques algorithm three knowledge motion smoothing object robust cost functions uniform regularisation image result regularisation fact optical consistent methods ensure optical flow estimates consistent estimating optical flow left image image motion point change image change scene flow estimated energy model data term smoothness data term three brightness consistency left brightness consistency brightness consistency left images smoothness term differences neighbouring image note robust cost functions required data smoothness terms smoothing object level motion smoothing applied approaches energy image-based variational approach discrete markov random field surface regularization approaches model scene order constrain motion parameters estimated described motion approach required expect neighbouring points scene move respect applied table neighbouring image expected move respect regularisation local surface scene expected move piecewise based methods estimate scene motion surface piecewise model mesh model framework major difference methods piecewise knowledge mesh model knowledge piecewise surface approaches regularise motion surface models tend regularise solution three data popham computer vision image understanding 121 set surface surfels form piecewise description surfel fitted motion single frame motion components estimated three three three surfel surface normal change patch set planar patches scene surfaces tracking individual patches estimate motion clustered assuming clustered components local rigid-body motion model scene finding motion model scene motions required rigid-body rigid-body model required estimate scene clustered patch components rigid components gaussian model time pons estimate scene flow variational approach surface model position frame achieved image-based matching methods estimate motion surface pons approach directly estimating dense scene flow interesting surface model matching score estimating dense motion order regularise operator scene flow single frame motion discrete laplacian operator motion smoothness algorithms mesh major question predicted mesh positions furukawa ponce constraints estimate motions node mesh aguiar presented set methods estimate motions directly image optical flow features stereo constraints motion constraints position mesh forces mesh previous frame current stereo constraints mesh positions lead accurate motion estimation represent true stereo reason methods meshes accurate motion constraints positions frame estimated work furukawa ponce vogel basha algorithm furukawa ponce estimates scene motion step local rigid motions mesh estimated technique second global mesh deformation motions calculations sequence meshes sequences 100 global smoothness prior introduced individual motions problem scene major difference work method motion estimates patch mrf variational linear laplacian rigid local mesh smoothness clustered components deformation model scene motion estimation regularization global smoothness scene motion field global surface smoothness colour features consistency image motion field surface based texture score table scene flow algorithms image based texture score mesh mesh mesh patches pons depth-map depth-map depth-map vedula points surfels basha point vogel article interconnected patches aguiar furukawa ponce aguiar form model optical flow estimates mesh deformation popham computer vision image understanding 121 patch surface texture neighbouring patch result motions estimated scene vogel rigid motion model similarities proposed earlier conference paper major difference work incorporate motion estimation confidence surface patch highly patches neighbouring patches texture estimating work similarities basha flow method step neighbouring surface algorithm estimate motion patch time number probabilistic model energy minimisation estimate motion directly model surface patch planar patches fitted scene surfaces frame multi-view stereo technique tracked subsequent scene flow problem estimating local motion planar patch described three parameters patch three parameters patch represent description patch time problem patches texture patches visible single markov random field smooth motion prior assuming uniform uncertainty measurement covariance surface motion estimate regularisation probabilistic surface patches accurate motion estimates texture constrain neighbouring patches energy minimisation techniques single belief propagation inferences mrf iterations observed motions belief propagation messages discrete distributions neighbouring number parameters node uncertainty patch described covariance gaussian distributions messages surface reconstruction stereo computer vision algorithms gaussian belief propagation efficient mrf inferences algorithm applied measurement covariance matrix algorithm major local motion observation patch node belief propagation graph graph nodes representing patches representing expected motion smoothness neighbouring patches motion observations set patch parameters probability probability observations patch motions motion estimate node motion node prior expressing smooth motion neighbouring nodes aspect method motion estimation patch novel technique applied covariance matrix motion estimate patch expressing state patch terms position orientation graph representing observations representing popham computer vision image understanding 121 model observation uncertainty patch gaussian covariance matrix local patch probability observation patch motion exp motion node neighbouring node distance small propagation algorithm incorporate prior motion algorithm motion estimates patch patch texture function patch image camera projected patch function point patch surface image camera state projected image camera patch motion patch state patch time initial best estimate patch position time position previous frame subsequent better estimate distributions node current best estimate patch position time iteration iterations exp calculated distance nodes problem probability methods belief propagation order belief messages neighbouring nodes expressing current belief message node node k2n motion prior observation node calculated k2n problem finding iteration reason mrf initial motion estimate step neighbouring motion estimate observation node mrf model uncertainty node gaussian message node covariance exp lij covariance matrix sij calculated sij estimating motion patch single expect patch texture consistent input cost function motion differences patch texture input images projected k2n sij uncertainty node belief node requires sij uncertainty calculated messages neighbouring nodes messages form covariance matrix normal distributions covariance matrices lij calculated lij sij k2n set cameras patch set differences errors texture model images normal c2c p2w observed motion patch node obtaining estimate covariance matrix local patch motion estimation iteration previous described framework local motion estimates patch belief order cost function order projected image patch motion predicted set motion motion image patch texture changes patch motion templates patch image gradients respect original texture motion templates example note translation change motion smaller patch texture function c2c p2w popham computer vision image understanding 121 features selected patches feature points fitted scene experimental example patches katy estimated measurement covariances three translation three patches uncertainties ellipsoids arms patches form regularisation patches body texture constraints three patches ellipsoids texture components sufficient constrain surface provide position motion field estimation obtaining motion field arbitrary point scene neighbouring simple solution linear message node messages patch texture motion templates example patch katy top row motion templates row motion templates motion templates calculated image single camera k2n respect experimental implementation algorithm motion vedula ensure cameras surface motion patch visible cameras frame case visible cameras tracking visible neighbouring estimates estimate observed noise visible scene patches sequence visibility calculations required set cameras patches move respect patch parameters previous frame initial result tracking solution patches lead motion simple model motion estimate previous frame initial current patch texture model energy minimisation technique requires texture image previous frame texture texture model c2c p2w time c2c p2w position estimated motion patch c2c p2w c2c p2w c2c p2w estimation measurement uncertainty patch question measurement noise expect patch image gradients respect measurement small noise true image estimating covariance hessian image feature question estimated covariance matrices represent uncertainty estimated covariances performance computer vision c2c popham computer vision image understanding 121 input images frames sequence input images frames texture sufficient iteration texture models patches iteration estimates better neighbouring patches selected number iterations estimated requires iterations better estimates patch neighbouring motions neighbouring small magnitude motion field frame katy colour magnitude scene flow point model red surface green motions motions left arm body left interpretation references colour figure reader referred web version evaluation proposed method demonstrated sequences performance capture sequence frames long captured frames second sequence frames long captured frames second example images sequences sequence presents scene flow sequence surfaces texture strong changes sequence captured cameras motions sequence patch tracked motion smooth note large regions arms katy motion surface accurately tracked scene flow demonstrated highly technique global accurate motion estimation frames interesting note feature matching approaches tend features textureless evaluation sparse ground truth order performance proposed small number ground truth tracks tracks feature set tracks view order ground truth achieved simple error tracks level error result camera ground truth points finding strong features markers selected provide motion field large number markers markers katy sequence markers skirt motion field ground truth points initial positions frame predicted positions popham computer vision image understanding 121 surface motion estimation katy skirt colour temporal aspect red representing earlier green representing sequence note motion smooth textureless arms katy interpretation references colour figure reader referred web version subsequent error frame average distance markers ground truth table parameters optical ground truth position position estimated scene flow ground truth human operator accurately features dense ground textureless texture estimating ground truth motion errors arms katy sequence ground truth error average error estimated motion ground truth motion katy average error frames error error directly data prior terms model errors data term result patch model surface fitted changes visibility model cameras view patch surface motion estimation comparison scene flow algorithm vedula method presented article original scene flow algorithm vedula optical order estimate three-dimensional order provide optical flow estimates scene flow state-of-the-art implementation robust cost functions motion object ensure earlier patch surface model visibility calculations table parameters optical flow error number iterations number level tracked surface points scene flow algorithm vedula frames estimated scene flow accurate sequence estimates arms surface points left reason optical flow estimates motion estimates surface second reason scene flow method robust surface point motion scene flow method presented article smooth patches surface fitted patches patches tend strong patches tend forces fitted patch forces number surface points problem patch-based scene flow fact cameras required estimate motion surface patch-based surface motion regularisation camera required point motion estimation error note ground truth distance figure assuming skirt performance algorithms frame frames point average popham computer vision image understanding 121 vedula scene flow algorithm skirt sequence frame colour temporal aspect red representing earlier green representing note number tracked surface points smaller method presented article cameras required estimate motion surface point vedula note points left arm skirt motion smooth tracks katy interpretation references colour figure reader referred web version point motion estimation errors red lines errors method lines errors scene flow algorithm vedula interpretation references colour figure reader referred web version performance scene flow algorithm estimates average motion estimation error patch-based scene flow katy ground truth errors performance method proposed sparse ground truth human operator estimate motions textureless regions proposed method regions ground truth comparison scene flow algorithms proposed method estimate scene flow multiple frames single frame method motion prior scene surfaces optical flow problem smoothing object method demonstrated sequence regions whilst scene flow sequences tend highly method consistent meshes long techniques multi-view reconstruction frame proposed method reconstruction provide surface point tracks time motion comparison techniques methods 100 frames proposed approach time method surface popham computer vision image understanding 121 evaluation tracking tracked surface model points projected camera left scene flow algorithm vedula surface points based method drift top surface points tracked method original interpretation references colour figure reader referred web version translation uncertainty single features constrain motion scene number cameras confidence measurement aspect algorithm measurement uncertainties patch estimating measurement uncertainties patch provided measurement covariance patch image example measurement uncertainties ellipsoids translation motion components cameras motions estimated single camera algorithms smaller patches multiple cameras translation uncertainties set individual cameras case optical ellipsoids motion ellipsoids point camera expect cameras constrain patch patches cameras arms translation uncertainty popham computer vision image understanding 121 tracking textureless katy frames top row smooth motion prior uniform measurement noise row covariances patch lines magnitude estimated motion note uniform measurement noise patch covariances patches drift original interpretation references colour figure reader referred web version expected motions arm estimating motions cameras reason cameras error patch cameras texture cameras estimating scene confidence measure simple estimated measurement covariances example patches tracked uniform measurement noise figure patches tracked measurement covariances estimated version uniform measurement noise patches left drift true estimated patches work article presented method estimating scene combining local global approaches achieved set interconnected patch provided local regularisation solution interconnected nature model provided global performance proposed method better state-of-the-art optical flow algorithm camera point optical flow techniques proposed method well well second article method estimating scene flow confidence hessian matrix patch image gradients respect method extension existing methods estimating confidence feature article patches probabilistic global motion interesting case knowledge motion estimation feature detector best features tracking feature detector best features patch finding texture features surface proposed mesh matching surface features tracking novel extension work proposed interesting work technique mesh current surface description knowledge distance patch mesh model ensure true neighbouring surface points smooth change scene method patch motion estimation mesh propagation gaussian method described existing mesh based lead probabilistic laplacian mesh deformation measurement noise point surface ellipsoids references three-dimensional scene ieee pattern multi-view scene capture surfel video non-rigid vision popham computer vision image understanding 121 image technique stereo international conference optical feature tracking human motion capture dense motion capture video ieee conference computer vision pattern mesh tracking human motion ieee international conference computer vision pattern recognition video based surfaces multi-view stereo reconstruction scene flow estimation global image-based matching vision efficient dense scene flow sparse dense stereo european conference computer variational method scene flow estimation stereo international conference computer dense motion estimation belief vision multi-view scene flow view variational ieee conference computer vision pattern recognition scene flow estimation rigid motion ieee international conference computer vision combining local global flow vision detection tracking point computer features ieee conference computer vision pattern covariance matrices image international conference computer smooth motion prior efficient surface true video estimation patch model smooth motion british machine vision conference body motion capture vision scene flow motion vision dense motion capture human ieee conference computer vision pattern performance capture sparse multi-view international conference computer graphics surface estimation tracking methods video based ieee international conference image linear variational surface deformation ieee graphics temporal surface tracking mesh vision mesh multi-view image features vision surface features accurate surface british machine vision approach multi-view stereo ieee conference computer vision pattern dense stereo belief ieee computer conference computer vision pattern stereo orientation surface reconstruction algorithms belief propagation graphical ieee computer conference computer vision pattern energy methods markov random ieee pattern belief propagation gaussian graphical belief propagation gaussian graphical models arbitrary vision framework level feature european conference computer estimation uncertainty feature british machine vision motion capture tracking surface ieee conference computer vision pattern surface feature detection description mesh ieee vision pattern non-rigid surface feature european conference based approach optical pattern recognition graphics tracking estimated image vision multiple 100 british machine vision local patch orientation sparse british machine vision patch efficient object ieee conference machine vision pattern surface ieee international conference computer
scene flow estimation julian eric james julian eric james scene flow ieee international conference image scene flow estimation julian eric james scene flow motion point scene time method estimate dense scene flow intensity depth well local methods robust global dense motion combine local global constraints solve scene flow variational adaptive regularization motion constrain motion set correspondences deal large approach previous scene flow intensity depth methods scene depth variational set motions structure scene flow estimation estimate dense scene flow intensity combine local global constraints solve scene flow variational locally constrain scene flow image local motion dense scene flow performing adaptive estimate dense scene flow preserving motion include set correspondences deal large formulation motion work motion scene interest computer scene flow defined motion field scene computation performed data provided color depth motion scene flow visual scene flow scenes estimated stereo camera depth compute scene flow depth intensity computing scene flow intensity scene flow depth motion computing scene flow performing surface computing motion intensity depth image motion field estimated optical flow variation based method time depth scene flow optical flow computation motion regularization estimation untextured regions motion discontinuities optical flow solved observed data motion depth data estimate computation performed intensity depth data set motion estimated surface point approach efficient motion field estimation scene flow approaches proposed solve solve structure motion data provided stereo camera compute scene flow optical scene flow observed optical introduce constraints stereo structure depth data structure estimation intensity depth motion solve optical range work depth data optical flow equation constrain observed depth approach color depth approaches motion field computed flow intensity depth images range flow motion work depth data model image motion constrain scene work locally rigid assumption compute local scene local approach applied untextured regions regions solve dense scene flow motion estimate scene flow moving particle method large set motion data efficient motion best optical flow previous methods computation time introduce auxiliary allows efficient solution alternating based model locally rigid motion point camera frame time time projection optical image projection image flow motion component image provided depth include depth constraint component motion pixel equation consistency motion depth sensor estimated order motion robust applied data term term term image flow pixel scene moving projection point motion locally rigid assumption pixels motion define warp function maps motion scene flow model intensity depth scene flow computation minimization problem intensity depth constraints regularization compute dense scene scene regularization performed component motion rigid regularization reliable local optical flow scenes interest well locally rigid scenes rigid assumption data term measure estimated local scene flow image consider pixels image rigid surface constraint small surface frames component data term motion field term data term estimated scene flow observed intensity deal large motion include sparse matching term flow motion set interest regularization term based adaptive component motion locally rigid motion preserving motion data term formulation scene flow best intensity depth assumption warp function maps pixel scene flow equation motions estimated computed compute measure consistency local scene flow intensity depth function allows compute local scene flow sparse matching term solution scene flow based data small order regularization deal large motions solution performed motion larger structures initial motion structures motion larger approach well global flow structure motion larger scene flow well larger structures estimation local include constraint consistency scene flow set sparse computed depth set component estimated scene matching intensity set correspondences frame point frame define matching function defined fixed solve minimizes point matching term defined interest point measure consistency scene flow set function robust applied deal regularization term regularization optical flow regularization motion field locally rigid motions motion provided reliable surface depth data discontinuities motion field observed depth image discontinuities define function initial estimate solve warp problem solved auxiliary flow initial estimate scene flow computed regularization motion field depth regularization term defined compute scene flow introduce auxiliary flow solve motion field minimizes proposed regularization estimate motion untextured order method middlebury stereo images fixed camera moving image dataset frame image ground truth optical flow frame approach scene flow stereo based approaches particle based method locally rigid approach variation lgsf include scene flow tv-l1 optical flow depth computed methods normalized optical flow small observed approaches auxiliary flow allows solved alternating fixed solve minimizes problem formulation image solution model lgsf tv-l1 lsf nrmsof aae nrmsof aae table optical flow errors middlebury frames lgsf tv-l1 lgsf tv-l1 lsf original nrmssf modified nrmssf motion field presented middlebury color table scene flow errors original modified nrmsof normalized range optical flow consider errors presented table lgsf best nrmsof aae stereo based lsf intensity lgsf machine time order scene flow errors compute normalized motion compute nrmssf pixels scene flow estimation ground truth ground truth scene flow constant stereo consider modified original dataset include frame ground truth scene flow constant point depth images table lsf best performed image motion field estimation images performing motion computation sensor motion estimation moving sensor motion estimation tv-l1 scene flow depth motion field lgsf allows reliable motion motion field tv-l1 preserving motion structure frames lgsf tv-l1 motion presented color range proposed approach compute dense scene flow intensity depth combine local global constraints solve motion field variational previous intensity work depth data model motion image constrain scene flow local method local motion dense solution alternating adaptive regularization component motion approach allows solve dense scene flow preserving motion approach previous scene flow methods optical flow estimation middlebury stereo proposed approach modified dataset method estimation motion component scene flow ground truth constant lgsf data estimated scene flow motions deal define scene preserving optical flow ieee pattern machine optical approaches visual motion particle based scene flow depth international conference computer flow intensity depth conference computer vision pattern scene international conference computer scene ieee pattern machine dense scene flow sparse dense stereo conference computer variational method scene flow estimation stereo international conference computer scene flow variational conference computer vision pattern scene flow estimation rigid motion international conference computer range flow depth intensity international conference pattern machine vision scene flow intensity visual image variation minimization minimization methods computer vision pattern based approach tv-l1 optical stereo depth maps conference computer vision pattern
real-time daimler daimler accurate motion field estimation stereo image sequences real-time daimler paper approach three motion field stereo image sequences approach dense variational optical flow kalman filtering temporal smoothness accurate three-dimensional motion field current scene computed parallel implementation yields directly driver assistance field paper proposed algorithm approaches respect computation time three-dimensional motion vector field stereo image sequences computational computer problem estimation motion object motion motion motion segmentation knowledge motion field driver assistance field accuracy real-time capability approaches literature sparse tracking methods robustness temporal integration dense computation consecutive dense variational approach image domain kalman filters single pixel temporal smoothness dense three-dimensional motion algorithm motion field dense stereo optical flow real accuracy robustness respect approaches literature optical flow component eccv heidelberg accurate motion estimation real-time typical traffic motion estimated dense6d algorithm proposed color encodes velocity green observed dense6d variational scene flow term scene flow three-dimensional motion field optical flow disparity change optical flow vectors consecutive approaches image based motion field estimation proposed three model based approaches sparse feature tracking methods multiple image frames dense scene flow computation consecutive frames estimation motion vectors three scene stereo matching estimation point consecutive matching three mentioned model based object problem model model based approaches large feature tracking scene flow regularization problem regularization time domain tracking features smoothness motion field consecutive frames paper feature tracking dense scene flow rabe kalman filters image pixel dense robust three-dimensional motion field scene flow computation methods based optical flow algorithm flow field computed functional constant image flow improved flow large flow real-time optical flow methods joint motion disparity estimation scene flow computation introduced motion disparity estimation real-time capability application kalman filters real-time motion field estimation proposed 6d-vision dense stereo disparity field method yields sparse vision system robust accurate motion field dense variational optical scene flow computational large computation filtering modern processing real-time capability chapter approaches optical flow scene flow input filtering chapter kalman filters motion estimation dense filtered motion estimation approaches based optical flow scene chapter approaches robustness accuracy chapter future motion field estimation combination optical flow stereo estimation motion field stereo image image domain camera system images performs stereo determined optical flow depth time three-dimensional motion field accurate motion estimation real-time image corresponding single leads proposed global functional image data term regularization data method leads accurate yields real-time performance modern flow computation method proposed based leads improved method additional term iterative scheme data term solved directly smoothness term solved proposed dense method estimation disparity images algorithm parallel disparity computation real-time performance motion field estimation knowledge optical flow field consecutive frames depth images three-dimensional traffic scene stereo color encodes three-dimensional corresponding rabe motion field differential approach leads noisy depth variational scene flow noisy depth measurements noisy motion field estimation optical global variational approach disparity change optical flow depth measurement yields disparity field estimated left disparity change global scheme optical flow functional data term vector numerical data term additional term iterative scheme data term solved regularization optical flow solved mentioned disparity change method approach optical flow computational temporal integration motion field robustness accuracy estimated motion temporal integration kalman kalman filter model proposed dense6d variational6d accurate motion estimation real-time model stereo camera observed scene reconstructed stereo left image point projection point relation point pixel stereo camera matrix vector camera system position observed image point disparity time observed image point optical flow feature tracking motion directly reconstructed approach measurement robust method position motion point kalman filter kalman estimation improved state vector associated measurements disparity extended projection matrix dense optical flow error stereo error ground truth data synthetic sequences rabe kalman filters measurement motion field error distributions input data application kalman filter state vector kalman filter combination position velocity system model state vector previous time step current linear linear system components motion time measurement model kalman filter relation measurement vector state vector position components state vector directly relation projection reconstructed point measurement model measurement model extended kalman filters filtered 6d-vision features measurements kalman filters model mentioned current measurement vector determined xt-1 optical flow previous position xt-1 computed method corresponding disparity image position xt-1 image position previous projection filtered state image position features determined feature filtering velocity disparity filtering kalman filter accurate motion estimation real-time time filter multiple filters 6d-vision motion field estimation method robust real filtered dense optical flow dense6d 6d-vision approach stereo image feature measurement step dense optical flow algorithm modern parallel capability numerical computation filtering kalman filters single pixel input image sequence 640 480 real-time numerical implementation based proposed sparse measurement matrix computation step image pixel xt-1 discrete pixel associated kalman filter sub-pixel component determined dense optical flow field updated filters filtering updated kalman filter field sub-pixel accurate optical flow filter field discrete pixel sub-pixel components xt-1 time step sub-pixel component updated sub-pixel accurate optical discrete position pixel optical flow step pixel current image flow vector filter associated filter based pixel current image flow vectors filters corresponding pixel filters depth filter filter state performance implementation extended image rabe filtered variational scene flow dense optical flow method variational scene flow scheme proposed estimation disparity change additional measurement vector kalman filter matrix extended projection matrix kalman filter disparity measurements measurement evaluation motion field estimation differential motion field estimation optical flow stereo variational scene flow frames kalman filtered dense optical flow stereo introduced filtered variational scene flow approach introduced evaluation ground truth vision system synthetic stereo image sequence sequence image 640 480 evaluation dense optical flow dense scene flow computation 640 480 kalman filters dense6d algorithm variational6d camera artificial traffic scene turning motion vector accurate motion estimation real-time ground truth direct combination optical flow stereo scene flow dense6d variational6d estimated motion field color encodes green encodes encodes vectors position rabe direct scene flow dense6d direct scene flow dense6d variational6d frequency variational6d frequency endpoint error endpoint error position component direct scene flow dense6d velocity component direct scene flow dense6d frequency frequency variational6d variational6d endpoint error endpoint error velocity component velocity component error distributions position velocity components direct combination optical flow stereo scene flow dense6d method variational6d method error error position velocity components methods rms rms rms rms method direct scene flow dense6d variational6d left vehicle performs turning moving vehicle constant performs constant dense6d variational6d differential accurate motion estimation real-time three-dimensional ground truth position motion error distributions image sequence error distributions error error computed proposed dense6d algorithm scene flow computation method respect accuracy accuracy proposed computation scheme dense6d algorithm future motion field estimation approaches dense real-time methods literature typical traffic corresponding motion estimated dense6d algorithm proposed color encodes velocity green observed real proposed estimation methods directly robust object estimated motion vector field turning vehicle moving motion field multiple moving turning motion camera dense6d future driver assistance rabe typical traffic corresponding motion estimated dense6d algorithm proposed color encodes velocity green observed proposed approaches accurate motion field estimation dense variational optical scene flow estimation kalman linear motion evaluation error synthetic ground truth data approaches real-time literature real future implementation flow filtering robust approach estimation stereo heidelberg vehicle intelligent three-dimensional knowledge joint image segmentation international computer vision flow matrix image vision iterative image application stereo international joint conference artificial moving intelligent accurate motion estimation real-time three-dimensional scene international conference computer vision dense scene flow dense stereo eccv heidelberg optical artificial dense estimation segmentation optical flow robust image processing accuracy optical flow estimation based eccv heidelberg motion accuracy real-time international conference computer based approach optical heidelberg joint estimation method stereo images image conference approach linear filtering kalman filter based depth motion intelligent stereo motion robust heidelberg regularization accuracy optical international conference computer vision algorithm vision accurate stereo processing matching methods discrete image sequence
dense wide-baseline scene flow handheld video cameras valgaerts visual computing propose technique computing dense scene flow handheld videos wide camera baselines photometric sensors camera settings exposure white technique existing independently moving computes dense scene flow wide-baseline combining wide-baseline correspondence finding variational scene flow compute wide-baseline correspondences daisy descriptors matching cameras occluded pixels correspondence fields novel edge-preserving laplacian correspondence finally refine computed correspondence fields variational scene flow dense scene flow computed challenging datasets independently handheld cameras camera methods reconstruct geometry dynamic scenes multiple video depth cameras performance capture methods video-based methods combination correspondence finding camera views capture temporal correspondence finding temporal shape motion existing approaches limited controlled dense static camera setups controlled limited reconstruction dynamic capture scene dynamic scene reconstruction unconstrained scenes multiple methods reconstruct stereo cameras fixed baseline controlled including proposed mobile video mobile recorded video dynamic scene reconstruction videos recorded mobile cameras wide-baseline stereo correspondences robust independent camera motion wide geometric image characteristics sensors camera exposure white existing techniques assume static camera handle moving stereo fixed videos captured independently moving handheld cameras independent approaches reconstruct dense geometry space shape motion wide-baseline recorded changing dynamic scene reconstruction methods handle scenes step solution dense correspondence finding general dynamic scene based estimation scene motion scene term term motion compute dense scene flow general dynamic scenes handheld videos independently moving cameras wide baseline terms camera geometry sensor scene flow dense approaches narrow camera baselines wide-baseline approaches dense sparse scene previous techniques sensors handheld assume constant camera calibration propose technique combining wide-baseline correspondence finding variational scene flow computation approach estimates dense correspondence fields camera views camera views time sensor image characteristics novel correspondence finding technique daisy descriptors wide-baseline matching space time camera patchmatch belief propagation edge-preserving laplacian correspondence dense scene flow dense stereo computed challenging independently moving handheld video datasets wide camera work scene flow motion scene motion visible point time techniques proposed compute scene flow including dense camera setups correspondence scene flow computed non-rigid scene estimation scene flow approaches variational methods provide regularised techniques enforce motion affine rigid non-rigid scenes techniques rgb-d data depth sensors approaches limited depth unconstrained settings video compute scene flow time techniques enforce temporal consistency multiple time steps assume input achieved dense variational approaches handle moving cameras assume static camera narrow baseline wider camera baselines cameras moving methods wider baselines reconstruct sparse method dense scene flow stereo geometry wider baselines independently moving scene flow estimation non-rigid structure motion approaches strong scene motion work small video work stereo static camera setups controlled scenes scene flow including motion performance capture video case independent handheld video general wide-baseline matching finding corresponding points robust matching achieved techniques sparse correspondences image work variational formulation match propagation daisy descriptor designed dense wide-baseline matching daisy case dense matching camera views method approach computes dense reconstruction geometry scene flow dynamic scenes captured handheld video scene structure camera cameras sensor method appearance differences three flows stereo flow correspondence images cameras optical flow correspondence time scene flow motion correspondence occlusion scene flow input videos estimate bidirectional correspondences images novel technique stereo flow cameras optical flow camera stereo correspondences filling occlusions novel edge-preserving scheme based local linear finally refine computed correspondence fields variational scene flow daisy pmbp approach stereo flow epipolar term pmbp left image image point cloud stereo flow point cloud occlusion mask depth map figure daisy pmbp correspondences epipolar term pmbp geometry figure input frames handheld deer dataset exposure white captured angle degrees optical point cloud estimated stereo occlusion mask depth calibration assume input videos fixed camera video-based methods approaches wide-baseline stereo moving cameras input video frames estimating camera geometry structure-from-motion techniques fixed correspondence finding daisy pmbp correspondence finding based daisy descriptor designed sparse points daisy designed finding dense wide-baseline daisy descriptor local appearance image gradient gradient computed image points descriptor local matching spatial propose matching scheme geometric spatial epipolar energy term global matching scheme figure daisy key matching cost image weight descriptors epipolar epipolar geometry descriptor epipolar descriptor geometric pixels stereo flow colour consistency term image regions gradient example areas constant daisy descriptors colour term colours result compute colour term daisy descriptor difference colour consistency term epipolar term stereo matching pixels compute correspondence daisy term measures local image regions difference daisy descriptors computed images colour term matrix affine colour colours image image rgb colour differences exposure white images figure matrix estimate fit colours corresponding epipolar term measures image points epipolar geometry matrix stereo term correspondences epipolar reduces space hartley weight epipolar matching cost equation pixels smooth patchmatch initialisation good correspondences challenging smoothness correspondence patchmatch belief propagation technique pairwise term regularisation energy set pixel difference flows pairwise term enforce flow occlusions image diffusion filling laplacian filling terms figure laplacian occlusion filling images edges filled flow fields better simple diffusion data mpi-sintel image ground-truth flow image flow linear fit pairwise terms mee aae mee aae mee aae weight pmbp compute bidirectional correspondences consistency computation refine correspondences multiple estimate affine colour transform settings approach camera parameters constant stereo set colour weight estimating colour transform pairwise weight values enforce smoothness matching produces figure optical pmbp parameters optical flow computation daisy descriptors figure visualisation local rgb colours window corresponding flow values three constant colour flow constant flow image edges flow error measures endpoint error average angular error data mpi-sintel edges filled flow flow strong image edges filling result better computing occlusion mask bidirectional stereo flows pixels filled key flow corresponding points linear image colours small window rgb figure good fit local three example provide better fit linear window pixel laplacian occlusion filling pairwise correspondences areas point visible correspondences computed previous variational scene flow estimation invalidate fill occluded pixels stereo wide-baseline views large occlusion simple occlusion filling diffusion handle propose occlusion filling method based flow values colour small local linear occlusion filling figure window pixels regularisation term constant smoother solution daisy pmbp approach occlusions laplacian filling stereo flow point cloud figure flow invalidate occluded pixels fill novel laplacian filling technique initialisation input frames cam frame point cloud stereo flow stereo flow initialisation point cloud data stereo flow flow cam frame depth map depth map flow occlusions figure scene flow computation initialisation daisy pmbp juggler dataset wide camera juggler point stereo flow depth daisy pmbp flows occlusion mask juggler flows computed previous achieved optical flow combining robust dense matching variational combination scene flow wide-baseline videos large variational approach fails solution flows provide figure variational methods key methods pmbp strong regularisation smoother stereo flows computed methods pixel noise stereo optical flows initialisation variational scene flow computes smoother flows image method valgaerts estimates scene flow time steps energy constant provide solution cost cost matrix laplacian cost sparse linear energy terms finally scene flow difference corresponding points time example figure input video optical stereo visualisation scene flow reconstruction scene flow data epipolar smoothness propose dense scene flow technique independently moving cameras wide baselines geometric photometric figure scene flow handheld cameras deer boar jiang bear boy datasets frames colours exposure deer boar cameras capture dense stereo scene boy considerable camera changing colours optical flow figure moving stereo cameras moving matrix pixels matrix correspondences pixels compute equation figure example areas left filled flow scene flow computation compute scene flow time variational scene flow method valgaerts left view deer sequence independent handheld exposure view point cloud scene flow left view view point cloud scene flow stereo flow optical flow stereo flow optical flow depth map occlusion mask depth map occlusion mask left view boar sequence independent handheld exposure view point cloud scene flow left view view point cloud scene flow stereo flow optical flow stereo flow optical flow depth map occlusion mask depth map occlusion mask left view bear sequence independent handheld view point cloud scene flow left view view point cloud scene flow stereo flow optical flow stereo flow optical flow depth map occlusion mask depth map occlusion mask left view boy sequence independent handheld view point cloud scene flow left view view point cloud scene flow stereo flow optical flow stereo flow optical flow depth map occlusion mask depth map occlusion mask left view odzemok sequence frames static moving view point cloud scene flow left view view point cloud scene flow stereo flow optical flow stereo flow optical flow depth map occlusion mask depth map occlusion mask figure scene flow technique bear boy datasets odzemok images input frames stereo optical flows depth map occlusion mask left large images point cloud scene flow approach considerable differences camera sensor characteristics deer considerable camera motion changing colours optical flow left view view sequence handheld stereo point cloud scene flow left view view point cloud scene flow stereo flow optical flow stereo flow optical flow depth map occlusion mask depth map occlusion mask left view traffic sequence view point cloud scene flow moving stereo left view view point cloud scene flow stereo flow optical flow stereo flow optical flow depth map occlusion mask depth map occlusion mask figure scene flow technique moving stereo handheld synthetic traffic frame left view jiang depth map approach depth map stereo daisy pmbp rmse aae rmse aae laplacian filling variational refinement figure depth bear sequence jiang variational refinement produces smoother depth bear depth 100 150 200 250 100 150 200 250 frame number frame number fixed figure depth map computed jiang handheld stereo technique result smoother depth variational scene flow video video dense scene flow techniques narrow camera baseline wider example figure variational scene flow technique flow flow stereo optical flows computed flow refinement fails reconstruct flow initialisation input reconstruct occlusion filling evaluate laplacian occlusion filling technique mpi-sintel invalidate flow pixels occlusion fill technique ground-truth flow fields filling frames laplacian filling endpoint error pixels average angular error laplacian filling endpoint error figure error traffic dataset step reduces error estimated stereo flows error scene terms rmse 230 colour flow edges ground-truth existing datasets scene flow estimated handheld wide-baseline case approach evaluate scene flow synthetic dataset traffic ground-truth dataset stereo camera cameras fixed static synthetic dataset figure error evaluate stereo flow error image pixels ground-truth occluded figure steps reduces error estimated stereo flow scene flow approach assume stereo images stereo flow left view view optical flow depth map point cloud cam frame optical flow point cloud scene flow occlusion cam frame stereo flow figure approach robust photometric differences input input colours videos error average angular error rmse aae figure example motion frames dataset images optical flow estimation scene work step unconstrained dynamic scene reconstruction general handheld mobile cameras videos estimate calibration dynamic temporal cameras videos motion changing parameters correspondence method photometric video bear reconstruction input videos correspondence finding computing optical flow stereo flow image occlusion filling variational refinement set frames extrinsic estimating epipolar extrinsic daisy pmbp approach global extrinsic calibration input videos structure-from-motion produces cameras global scene flow proposed technique fails stereo optical flows example scene motion optical flow scene flow case camera views dataset figure optical stereo flows large areas constant regularised correspondences occlusion regions scene flow figure camera baselines wider depth dense scene flow technique general handheld captured wide camera camera sensor technique wider baselines previous dense scene flow techniques novel wide-baseline correspondence finding approach daisy descriptors colour consistency pmbp stereo optical flows computed scene flow estimation laplacian occlusion filling method image occluded finally refine flows variational scene flow smooth correspondences space combination techniques dense scene flow stereo geometry computation handheld challenging datasets variational scene flow computation method valgaerts estimates scene flow time steps energy energy data terms difference corresponding points figure data epipolar smoothness data epipolar terms matching cost terms variational formulation good term smoothness estimated flows spatial camera motion datasets optical flow view stereo flow time correspondence images match transform estimated equation appearance matching handle appearance gradient difference matching noise data terms pixels occluded occlusion flow epipolar smoothness regularised term energy correspondences epipolar variational formulation datasets captured independently handheld visible camera baselines cameras figure camera baselines 250 angle cameras degrees degrees odzemok dataset constant camera angle cameras traffic dataset figure fixed stereo calibration constant baseline cameras video reconstruction moving points monocular image ieee transactions pattern analysis machine video-based captured acm transactions patchmatch correspondence structure motion scene scene flow view variational international journal computer patchmatch belief propagation correspondence international journal computer optical flow scene flow estimation correspondence simple method non-rigid structure-from-motion international journal computer scene flow 3-d points optical flow optical flow stereo stereo figure scene flow bear bear boar boar boy boy deer 100 deer 100 100 150 200 100 150 200 odzemok odzemok 200 230 200 230 frame number frame number figure visualisation independent camera baseline cameras baseline angle cameras time angle pattern novel dense variational scene flow rgb-d dense variational reconstruction non-rigid monocular independently moving cameras scene scene flow ieee transactions pattern analysis machine hartley shape motion hartley multiple view geometry computer motion capture moving rgb-d dense 3-d motion estimation scene flow rgb-d variational method scene flow estimation stereo depth scene flow temporal international journal computer motion smooth rigid scene flow rgb-d reconstruction dynamic scenes multiple handheld wide baseline matching match scene flow depth solution image ieee transactions pattern analysis machine correspondence based approach ieee transactions video visual scene flow temporal wide baseline stereo descriptors based international journal computer general dynamic scene reconstruction multiple view reconstruction moving point stereo reconstruction scene flow estimation global matching international journal computer dense scene flow estimation edge-preserving correspondences optical stereo matching based noise video monocular reconstruction dynamic dense matching multiple wide-baseline scene flow reconstruction motion dense descriptor wide-baseline ieee transactions pattern analysis machine stereo image machine local computer estimation structure geometry stereo performance capture acm transactions scene ieee transactions pattern analysis machine scene flow estimation multiple scene flow estimation rigid scene international journal computer scene flow computation motion international journal computer diffusion image visual structure motion performance capture multiple stereo acm transactions non-rigid reconstruction rgb large scene flow occlusion shape dynamic 3-d scene flow structure image ieee transactions sparse dynamic reconstruction video view acm transactions visual dynamic ieee transactions pattern analysis machine
2009 2009 international machine machine vision vision image image conference conference scene flow multiple optical flows vision computer flow motion surface points optical flow image scene flow surface geometry objects scene objects methods calculating scene flow multiple optical flows multiple optical flows scene flow prior scene scene optical flow projection scene flow image optical flow three time methods optical flow optical flow scene flow knowledge scene flow number human motion capture determining structure scene multiple optical flows single scene flow error optical optical flow better video calculating scene flow pixels views allows calculated optical flows generate scene vedula three calculating scene flow scenario knowledge surface object vedula calculate scene flow single scenario knowledge corresponding image views determine scene flow scenario knowledge vedula large number scene flows 2009 ieee thresholded reveal moving objects method motion objects scene scene flow histogram calculate points image calculate scene flow points scene structure projection geometry optical flow scene flow surface point scene camera views calculate scene methods multiple optical flow generate scene flow method background subtraction determine surface points objects space projection matrix calculate scene flow surface point corresponding optical flow method background subtraction thresholding optical moving objects scene method thresholding scene flow areas scene flow areas method histogram space point scene flows histogram reveal point scene flows multiple knowledge camera camera projection matrix camera centre camera pixels image camera time points space time optical flow pixel time previous time current time scene flow figure camera pixel camera centre calculate ray points pixel points projected pixel ray equation vector number camera centre point figure determining point corresponding pixels cameras pixel image projection point calculate position points position intersection rays pixels described size pixels rays closest points rays closest points large size pixel pixels point error pixel determining scene flow corresponding optical flows pixels optical flows point scene flow calculate position point magnitude scene flow figure pixels rays closest points reveal position point optical flow calculate position time frame video optical flow frames determining pixel point point space projected pixel image projection matrix determining ray pixel single pixel calculate single point calculate ray points point ray projected pixel projection matrix pixel point ray scene ray camera centre point figure range scene flows optical flow point camera figure pixels calculate point intersection camera camera rays range previous points time frames prior knowledge maximum magnitude scene calculate closest point previous ray point previous points maximum magnitude scene calculate scene flow required knowledge corresponding general pixels methods surface points projected cameras corresponding pixels scene flows areas background subtraction background subtraction determine pixels belong objects pixels belong points point projected cameras equation pixels point belong object point point remaining points object cameras points better surface points remaining points points surface points points time point point object point allows cameras determining scene remaining points projected figure optical flows calculate scene flow point intersection camera camera pixels previous frame optical flow current pixels rays previous point calculated vector previous point current point scene flow determining range points scene flows single pixel single pixel optical flow calculate range points scene flows pixel optical optical flow calculate position pixel previous frame rays calculated current previous pixel described rays figure vector scene equation range points point cameras method described method described determine scene flow thresholding optical flow moving scene object background subtraction thresholding optical flow method described thresholded optical flow background subtraction scene flow optical flow camera estimation scene thresholding scene flow time scene flow calculated points magnitude scene flow histogram data histogram three position three dimensions scene flow pixel camera method described generate scene scene flow histogram histogram histogram maximum scene flow single point scene histogram thresholded points surface points object maximum methods human eva dataset black optical flow generate optical flow frames cameras figure result background subtraction thresholded optical methods figure background subtraction scene flow derived background subtraction figure frames method background human motion background subtraction background result good required prior knowledge background figure difference original optical flow projection scene flow camera difference large difference error scene flow original optical flow thresholding optical flow thresholding optical flow figure background subtraction good objects body scene flow derived method figure error scene flow good background subtraction general outline body motion head camera camera head thresholded optical flow head scene method prior scene better thresholding time better thresholding scene flow scene flow thresholding scene flow figure good scene flow derived background outline method previous method motion body time knowledge scene histogram data scene flow histogram scene flows figure good scene flow derived background outline method prior knowledge method large number scene flows good number dimensions scene flow size dimensions human eva dataset position cameras cameras surface point allows estimation scene cameras scene flow multiple better methods determining scene flow multiple optical background subtraction method general motion work required scene motion capture data human eva work required cameras surface point work required work image proceedings determining optical optical vedula scene ieee machine flow human motion capture scene flow structure image ieee motion multiple objects proceedings international conference computer ieee computer methods structure motion proceedings international vision multiple geometry computer black video motion capture dataset human black motion estimation computer vision proceedings ieee computer conference figure frames human eva dataset cameras result background subtraction thresholding optical figure scene flow background thresholding optical thresholding scene flow histogram time scenario seconds seconds seconds time calculate optical flow background figure scene flow result background subtraction projected camera magnitude difference original optical flow optical flow
ieee transactions pattern analysis machine september 2010 range flow varying algorithms comparisons tobias senior scharr estimation range flow handle brightness changes image data caused inhomogeneous standard range flow velocity fields range intensity image range flow estimation depth change model brightness constancy local brightness object surfaces relative camera light surfaces inhomogeneous investigate approaches handle brightness approach prefilter intensity data brightness changes homomorphic prefiltering reduce novel approach brightness constancy model gradient constancy combination gradient brightness constancy constraints earlier optical physics-based brightness change performance standard novel range flow estimation synthetic data ground additive gaussian noise shot noise compare range flow estimators real illumination brightness constancy homomorphic gradient structure motion brightness models velocity estimation range flow brightness modeling standard motion estimation provide equations brightness constraint equations parameter estimation scheme input work estimation plant growth plant botanical paper best estimation system method influence brightness changes motion botanical well image estimation motion fields range flow estimation data range schuchert institute system image systems machine vision institute computer rwth aachen scharr institute chemistry dynamics forschungszentrum received published digital object 2010 ieee spies range image reconstruction scene flow structure optical flow cameras proposed scene flow approaches optical approaches brightness constancy brightness changes sequence optical flow brightness constancy intensity gradient vector well physics-based brightness change models physics-based models presented extended moving surfaces inhomogeneous illumination suppressing brightness changes appropriate prefiltering intensity data approaches apply spatial high-pass filter global brightness homomorphic prefilters highly improve motion image sequences inhomogeneous approaches scene flow estimation gradient constancy constraint optical flow scene flow estimation robust brightness paper range flow presented spies extended inhomogeneous optical flow estimation range flow gradient constancy combining brightness gradient constancy physics-based brightness modeling performance standard novel range flow models published ieee computer schuchert range flow varying algorithms comparisons model robust variational estimators respect earlier work paper physics-based brightness prefilters models additive gaussian noise intensity influence neighborhood experiments synthetic data additional experiment real data paper range flow prefilters intensity constraints range flow presented parameter estimation experiments synthetic real data range flow range flow based motion range data intensity range constraint surface described depth function spatial coordinates denotes camera sensor coordinates optical assumed total derivative respect time yields range flow motion constraint equation plant images input strong brightness changes estimated motion vector fields range flow structure top standard range flow proposed taylor models tested synthetic data sets ground sequences illumination high-pass homomorphic motion estimation image sequence botanical experiment leaf growth experiments effects brightness models motion estimation designed experiments selected image aperture parameter estimation scheme optical flow estimators local squares local total squares variational approaches model robust error squares variational estimators additional aperture light model robust estimators handling variational estimators data apply local total squares estimation neighborhood partial derivatives range flow defined range structure motion algorithms range data data sets sensor coordinates time range flow constraint allows compute partial derivatives sensor range total derivatives coordinates respect calculated total derivatives respect time changes sensor optical yields ieee transactions pattern analysis machine september 2010 respect equation derivatives sensor calculated derivative sensor systems reduces prefiltering prefiltering technique illumination change image data illumination temporal spatial high-pass filtering brightness changes illumination changes spatial temporal high-pass filtering gpre intensity constraint range flow constraint range flow estimated three depth plant surfaces surfaces aperture range flow constraint proposed intensity data intensity point constant time brightness constancy constraint equation optical flow estimation constraint yields yields optical flow gpre denotes spatial gaussian filter standard deviation pre filter approach homomorphic filtering homomorphic suppressing illumination homomorphic filtering based modeling image intensity determined illumination camera surfaces reflectance surfaces note illumination object image sensor coordinates image intensity modeled structure scene reflectance motion point transform illumination reflectance additive estimated range flow range flow constraint intensity constraint intensity constraint reliable provide aperture range allows range constraint combination range intensity constraints estimation total squares described handling brightness changes range flow presented yields inhomogeneous illumination intensity constraint well range range range constraint range data estimated structure motion approach approaches handle illumination changes three constraints novel range flow assumed generated nonlinear point transform high-pass components high-pass filter suppressing brightness changes high-pass reflectance note nonlinear log-operation camera modeled motion parameter effects proposed prefilters signal temporal brightness high-pass homomorphic prefiltering signal noise-free data gaussian noise intensity-dependent noise intensity-dependent note cameras nonlinear image schuchert range flow varying algorithms comparisons increasing parameter homomorphic prefiltering pre noise additive gaussian noise shot noise homomorphic prefiltering brightness changes noise-free high-pass prefiltering signal nonlinear log-operation leads increased noise regions small intensity noise intensity-dependent shot homomorphic prefiltering reduce reduce signal shot noise data transform estimation optical constraint assume image intensity gradient constant motion gradient derivative high-pass constraint illumination leads gradient constancy constraints gradient constancy constraint intensity constraint prefiltering ieee transactions pattern analysis machine september 2010 change intensity values caused brightness changes temporal intensity changes caused illumination changes spatial neighborhood local coordinates brightness change model presented spatially-varying illumination changes caused inhomogeneous illumination changing surface spatially-varying intensity changes neighborhood local coordinates model illumination models presented assume intensity change pixels constant local brightness change function partial optical flow caused moving assumed changes time-dependent brightness changes order taylor yields intensity gradient constancy constraint gradient constancy constraint reduces structure images leads aperture intensity constraint gradient constraint reduces optical flow estimation leads three constraint range flow components physics-based brightness change model approach handling brightness changes model estimate optical flow brightness change parameters based fleet proposed optical flow estimation based models brightness caused time-dependent physical brightness changes temporal described function temporal derivative total optical flow leading estimation motion parameters brightness denotes image intensity time brightness change total derivative yields brightness reduces nonlinear changes intensity brightness physical model brightness optical parameter vector flow time-dependent brightness change models proposed changing surface motion physical models described modeling prefiltering motion input data selected model original parameters estimated motion parameters models physics-based brightness change model brightness change parameters taylor estimate parameters total range constraint yields equation schuchert range flow varying algorithms comparisons parameter vector equation three assume local neighborhood parameter vector equations error error yields jrc structure tensor jrc filter neighborhood described constraint components range range constraint intensity constraint gradient constancy constraints data selected constraints positions data vector positions parameters parameter vector physics-based brightness change model motion brightness change leading parameter vector scaled frame sinusoidal sequence illumination parameters experiments models compared range constraint intensity constancy constraint gradient constancy constraint intensity gradient constancy constraint intensity constraint modeling brightness changes taylor model int prefiltering original range flow spies investigate influence temporal temporal brightness changes noise-free input sequences well data additive gaussian noise intensity-dependent accuracy motion estimates cube illuminated light real synthetic ground range flow range data stereo described data allows range compute intensity filter sets described motion brightness change parameters determined model error described spies experiments structure tensors models prefilters model int earlier work structure tensors leading data range constraint brightness change constraint appropriate combining constraints yields structure tensor tensor jrc range constraint tensors intensity-dependent constraints tensor jrc models structure data scaled estimate parameter vector error defined range flow sinusoidal error analysis generated sinusoidal varying varying illumination generated three frames typical sequence parameters surfaces frame angular velocity surface camera synthetic sensor pixels size ieee transactions pattern analysis machine september 2010 error motion estimates versus increasing temporal brightness changes high-pass prefiltering prefiltering temporal brightness changes modeled parameters standard deviation pre models error motion estimates versus increasing spatially-varying temporal brightness changes high-pass prefiltering prefiltering spatially-varying temporal brightness changes modeled parameters standard deviation pre models synthetic camera relative error pixels pixels image structure tensor weighting matrix gaussian standard deviation order reduce errors sinusoidal prefilters designed described pre denotes standard deviation gpre compare estimation errors increasing illumination parameters brightness provide errors errors investigate influence gaussian noise shot noise intensity shot noise gaussian noise intensity-dependent standard deviation sinusoidal pattern intensity values range effects errors tested models prefiltering errors described models high-pass prefiltering parameter high-pass prefiltering reduces estimation errors models int models grad homomorphic prefiltering models int homomorphic prefiltering models int schuchert range flow varying algorithms comparisons data additive gaussian noise standard deviation error motion estimates versus increasing temporal brightness changes high-pass prefiltering prefiltering models input data gaussian noise shot noise error motion estimates versus increasing temporal brightness changes homomorphic prefiltering prefiltering models int prefilter yields high-pass standard deviation pre strong small pre degrades estimation model taylor yields best model grad performs error surfaces errors model int pre improved brightness changes highly models perform homomorphic errors models int taylor homomorphic prefiltering pre reduces brightness changes values prefiltering pre degrades small brightness model taylor performs better experiments brightness changes parameter brightness changes errors pre prefiltering standard deviation pre yields best models grad taylor perform well homomorphic prefiltering pre degrades signal models model model prefilter yields best effects proposed prefilters high-pass filtering noise homomorphic errors high-pass prefiltering gaussian noise standard deviation shot noise noise-free errors increased error surfaces model taylor yields best models homomorphic prefiltering estimation gaussian noise shot nonlinear log-operation noise homomorphic prefiltering intensity data brightness changes noise shot homomorphic prefiltering performs noise model taylor prefilter yields best reliable synthetic cube synthetic cube sequence allows models ground cube cube illuminated input data sequences frames synthetic cameras allows filter sets proposed compute range data stereo estimation algorithm presented frames cube frame high-pass prefiltering homomorphic regions left side well ground prefilters standard deviation pre weighting matrix ieee transactions pattern analysis machine september 2010 scaled motion estimates errors int grad high-pass intgrad homomorphic taylor ground cube experiment frame cube frame cube sequence high-pass homomorphic regions cube error ground gaussian standard deviation motion estimates original range flow model compared three grad high-pass intgrad homomorphic taylor errors small errors model int prefilter yields highly estimation side illumination changes side illumination range flow estimates estimates improved models brightness changes models prefilters yield original range flow left side brightness changes high-pass model grad motion estimates spatial brightness changes light motion estimates light model taylor model intgrad yield accurate motion side table errors models regions left side cube average angular error standard deviation estimated plant growth compare average relative growth cube estimated relative growth relative area change local surface calculated sensor coordinates vector relative growth rate determined table models perform left side side illumination changes homomorphic prefiltering estimates model taylor robust respect models schuchert range flow varying algorithms comparisons table average angular error average relative growth rate percent frame standard regions left cube errors standard degree degree note method input frames scene flow approach huguet technique stereo camera sequences optical flow approach apply algorithm huguet parameter rotating weighting parameter gradient constraint increased order reduce brightness algorithm input camera stereo frames frame cameras cube data compared original range flow side errors left side prefiltering improve input improve estimation appropriate compare performance models neighborhood estimation neighborhood defined gaussian filter standard deviation left side models yield higher values intgrad homomorphic prefiltering yield higher data estimation side model int prefiltering performs model grad high-pass prefiltering yields model intgrad homomorphic prefiltering model taylor prefilter perform better size filter conclude brightness constancy homomorphic prefiltering high-pass prefiltering size prefilter estimation small modeling brightness changes yields accurate model effects brightness average angular error motion estimates left cube increasing weighting matrix real data experiments model taylor yields reliable fleet tested brightness change model sequence arm changing experiment range flow estimate structure motion stereo camera structure algorithms described prefilters size cube ieee transactions pattern analysis machine september 2010 image rotating arm sequence area structure motion estimated model taylor structure tensor matrix size estimate parameter parameter denotes signal note equation presented additional arm error ellipsoids error covariance range flow point ellipsoids percent frames input sequence well estimated motion uncertainty ellipsoids estimates model int prefilter regions brightness changes motion estimates neighborhood matrix gaussian filter motion estimates frame sequence illumination estimated structure motion arm model taylor fleet calculated error optical flow error covariance range flow error covariance area frames rotating arm motion estimates uncertainty ellipsoids model int model grad high-pass model intgrad homomorphic model taylor better model grad high-pass uncertainty ellipsoids model intgrad illuminated side estimates model taylor prefilter performs uncertainty ellipsoids compared motion estimates motion arm schuchert range flow varying algorithms comparisons estimated velocity fields moving scene illuminated light top leaf depth reconstruction images camera positions time motion sequence frames camera rate frame time images leaf motion camera area caused top leaf model taylor estimation regions illumination changes compared standard range flow model assumed temporal varying vector well accurate plant growth relative error percent spatial accuracy side cube estimation scheme designed estimate growth method accurate leaf growth estimation range plant leaf arm tobias schuchert institute chemistry dynamics extended range flow estimation described approaches handle inhomogeneous presented error analysis brightness constraints combination homomorphic prefiltering synthetic image experiments real sequences models prefiltering improved estimation data illumination changes standard deviation pre signal accurate motion high-pass filtering performs well gaussian noise shot noise homomorphic filtering shot errors gaussian noise strong brightness changes conclude homomorphic prefiltering allows accurate range flow estimation filter motion prefiltering increased uncertainty motion estimates signal modeling brightness changes models grad taylor high-pass prefiltering motion estimates higher accuracy model intgrad standard range flow model best model taylor noise brightness changes strong spatial noise-free data homomorphic prefiltering conclude modeling brightness changes approaches accuracy typical growth percent typical frame rate frames cube experiment growth estimates accurate homomorphic prefiltering model taylor side best growth estimates flow computer vision image estimation range flow rate range ieee pattern analysis machine motion estimation range ieee range flow scene flow structure image ieee computer vision pattern scene ieee pattern analysis machine brightness workshop optical flow computer computer vision optic flow accurate computer accurate optic flow computer brightness optical flow estimation ieee image optical flow physical models brightness ieee pattern analysis machine schuchert estimation surface depth changing change ieee image analysis filtering ieee transactions pattern analysis machine september 2010 stereo reconstruction scene flow estimation global computer huguet variational method scene flow estimation stereo ieee computer scene flow stereo ieee workshop computer image technique stereo image estimation analysis optical ieee pattern analysis machine combining local global optic flow computer robust estimation flow computer vision image optical flow ieee computer vision pattern flow varying computer processing scharr depth estimation extended optical workshop surface range data total squares ieee computer vision pattern tobias schuchert received diploma degree doctoral degree rwth aachen doctoral scientist digital image processing group institute chemistry dynamics institute forschungszentrum scientist institute system image systems machine vision interests include signal image pattern computer received diploma doctoral rwth aachen university doctoral scientist institute rwth aachen image medical image medical image university institute signal university institute computer rwth aachen interests include medical image signal pattern computer received paper published ieee transactions image processing ieee transactions image ieee image analysis signal processing ieee signal processing senior scharr received diploma doctoral group senior digital image processing group institute chemistry dynamics institute forschungszentrum university university group interests include signal image processing data sets analysis digital
ieee transactions pattern analysis machine june 2012 spatiotemporal stereo scene flow stequel matching ieee paper recovery temporally coherent estimates structure motion dynamic scene sequence binocular stereo novel approach presented based matching spatiotemporal quadric elements primitive spatial temporal image structure match constraints stequels correspondence binocular correspondence temporally coherent disparity estimates explicit motion matched stequels support direct recovery scene flow algorithmic evaluation ground truth data incorporated local global correspondence benefit stequels matching primitive comparison alternative methods temporal coherence disparity experiments stequel matching scene flow scene quadric image space defined spatial defined stereo correspondence integrated basing matching recover temporally coherent disparity optical flow scene flow vectors computed directly matched spatiotemporal representation allows spatial temporal image structure resolve ambiguous matches fashion consistent representation stereo correspondence general demonstrated local global dynamic visual system image data temporal spatial scene stereo motion areas computer integrated stereo motion received recovery scene structure respect dynamic estimates temporally motion estimates consistent scene situations instantaneous binocular matching ambiguous weakly textured surfaces epipolar aligned pattern dynamic resolve correspondence response paper novel approach recovering temporally coherent disparity estimates well scene flow estimates sequence binocular stereo correspondence matching primitives inherently spatial temporal image temporal imagery locally terms orientation captured spatiotemporal quadric referred orientation orientation structure department computer science vision york received june digital 2012 ieee previous work work stereo motion concentrated spatial temporal matches based approach exploited constraints temporal disparity based analysis work matched binocular recover estimates temporal tracking stereo motion emphasis recovery dense approach recovery binocular recovery consistent left optical flows combination flow general disparity left-right consistent optical flows consider space carving estimation optical flow cameras approach recovery motion optical flow multiple binocular disparity maps proposed ieee computer sizintsev spatiotemporal stereo scene flow stequel matching work focused integrated approach spatiotemporal stereo interest scene recovery video work concentrated recovery surface models individual stereo tracking time yield temporally consistent models multiple carving matching spatiotemporal volumes consideration image motion differences views work stereo motion estimation problem solved images surface proposed approach lines emphasis integrated approach stereo motion explicit surface multiple objects lines integrated approaches stereo motion work concentrated focused temporal integration correspondence simply disparity estimates previous frame optical flow combined stereo motion estimation terms work direct methods integrated recovery structure approaches estimation problem associated stereo optical flow recovery single estimation well space carving spacetime approaches considered motion stereo disparity single equation proposed emphasis integration binocular imagery novel basing matching representation image spacetime terms local spatiotemporal richer image employed previous image employed proposed approach representation spacetime imagery terms oriented spatiotemporal documented optical flow recovery tracking dynamic texture analysis recognition filters local spatiotemporal previous considered spatiotemporal quadric capture orientation image application motion flow comparison exploited spatiotemporal spatiotemporal stereo disparity estimation scene flow previous stereo work defined binocular correspondence based spatial filters proposed approach orientation application filter filters spatial temporal basing matching richer light previous work spatiotemporal quadric proposed matching primitive spacetime primitive captures local spatial temporal structure matching data estimate optical flow relationships corresponding spatiotemporal binocular views derived match spatiotemporal match primitives cost incorporated local global method recovery scene flow presented based left-right spatiotemporal quadric evaluation disparity scene flow estimation algorithms evaluation laboratory acquired binocular video ground truth evaluation naturalistic laboratory imagery associated ground truth preliminary technical approach spatiotemporal matching primitive temporal sequences binocular stereo correspondence terms image spatial temporal local pattern image spacetime pixel consideration local spatiotemporal orientation access richer local orientation visual orientations parallel image plane capture spatial pattern observed surfaces spatial orientations temporal dimension capture dynamic aspects temporal dimension matching inherently temporal combination temporal spatial structure match consideration data steerable filters representation orientation filter data oriented applied data ieee transactions pattern analysis machine june 2012 image spacetime spatiotemporal instantaneous motion orientation spatiotemporal left corresponding arbitrary vectors correspondence space explained pointwise rectified filtering set orientations unit measure local computed spatiotemporal image image denotes direction response orientations large motions exploited binocular image filtering applied left image filters oriented directions directions orientation employed directions defined vectors normalization vector unit point spacetime associated set values oriented local structure considered versus direction local richer orientation considered quadric structure terms spacetime binocular match context binocular quadric referred spatiotemporal quadric constructing match primitive individual energy terms spatiotemporal representation captures local orientation well spacetime captures local shape spacetime point versus surfaces plots steerable filter pair oriented red black spacetime corresponding set filter local energy response normalized normalization constructing local frame orientation corresponding local estimate quadric binocular computed pointwise spacetime left image sequences provide matching reference left local energy steerable filters pointwise implementation parallel construction filter observed efficacy filtering filters constructed alternative fashion spatiotemporal local spatiotemporal lower order preliminary experiments yields generally inferior context considered experiments stequel versus sizintsev spatiotemporal stereo scene flow stequel matching grammian explained stequel constructed filter orientation grammian stequels constructed pair provide measure orientation structure pointwise construction grammian aggregation spatiotemporal epipolar correspondence constraint correspondence binocular incorrect simply local spatiotemporal orientation change views constraint derived corresponding stequels rectified binocular constraint derived relationship local spatiotemporal orientations left image spacetime derived scene point arbitrary relative imaging displacement imaging combination analysis scene relationship left right-based flow sets light spatiotemporal orientation differences application disparity previous work disparity estimation focused flow relationships capture relationship arbitrary orientations left directly relationship binocular stequels denote vectors denotes points relative denotes points relative denote left cameras coordinate vectors image spacetime time representation time left image coordinate vector image spacetime left-right flow relationship consider observed cameras situation cameras stereo reference coordinate system stereo cameras rectified vector left optical point arbitrary displacement note moving points corresponding image left view time point upper image spacetime coordinate left consider flows views temporal relate left spatiotemporal left camera flow vectors camera terms temporally left flow substitution yields substitution letting time denote yields components time left views relationship left flows ieee transactions pattern analysis machine june 2012 captures instantaneous change general orientation relationship relationship derived dominant motion stequels capture directions left consider directions binocular arbitrary yields vectors left capture purely spatial orientation corresponding elements case frontoparallel surfaces oriented texture binocular general case surfaces slanted respect imaging orientation corresponding elements change linear considering third element vectors stereo epipolar construction constraints rectified relationship form yields case associated situation frontoparallel stereo case point view yields well estimation relate corresponding stequel intensity directions response unit direction spatiotemporal orientation intensity corresponding yields arbitrary orientations stequels general constraint binocular referred stequel correspondence constraint approach stereo letting yields corresponding vectors substitution stequel match cost stequels correspondence disparity match cost cost derived based stequel correspondence error direction vector orientation matching stequel correspondence yields equation form set set directions yields set equations estimated form intensity normalized filters stequel construction sizintsev spatiotemporal stereo scene flow stequel matching solution error terms order rfm right-based optical flow left stequels recover optical flow left image region projecting stequels smallest captures locally dominant spatiotemporal image plane corresponds smallest eigenvalue left optical flow respect rfm simply elements solved standard linear letting rfm yields stequels consideration stereo local match preliminary experiments match cost based linearized inferior comparison original nonlinear support nonlinear estimation optimization solution context described original nonlinear versus linearized relative disparity estimation computation time order performance linearized solution explained interest error measure error matching simple solution matching normalized terms computational left flows combined disparity yield flow methods method presented direct access provided camera spatiotemporal stereo disparity estimates recovery scene scene flow displacement spatial stequels defined image consider disparity matched correspondence disparity spacetime relative left consider point left spatiotemporal volumes flow allows relationships disparity spacetime reference components relate left flows locally dominant spatiotemporal orientation captured smallest eigenvalue local arg min qk2 scene flow estimation derived spatiotemporal disparity estimation explicit recovery motion stequel correspondence match matched stequels estimate scene motion directly arg min qk2 denotes vector considered right-based camera considered ieee transactions pattern analysis machine june 2012 substitution equations relate allows combined arg min qk2 qk2 explicit normalization allows previous equation arg min arg min standard solved associated smallest eigenvalue recovery flow vector projecting fashion projecting optical flow recovering motion matched note spatiotemporal structure region structure motion recovery allows situation case vector corresponds measure normalized flow problem flow evaluation stequels constructed steerable filter filters spatiotemporal support employed stequel computation disparity estimates subpixel stequels matched estimate scene flow algorithmic motion recovery equations subpixel disparity estimates recovered 640 480 video implementation pixel local global matchers work simply single frame pixel matchers denoted nost-local normalized spatial aggregation local global pixel data cost computed images order robust differences images stereo alternative method temporal optical flow estimated spatiotemporal direction match cost aggregation oriented filtering stequel construction optical flow recovered stequel representation comparison optical temporal aggregation local matcher global matcher constructing spatiotemporal mrf graph local aggregation matcher denoted comparison alternative spacetime matching directly approach matching temporal aggregation local spacetime optimization spatial depth allow local global denoted zhang-local comparison local methods matching primitives allow access performance stequel matching comparison global methods generally provide superior stequels order algorithmic implementation binocular stequels sequences local match disparity stequels approach disparity local match local global occlusions matcher matchers denoted st-local lab sequences laboratory data sets sequence captured stereo camera ground truth disparity maps recovered light approach scene slanted depth texture oriented epipolar lines central central complicated objects nontrivial boundaries sequence stereo camera sizintsev spatiotemporal stereo scene flow stequel matching lab1 example left frame 640 480 ground truth disparity labeled boxes recovered disparity maps algorithms truth error values pixels intensity corresponds regions interest red best complicated motion depth scene moves camera scene visual image nost-local regions epipolar aligned texture generally simple temporal aggregation provided performance boundaries recovery flow estimates zhang-local matches camouflage region estimates epipolar-aligned textured aspects zhang-local errors image temporally st-local best local matchers ability include temporal allows resolve match explicit flow improvements st-local nost-local consider lower left regions red rectangles plane st-local accurate local methods objects space image captured stequels global nost-global recover complicated boundaries disparity estimates regions defined nost-global regions epipolar aligned texture incorrect estimates epipolar-aligned camouflage recover disparity stequel representation situations ambiguous purely spatial note camouflage nost-global estimate disparity simple epipolar-aligned incorrect matches st-global temporally consistent nost-global second lab constructed acquired depth motion motion spatiotemporal left spatiotemporal volumes slanted surfaces depth large image ieee transactions pattern analysis machine june 2012 lab2 example left frame 640 480 ground truth disparity labeled boxes recovered disparity maps algorithms truth error values pixels intensity corresponds regions interest red best motions individual left frame algorithmic considered analysis lab1 respect local st-local benefit weakly textured regions performance large image recover direct stequel-based matching temporal flow well nontrivial respect global stequel-based matching st-global weakly textured slanted light note performance zhang methods textured associated captured simple temporal stequels constructed pointwise intensity structure explicit temporal aggregation problem error plots lab1 lab2 improvements stequel-based matching comparison zhang average errors sequences benefit stequels local global plots average temporal stequel-based plots comparison purely spatial matching provided temporal dimension inferior naturalistic imagery superior temporal coherence comparison differences presented presented previous report disparity estimation differences average differences stequel report stequel change representation support scene flow considered office sequence third data naturalistic ground office scene camera moves sizintsev spatiotemporal stereo scene flow stequel matching error lab1 lab2 error pixel estimated plots average error points pixels surface black error frame plots points error frame superior ability stequel matching temporally coherent disparity maps comparison stequel single frame st-global best temporal coherence corresponding respect consistent disparity estimates recovered stequel nost-local accurate rover sequence data sequence acquired rover central left example frames processing stereo sequence camera frames reference frames video recovered motion estimator respect original comparison improvements temporal coherence st-local method consideration temporal frames temporally coherent inferior recovery boundaries office left images frames boxes labeled recovered disparity accompanying videos ieee transactions pattern analysis machine june 2012 rover left view frames 640 480 recovered disparity maps corresponding accompanying videos errors ground errors recovered stequel-based spatiotemporal direct motion estimation performance described motion third lab data lab example frames scene textured rectangles frontoparallel respect left camera camera parallel optical cameras rectangles parallel optical disparity lab lab acquired scene flow scene subpixel tracking lab data set ground truth disparity example motion estimation pair stereo frames 640 480 left original intensity images time disparity flow components associated accompanying videos sizintsev spatiotemporal stereo scene flow stequel matching example motion estimation frame lab data set left recovered recovered angular error corresponds corresponds motion disparity space recovered robust estimator yield dense disparity flow maps described motion estimator lab data set angular error recovered motion vectors data set angular error maps left surface motion large motions parallel imaging plane documented error moves upper motion upper motion recovered direction flow plots recovered estimates recovered motion frames lab lab office flow vector confidence recovered motion considering explicit optimization flow vectors lab light purple light corresponds camera moving lab capture directions purple light camera moving office sequences capture motion lower image moving purple purple confidence areas example motion estimation lab lab office data sets left left confidence values corresponds accompanying videos ieee transactions pattern analysis machine june 2012 image texture yield structure support motion estimates black energy models optical computer vision black robust estimation multiple flow computer vision image tracking spatiotemporal oriented energy 11th european computer recognition european computer depth ieee pattern analysis machine disparity maps image motion computer based spacetime oriented structure ieee computer vision pattern derpanis texture recognition based spacetime oriented ieee computer vision pattern derpanis linear solution motion estimation disparity ieee derpanis steerable ieee image derpanis spatiotemporal oriented energy ieee computer vision pattern recognition ieee visual performance evaluation tracking spatiotemporal system digital ieee computer vision pattern stereo motion robust steerable ieee pattern analysis machine temporal stereo european computer processing computer stereo motion analysis direct estimation scene ieee computer model image optical method scene flow estimation stereo 11th ieee computer motion disparity estimation computer temporal constraints dynamic stereo computer image paper described novel approach recovering temporally coherent disparity estimates stequels spatiotemporal matching temporal coherence primitives match cost inherently temporal matches ambiguous considering spatial pattern temporal stequel matching simple linear evaluation data sets benefit stequel matching incorporated local global stereo sequences ground truth comparison algorithms benefit stequel matching ability temporal image motion optical flow estimation weakly textured paper demonstrated stequels situations provide temporally coherent estimates stequels allow stereo matching spatiotemporal optical flow recovery local flow data optical flow yields temporal stequels spatiotemporal structure note nontrivial model time mrf model temporal graph defined flow directly standard mrf performance documented efficacy binocular correspondence disparity stequels provide estimation scene matched stequels allow direct recovery flow left image flow efficacy approach demonstrated simple solution spatiotemporal stereo scene flow estimation robust described consider stequel matching spatiotemporal explicit work space sizintsev spatiotemporal stereo scene flow stequel matching computational stereo correspondence set linear spatial second european computer based machine vision visual correspondence occlusions graph ieee computer second shape space computer consistent multiple video 11th ieee computer energy approach dense pattern motion structure estimation stereo computer vision image estimation structure motion ieee computer stereo computer point stereo scene flow estimation ieee computer stereo optical linear model estimation motion motion video depth estimation stereo depth maps ieee computer vision pattern motion ieee pattern analysis machine sizintsev stereo spatiotemporal quadric element technical report york sizintsev stereo spatiotemporal quadric element ieee computer vision pattern sizintsev stereo vision accurate image vision estimation motion scene structure moving stereo ieee computer vision pattern applied integration depth european computer integration flow optical motion carving ieee computer vision pattern image ieee pattern analysis machine structure estimation stereo image ieee june disparity occlusions stereo video ieee computer vision pattern spatiotemporal oriented comparison 11th european computer shape recovery dynamic ieee computer vision pattern zhang scene flow structure ieee computer vision pattern zhang motion computation sequence stereo computer sizintsev received computer science york york department computer science sarnoff corporation stereo areas include stereo emphasis spatiotemporal processing member ieee ieee computer received sarnoff corporation member technical vision department computer science york member vision areas interest include computational vision well aspects image include sarnoff corporation technical ieee paper ieee member ieee ieee computer digital
range flow depth intensity data combined intensity depth estimation local moving combined local total squares algorithm global locally constant smoothly varying intensity synthetic real test constraint equations depth time total time range flow motion constraint equation denote intensity data well brightness constraint equation intensity constraint equation sequence depth maps motion surface computed denote motion depth maps depth laser typically intensity intensity depth data range flow intensity depth data total squares estimation based locally constant flow model based estimation motion moving rigid range sequence intensity depth presented local rigid model based range flow estimation range flow optical flow surface depth equations three range tls solution locally constant flow solution based smoothly varying flow constraints constraints intensity depth relative weight range data intensity typically factor 100 depth weight constraints factor computed intensity depth data intensity depth additional based local tls estimation total squares range flow solution solution total squares solution aperture energy range flow constraints smallest eigenvalue allows depth solution minimum flow linear constraints corresponding minimum solution normal flow corresponding combined depth intensity corresponding normal intensity computation full flow global smoothness computation smoothly varying range flow presented well optical flow algorithm model yields energy presented integration number currently moving locally yields eigenvalue integration minimum smallest eigenvalue range flow corresponding equation smallest eigenvalue motion moving smoothness constant factor time integration image clearly motion smoothness optical flow better minimum energy equation equations computed linear aperture depth intensity constraints aperture three linear denote compute solution figure synthetic test depth intensity currently intensity well depth intensity range flow estimation methods relative error flow correct movement figure depth global smoothness depth data zonly zonly zonly iandz iandz iandz iterations 100 500 synthetic data additional intensity synthetic sequence correct movement depth maps local tls estimation full surface allows computation flow lines movement lines linear intensity lines full flow density clearly case intensity depth lines surface intensity allows computation normal intensity relative error error test case global smoothness algorithm compute correct flow depth case number iterations intensity yields numbers numbers so00 100 500 so00 re1 mag error dir error clearly methods real data data laser range intensity brightness laser figure real test correct movement linear depth computed flow weight density tls case numbers figure real test depth intensity data zonly iandz density re1 mag error dir error density error figure combined intensity depth global flow algorithm better number data zonly zonly iandz iandz iandz iterations 500 500 so00 re1 mag error dir error figure depth global smoothness depth three optical linear depth brightness rigid motion range image optical estimation motion range range flow range motion range image estimation range flow range data better depth depth data algorithm intensity intensity depth data range flow typically additional intensity aperture compute flow methods synthetic real test methods currently motion rigid motion depth optical intensity depth factor
computer vision image range flow estimation center center scientific university barron computer university 2002 discuss computation instantaneous displacement vector fields deformable surfaces sequences range version motion constraint equation evaluated directly sensor aperture problem encountered derived constraint solutions total squares regularization scheme compute dense full flow fields sparse tls performance algorithm synthetic real finally method compute motion field 2002 science living plant range data total aperture instantaneous velocity field describes motion deformable surface denoted range range flow velocity field derived sequences range data range flow field study dynamic changes displacement vector field called scene flow computed directly image sequences 2002 science barron extended compute shape observed surface work range sequence analysis estimate sensor respect rigid estimate instantaneous velocity field moving deformable velocity described differential equation parameters constant small surface motion estimation nonrigid surfaces surface movement described study range flow extension optical flow estimation approximation local image image sequences image motion estimation determine region image differential displacement energy functional examples differential compute displacement vector optical flow method extended work range presented range flow estimation considered differential work range flow constraint equation evaluated directly sensor total squares problem range flow regularization scheme compute dense velocity fields sparse tls paper constraint equation range aperture problem encountered equation examined introduce additional constraint derived intensity constraint equations velocity field estimated locally total squares regularization method dense flow field sparse tls finally synthetic real data motion constraint equation surface viewed depth function local planar function expressed order expansion denote partial derivatives respect local coordinates point change depth range flow estimation time assumption motion pure change written local displacements form range flow field denoted equation called range flow constraint equation equation rate constraint equation change constraint equation optical flow image motion computation motion problem implies temporal sampling dense note approximation constraint equation tls estimate described observed scene order range flow motion constraint equation derivatives depth function respect coordinates entirely desired derivatives local model surface data grid membrane work directly version range flow constraint notice range sensors considered yield depth function terms data sampling grid sensor coordinates denoted note measured values independent observed point corresponding sampling three components range flow field total derivatives respect time derivatives yields total derivatives respect time partial derivatives change sensor coordinate frame range flow motion constraint expressed sensor coordinates barron respect notice computed derivatives sensor frame derivative constraint evaluated entirely sensor grid linear filter equation constraint equation independent sensors sensor coordinate range flow constraint equation yields initial constraint equation notice partial derivative respect time assumption sensor coordinate assume orthogonal projection implies case constraint equation simply aperture problem constraint three initial assumption locally planar surface plane movement perpendicular plane three-dimensional version aperture problem optical flow optical flow constraint equation describes velocity range flow constraint equation plane surface normal solution achieved vector minimum origin constraint solution defines normal linear dependencies range flow constraint equations region discuss range note flow data range flow estimation example range data three types neighborhoods encountered three-dimensional three types neighborhoods illustrated structures clearly three components movement determined estimate called full movement linear structures local velocity perpendicular planar surfaces velocity plane perpendicular surface denoted plane three region types constraint equations discussed define planes velocity three-dimensional three planes constraint equations local region yield full independent constraint equations considered neighborhood full velocity vector computed constraint planes illustrated linear structures planes result constraint planes examined point origin appropriate flow plane perpendicular linear structure motion direction structure considered neighborhood single constraint equations averaged constraint point plane origin defines velocity normal planar plane flow averaged derivative values note neighborhood plane flow estimate noise normal flow computed describes local estimate total squares described linear dependencies compute appropriate independent constraint equations neighborhood full plane barron normal squares solution range flow appropriate normal flows described intensity constraint apart structure discussed optical range sensors intensity observed clearly order constraint equation assume intensity observed point remains depth optical changes intensity motion horizontal yields constraint combined additional constraint written intensity changes distance vertical motion range flow estimation algorithm solution apart estimate additional intensity constraint allows estimate full range flow range constraint equation local tls solution tls solution presented extension structure tensor algorithm optical flow estimation method viewed case technique parameter estimation image sequences constant flow region equations equations written form constraint pixel order parameter estimation local neighborhood assume parameters constant assumption smoothness method described assumption locally constant parameters minimization min range flow estimation function defines spatiotemporal neighborhood parameters solution yield unconstrained energy functional energy functional minimum derivatives respect derivatives locally constant parameter field assume function equations written matrix form eigenvalue equation extended structure tensor eigenvectors energy functional components computed image data vector denotes filter local spatiotemporal structure extension structure tensor encountered optical flow computation energy minimized written energy simply corresponding eigenvalue corresponding minimized eigenvector smallest eigenvalue minimization problem orthogonal approximation problem solution tls solution real computed desired range flow e14 e24 barron order time flow accuracy compute range flow tensor regions magnitude regularization described confidence measures local constant flow model smallest eigenvalue directly measures motion data described single flow case pure noise introduce confidence determine eigenvalue considered choose note smallest eigenvalue noise denotes local average computed filter example noise choose high confidence parameters estimated independent eigenvalue close eigenvector vector nullspace yields aperture problem compute sensible compute type well nullspace eigenvalue define measure encountered type flow flow full flow normal flows vector nullspace solutions minimum norm motion components result changes measured solution vector algorithm presented discuss minimum norm solutions expressed linear eigenvectors result norm discussed types normal flows range linear structures motion range flow estimation example flow types range synthetic depth components estimated flow full plane components estimated flow full plane structure corresponding minimum norm velocity flow computed e11 e14 e12 e24 e14 e24 e13 planar structures movement perpendicular plane eigenvalue plane flow e11 e14 e14 e12 e14 e11 e12 e13 e13 e13 motion measured full flow computed eigenvector single eigenvalue synthetic example three encountered flow intensity illustrated intensity yields constraint range order constraint barron appropriate combined energy minimized written min data channels weighted constant intensity depth data additional constraint combined structure tensor simply weighted depth intensity remains combined tensor intensity data local increases intensity well depth aperture planar surface computation full flow plane flow additional intensity synthetic sequence correct movement depth local tls estimation sparse full surface allows computation plane flow perpendicular lines movement direction lines linear intensity pattern displacements lines computed full flow increases clearly case intensity depth lines planar surface intensity allows computation normal intensity direction remains relative error directional error test case synthetic depth intensity range depth intensity range flow estimation regularization additional intensity data full flow estimate regions normal flows estimation data sensible flow regularization scheme close tls estimation estimate dense flow flow tls algorithm denote computed structure tls solution eigenvectors desired define projection matrix subspace determined tls estimated flow vector solution regularized solution close confidence tls solution data term confidence values parameters membrane smoothness membrane model data moving smoothness term neighborhoods extended temporal smoothness well data smoothness terms considered yields minimization min barron smoothness constant minimum achieved equations extension temporal term denote partial vector equations derivatives yields idempotence projection matrix denotes local average approximation solution minimization introduce solution a-1 a-1 energy functional tls improves rate find a-1 a-1 idempotence projection matrix examine yields idempotence range flow estimation example regularization range flow field computed data field orthogonal initial tls solution local average subspace determined tls solution weighted local average determined regularization constant confidence measure notice tls subspace orthogonal regularization range flow field illustrated algorithm data regularization flow field normal increasing full flow regions initial full flow estimation time example filter compute performance proposed algorithm synthetic real test examples real scientific error measure order accuracy estimated range error measures correct range flow denoted estimated flow error measure describes relative error velocity 100 measures difference estimated correct velocity correct direction barron directional error second error directly angle correct velocity vector estimated vector describes correct direction apart averaged error values determine range error values synthetic test data synthetic range data model structured light sensor pixel sensor sensor study plant regularized pixel compute averaged error test sequence synthetic plane distance 300 angle surface normal intensity pattern second type synthetic data sphere 300 center intensity sphere described terms angles surface choose computed range data 100 full flow estimation performance structured light normal flows intensity data noise data channels accuracy translation plane tilted movements relative magnitude error directional error observe qualitative range flow estimation movements considered case intensity structure synthetic range intensity depth range flow estimation error depending noise data relative magnitude direction error movements find noise data noise intensity estimation accuracy order planar scenes considered locally depth accuracy coordinates velocity examine performance three noise sensible range sensors distance relative error types movements noise translation find error values noise data channels relative magnitude error translation rotation plane barron accuracy estimation higher velocities velocities range flow estimated confidence case high computed type motion rotation achieved angles surface velocity relative error increasing velocity small changes angles flow field remains rotation change angle surface normal accuracy changes implies observed surface frame rate range flow accuracy data synthetic relative directional error translation qualitative plane test data data error order magnitude error order performance nonrigid test example sphere increasing second example intensity pattern tilted relative error tls range flow estimation sphere find errors expansion encountered speed high algorithm compute flow plane accuracy computed flow examined error increases higher expansion high expansion relative error directional error note noise full flow computed tls estimation synthetic data examine regularization example moving sphere relative magnitude error error tls range flow sphere depending velocity relative error directional range flow estimation relative error tls range flow nonrigid sphere expansion tilted directional error tls error values order 100 order presented displacement sensor approximation displacement projection horizontal movement denotes pixel displacement exceed example displacement 300 vertical displacements real test data synthetic data real examine real test observed objects linear pure movements error regularized range flow sphere depending velocity relative error directional barron real test data biris intensity depth data accuracy biris structured light range biris range sensor objects test range flow intensity depth data examples data tls regularized correct flow clearly regularization yields dense full flow improves initial tls computed velocity accuracy examine range flow accuracy structured light sensor university test objects linear displacement examine crumpled paper leaf example intensity depth data test scenes difference motion horizontal vertical direction relative directional errors depending observe difference movements function horizontal vertical movements horizontal vertical motion horizontal clearly relative error depending find observed increasing regularization accuracy small test objects crumpled paper order relative error speed exceed horizontal note full flow range flow estimation real structured light data range flow frame example sequence crumpled paper intensity leaf intensity crumpled paper data high increasing directional error depending horizontal movement higher error estimated velocity direction speed exceed real data relative error directional error depending motion relative error directional error depending motion barron example data movements castor bean structured light sensor sampling biris sensor sampling leaf viewed leaf unconstrained leaf leaf viewed unconstrained leaf error vertical velocity observe velocities qualitative synthetic errors real data regularization improves crumpled paper relative error remains average well velocity find directional error regularized flow considered leaf motion finally examples scientific proposed range flow estimation range flow fields living castor bean leaves types movement leaves plane entirely movement leaf horizontal vertical motion example leaf moving unconstrained leaf example leaf movement motion velocities encountered algorithm compute dense range flow fields sequences range data constraint equation evaluated range flow estimation directly sensor local velocity estimate total squares technique plane normal well full range flow sparse tls technique yield dense full flow performance proposed algorithm synthetic real data method finally motion living castor bean leaves minimum norm solutions estimate parameter vector linear dependencies vector nullspace sensible estimate estimate derived assume linear combination eigenvectors solution enn norm additional constraint expressed ein equations combined energy functional min minimum partial derivatives respect yields ein ein barron minimum norm solution ein ein solution terms eigenvectors note components ein ein ein ein parameter vector expressed vector ein ein ein ein parameter solution expressed linear combination eigenvectors case smallest eigenvalue parameters enn enn enn solution work sequence analysis dynamic science three-dimensional scene september scene flow structure image scene shape july range flow estimation linear depth rigid motion range image motion estimation range estimation rigid motion range intensity estimation motion range motion sparse range data nonrigid motion sequences range point image vision combination estimation range flow deformable shape rate range pattern performance optical flow vision computer vision optical computer vision image optical range flow measure plant image vision barron regularized range vision dense range flow depth intensity september study spatiotemporal image total squares analysis dynamic scenes range july computer total squares differential range flow september performance three vision optical dynamic range data university july three-dimensional
computer vision oriented light-field windows scene flow ren light-field images enable accurate robust pixel comparisons spatial pixel distribution angular focused correct effective depth angular rays consistent lambertian rays scene focused correct depth angular rays spatial sensor spatial scene develop oriented light-field account shearing light field ray space translation pixel general representation approaches single spatial pixel angular spatial loss precision defocus application scene involves estimation corresponding spatial points scene flow vector field motion scene point conventional optical flow change scene flow applications including computing camera object estimating propose algorithm compute dense scene flow light-field light-field camera demonstrate oriented light-field windows provide better matching conventional spatial light-field matching energy traditional brightness demonstrate oriented light-field windows filtering images bilateral filter images better traditional spatial image windows pixel values computer vision applications correspondence optical flow bilateral image pixel window comparisons defocus blur loss light-field cameras propose oriented lightfield windows enable robust accurate pixel lambertian focused correct distribution angular rays pixel develop oriented light-field window shearing translation application scene optical flow vector field motion point benefits oriented lightfield windows standard spatial demonstrate applications oriented light-field windows bilateral filtering image pixel comparisons computer vision including images brightness consistency optical scene weights bilateral pixel image conventional compare pixels rgb color robust compare pixel windows involves loss pixel comparisons window comparisons depth windows corresponding scene points depths focal blur propose method represent scene work light-fields computer vision light-field distribution light rays light-field figure scenes camera point focal second focal conventional camera images focal change depth corresponds shear compare spatial shear light-fields account figure oriented light-field windows gaussian flatland scene point position depth match windows detect scene bilateral filtering image segmentation bilateral filter applications image image images bilateral filtering rgb cie-lab color edges figure oriented light-field edges improve bilateral image segmentation computer images images figure improve slic superpixel segmentation algorithm traditional pixel values oriented light-field windows cie-lab color space pixel better detect edges pixel intensity capture light-field camera light-field data applications image glare stereo single capture light-field images general depth estimation defocus single work light-fields depth estimation image light-field representation scene points improve scene bilateral image scene flow scene flow computed estimating optical scene flow image camera motion field optical compute scene flow stereo images involves estimating disparity change disparity optical typically variational approaches estimate methods regularization piecewise scene flow method local estimation compute methods accurate dense depth rgbd cameras compute scene flow pairs rgbd typically rgbd point representation oriented light-field windows angular rays light-field corresponding point scene consistent lambertian rays account shearing difference depth scene point light-field focal flatland image represent point scene oriented window light-field ray visualized window defined shear effective window defined spatial angular gaussian oriented light-field window corresponding scene point computed shear translation operator shear operator depth figure scene point moves position flatland corresponds shear shift flatland conventional image corresponds focal blur shift match scene points oriented light-field shear compute window defined spatial angular gaussian weights difference oriented light-field match scene points image compute window defined spatial gaussian weights difference spatial camera scene depths translation operator defined matching application oriented light-field windows match pixels discussed scene compare oriented light-field example computing matching oriented light-field windows standard spatial windows visualized general oriented light-field spatial angular conventional spatial window central pinhole spatial angular directions oriented lightfield window approaches light-field image angular directions spatial angular operator defined gaussian distribution centered variance oriented light-field window operator scene point position maximum precision spatial light-field windows formulation matching general oriented light-field develop algorithm light-field scene standard brightness consistency light-field scene flow energy develop regularization compute scene scene flow depth range centered effective depth scene computing data regularization scene flow method piecewise flow field accurate flow low confidence local flow energy smoothness regularization term light-field brightness consistency regularization scene flow vector pixel point effective depth traditional brightness consistency optical flow intensity images point spatial pixel windows oriented light-field windows point confidence optical flow depth confidence occlusion computed scene flow pixel search search space depth optical flow method estimate depths light-field images well depth estimation confidence compute energy algorithm compute confidence data energy search account scene flow energy data term smoothness term scene flow estimation scene flow formulation formulation depths scene point smoothness term discussed data spatial angular note formulation light-field images traditional angular traditional optical flow formulation spatial pixel windows pixels central pinhole search confidence regularization smoothness term estimate pixel occlusion consistency optical regularization smoothness term pixels optical flow pyramid pyramid srsf mdp-flow2 table cow teddy figure local matching optical flow pyramid synthetic optical flow visualized middlebury color confidence values visualized cold-to-warm color colors higher oriented light-field windows accurate optical flow higher table rmse values algorithm scene flow optical flow algorithms three example algorithms table scene algorithm cow teddy scenes rmse scene algorithm top performer rmse top performer estimation filter flow discussed oriented light-field windows computing scene algorithm state-of-the-art scene flow optical flow algorithms synthetic scenes including comparisons synthetic scenes figure local optical flow search pyramid synthetic rgb pixel values central pinhole images pixel values cie-lab color oriented light-field rmse values maximum flow rmse values range oriented light-field windows accurate higher oriented light-field windows scene points pixel intensity figure synthetic spatial angular oriented light-field note rmse angular variance spatial directions benefits light-field windows pixel spatial pixel window spatial window precision oriented light-field windows angular spatial corresponding spatial variance optimal angular variance spatial window figure synthetic scenes state-of-the-art scene optical flow example algorithms table algorithm cow teddy scene lambertian object single point light compare optical flow scene flow algorithm optical flow algorithm motion preserving optical flow algorithm top performer middlebury optical flow provide optical flow algorithms srsf algorithm central pinhole light-field image provide method pinhole images spatial lightfield sensor mdp-flow2 method srsf algorithm rgbd data sensor dense accurate depth estimation input input light-field provide input depth estimation light-field images compare algorithm scene flow algorithms pairs stereo images well pairs pinhole images light-field table rmse values algorithms three algorithm srsf algorithm compute flow figure rmse synthetic rmse spatial angular optimal spatial angular synthetic optimal local optical flow estimation optimal pyramid optical flow local optical flow estimation flow oriented light-field rmse angular variance maximum optimal optimal optimal difference optimal figure compare algorithm srsf scene flow algorithm mdp-flow2 optical flow algorithms three synthetic rmse values table optical flow visualized middlebury color flow depth visualized cold-to-warm color color three scenes cow moves table moves teddy moves scene flow accurate figure compare bilateral filtering image rgb cie-lab oriented light-field better preserving edges compare rmse optical flow rmse algorithm accurate algorithm accurately estimate scene flow scenes spatial light-field camera flow estimation scenes figure scene flow algorithm camera compare scene flow srsf optical flow mdp-flow2 oriented lightfield windows enable accurate scene flow correctly estimate scene flow depth scenes penguin well scenes scene tennis note better capture motion tennis penguin scene flow figure compare slic superpixel segmentation algorithm cie-lab pixel values oriented light-field color figure better oriented light-field windows better range cie-lab pixel figure oriented light-field windows better superpixel segmentation edges pixel low work oriented light-field accurate robust method pixel lightfield oriented light-field windows represent scene points shearing translation oriented lightfield windows compute scene benefits standard spatial windows spatial angular variance demonstrate benefits oriented light-field windows algorithm state-of-the-art methods scene optical demonstrate applications bilateral filtering image better detect oriented light-field windows general scene work light-field images computer vision graphics work work ren applications bilateral filter weights spatial typically rgb cie-lab color oriented light-field windows range figure methods oriented light-field windows edges pixel low superpixel image segmentation algorithms segmentation motion slic superpixel segmentation algorithm oriented light-field windows bilateral filtering image segmentation figure compare scene flow srsf rgbd scene flow mdp-flow2 optical flow optical flow visualized middlebury color second flow depth visualized cold-to-warm color color color corresponds depth colors depth motion colors depth motion top example algorithm accurately estimate scene occlusion accurate second example tennis shift depth algorithm accurately estimate scene correctly example penguin moves algorithm correctly scene flow object slic state-of-the-art superpixel single stereo segmentation optical scene flow centered variational scene flow estimation correspondence image disparity disparity flow estimation graphics scene scene flow dense motion estimation color variational method scene flow estimation stereo flow depth color light field local image bilateral computer graphics dense scene flow estimation rgbd glare ray glare camera ren image optical flow estimation depth defocus correspondence lightfield optical flow depth estimation light-field bilateral filtering color light field space dense depth cameras scene piecewise scene consistent depth light scene flow motion motion preserving optical flow
cascaded scene flow prediction semantic segmentation frames stereo scene flow methods estimate geometry motion superpixels motions rigidly moving assume scenes foreground objects rigidly moving semantic cues produce scene flow cascaded classification framework models scenes iteratively semantic segmentation stereo rigid motion optical flow evaluate method challenging kitti autonomous driving motion leads state-of-the-art scene flow dense geometry motion motion field motion estimates depth map inferred stereo scene flow algorithms motion capture optical flow estimation stereo scene flow methods geometric cues improve stereo flow scenes challenging kitti scene flow benchmark state-of-the-art scene flow algorithms assume superpixels rigid conditional random fields provide temporal spatial regularization motion methods perform background regions accurate moving foreground estimating geometry moving foreground objects motion challenging stereo flow accurate estimation vehicle geometry motion autonomous driving improve models model motion objects background regions methods estimation optical flow semantic cues improve motion segmentation bottom-up cues semantic segmentation segment scenes object motion models accuracy optical flow instance-level semantic segmentations scene flow estimates cascade conditional random fields define crfs dense segmentation stereo depth optical flow rigid motion estimates foreground high dimensionality refine iteratively cascaded classification model stage cascade structural svm learning algorithms evaluate previous scene flow challenging kitti autonomous driving benchmark improve state-of-the-art two-frame scene flow work semantic cues recovery geometry motion methods scene flow estimation defined scene flow dense motion flow estimates optical flow fields proposed approach stereo optical stereo flow algorithms improve segmentation geometry initial flow motion initial geometry flow reference frame updated geometry updated flow figure method scene flow frames stereo initial geometry optical flow estimates scene flow algorithm semantic segmentation cues foreground cascade crfs iteratively inferred updated geometry estimate flow errors stereo optical flow scene flow algorithms input standard optical flow stereo challenging high dimensionality disparity motion reduce piecewise rigid scene flow model superpixels scene flow scene flow methods stereo optical flow methods large challenging kitti dataset work multiple frames improved prsf model bottom-up cues superpixel foreground objects motion global kitti dataset foreground objects scene flow proposed object scene flow algorithm scene independently moving superpixels performance osf improved model bottom-up motion objects inference time osf method convolutional networks vision methods estimating optical disparity scene flow estimation large models scene flow predictions networks data state-of-the-art methods kitti scene flow benchmark work motion segmentation optical flow prediction large motion objects multiple input semantic segmentation semantic cues improve optical flow methods scene object apply optical flow model semantic scene object cascaded prediction framework solve high-dimensional inference evaluate algorithm challenging kitti dataset semantic cues leads state-of-the-art scene flow modeling semantic scene flow frames corresponding stereo estimate segmentation stereo optical flow pixel reference frame denote variables pixel reference semantic foreground object rigid motion optical denote disparity semantic segmentation pixel second frame frames estimate scene model motion second scene flow algorithms predictions superpixel modeling semantic scene predictions semantic object noisy assume scene foreground objects autonomous driving rigidly moving accurate semantic segmentation foreground geometry pixels segment optical flow rigid high dimensionality scene flow refine estimates cascade models parameters structural svm stage cascade improvement scene current estimates scene semantic segmentation instance-level segmentation algorithm disparities optical flow fields prsf method cascaded signed distance dist segmentation crf dist crf dist semantic segmentation initial segmentation regions stereo depth estimates provide improve segmentation define crf pixels bounding box estimate foreground segmentation initial noisy segmentation data term inferred segmentation initial segmentation kitti scene flow dataset segmentations inaccurate object foreground background objects define feature signed distance pixel segmentation function map dist data energy crf model data eseg dist dist signed distance feature crf reduce feature cues improve segmentation spatial crf pixel spatial space eseg figure kitti segmentations signed distance feature dist crf segmentation accuracy img img disp disp current disparity parameters svm error bounding box feature img disp validation perform inference space data eseg eseg efficient tree-reweighted belief propagation pixel inference takes apply crf model scene flow independently estimate segmentation instance estimation scene geometry disparity map camera point cloud scene standard stereo estimation algorithms semantic perform autonomous driving depth estimates vehicle inaccurate depth estimates motion flow model inferred segmentation define crf model pixels semantic segment inferred cascaded pixel camera disparity denote corresponding pixel camera data term defined difference smooth pixel data egeom csad img disp ground ground truth truth optical optical flow flow image image initial point cloud refined point cloud initial initial flow flow initial initial flow flow error error initial disparity error refined disparity error motion motion flow flow motion motion flow flow error error figure point corresponding disparity errors orange initial prsf depth estimates refined depth estimates crf csad csad cost pixels csad difference pixel encourage depth difference pixel space egeom depth depth refined refined flow flow refined refined flow flow error error figure visualization estimated flow fields error orange rigid motion flow object refined estimates crf model improve transforms disparity depth consistency depth disparity objects full crf models inaccurate object stereo crf standard challenging regions foreground objects better capture depth depth distance transforms space data efficient inference egeom egeom takes perform inference 200 200 200 disparity refine disparities frame denote motion second motion rigid motion current flow field estimate energy estimation motion segmentation mask disparity estimates object instance apply rigid motion point cloud image recover optical denote motion flow imperfect geometry estimates motion flow optical flow cues estimation motion flow disparity camera smooth function regularization motion estimation set regularization validation example motion flow map estimation optical flow estimated motion flow previous stage cues optical flow example motion flow errors imperfect flow field corresponding pixel frame pixel define crf model pixels segment frame data csad jpf encourage smooth flow field space image image image image estimated estimated flow flow estimated estimated flow flow error error motion motion flow flow motion motion flow flow error error fused fused flow flow fused fused flow flow error error optical flow methods superpixel inference efficient belief propagation crf distance transforms refined optical flow initial flow motion optical flow perform inference image algorithm full image optical flow algorithms motion estimation pixels produce errors pixels image field optical flow rigid flow fusion crf estimated optical flow motion flow crf optical flow motion flow better estimate flow pixel pixels cost flow pixels motion flow data figure visualization flow fusion crf reduce motion errors orange optical motion flow better apply flow fusion noisy background flow optical flow cascaded scene flow prediction crf models defined refine scene model estimating current estimates approach utilize temporal segmentation geometry initial optical flow motion flow estimates better capture full set geometric temporal multiple stages cascaded prediction refine scene flow stage cascade temporal segmentation consistency frame second stage inferred flow field encourage temporal pixel frame pixel frame time eseg csad jpf spatial space spf parameters validation energy tree-reweighted belief propagation fused flow estimate learning crf parameters space data time eseg eseg eseg segmentations belief spf spf csad jpf stereo stereo input input motion motion input input figure cascaded approach estimation object segmentations disparities rigid motions optical flow stages temporal stages temporal geometric consistency temporal segmentation stereo crf encourage smooth pixels optical flow time egeom depth figure input initial disparity flow refined disparity flow second stages refined flow estimates stage object improved stereo estimates stage disparity pixel second frame rigid motion parameters validation efficient distance solve space data time egeom egeom egeom example improved disparity flow estimates multiple stages recovery optical flow initialization initial noisy optical flow cascade recover motions objects assume motion flow optical updated semantic segmentation rigid motion pixels segmentation mask cost defined bounding box second frame disparities kitti scene flow dataset ground truth disparity second frame disparity apply estimated rigid motion pixel estimated geometry accuracy disparity estimates performance motion estimation global energy function global energy function cascade crfs energy optimization variables cascaded prediction framework energy function accuracy previous cascaded framework large space flow number inference test semantic scene flow algorithm cascaded prediction challenging kitti benchmark evaluate performance disparity estimates frames flow estimates reference scene flow estimates foreground pixels background pixels pixels table difference map cost function estimation cost term standard recover motion table scene flow pixels kitti test evaluation algorithm ssf prsf csf osf frames osftc prsm takes multiple frames better prsf csf osf ssf-p osftc prsm d1-bg d1-fg fl-bg fl-fg time csf prsf osf ssf-p prsm osftc table scene flow pixels kitti test ssf methods two-frame d1-bg d1-fg fl-bg fl-fg table test ssf prsf initialization osf initialization cascaded approach published two-frame scene flow algorithms evaluation ssf-p accurate two-frame prsf ssf-p accurate time published methods perform better proposed method prsm accuracy published two-frame methods provide training data figure table evaluate performance provide stage cascade training improvement stage cascade modeling segmentation geometry independently improvement second stage temporal consistency performance scene flow estimation efficient algorithms accuracy number variables scene flow cascaded algorithm utilize efficient algorithms solve high-dimensional inference time feature challenging semantic segmentation algorithm table kitti validation noisy prsf stage segmentation mask scene flow prediction improved prsf iter iter iter d1-fg d1-bg fl-fg fl-bg vehicle scene flow estimates semantic optical flow methods ground truth segmentation performance table framework cascaded scene flow estimation semantic instance utilize semantic cues rigidly moving produce accurate scene flow estimates cascaded prediction framework efficient recovery high-dimensional motion geometry utilize cues semantic segmentation improve state-of-the-art challenging kitti scene flow benchmark vehicle improve scene flow estimates autonomous cascaded scene flow framework objects stereo stereo input input disparity disparity motion motion input input flow flow disparity disparity input input updated updated disparity disparity flow flow input input updated updated flow flow initial disparity disparity error error initial updated disparity disparity error error updated initial flow flow error error initial updated flow flow error error updated stereo input input stereo disparity disparity motion input input motion flow flow disparity disparity input input updated updated disparity disparity flow flow input input updated updated flow flow initial initial disparity disparity error error updated updated disparity disparity error error initial initial flow flow error error updated updated flow flow error error stereo stereo input input disparity disparity motion motion input input flow flow disparity disparity input input updated updated disparity disparity flow flow input input updated updated flow flow initial initial disparity disparity error error updated updated disparity disparity error error initial initial flow flow error error updated updated flow flow error error stereo stereo input input disparity disparity motion motion input input flow flow disparity disparity input input updated updated disparity disparity flow flow input input updated updated flow flow initial disparity disparity error error initial updated disparity disparity error error updated initial flow flow error error initial updated flow flow error error updated figure visualization semantic scene flow estimates kitti training set imperfect segmentation leads large disparity flow number work semantic optical evaluation optical full optical flow estimation global optimization semantic segmentation learning optical flow convolutional efficient belief propagation training structural inference dense motion capture image geometry autonomous kitti vision benchmark cascaded classification models scene method scene flow estimation stereo optical flow semantic training structural tree-reweighted energy conditional random models optimization optical flow optimization approach efficient accurate scene large dataset convolutional networks optical scene flow object scene flow autonomous optical flow motion field estimation object scene flow temporal vision image model motion segmentation moving camera optical evaluation dense two-frame stereo optical flow semantic segmentation optical flow estimation scene evaluation data optical piecewise rigid scene scene flow estimation piecewise rigid scene map estimation efficient dense scene flow dense stereo efficient stereo flow transforms instance-level segmentation autonomous driving
layered rgbd scene flow estimation rgb image depth segmentation motion figure layered approach handle multiple moving objects estimate motion occlusion key observation depth depth ordering computational bottleneck rgb layered methods figure detected occlusions estimated motion semi-rigid scene flow method depth scene flow rgbd sequences depth allows motion single depth boundaries rgb image cues motion methods optical flow formulation large errors occlusion better depth occlusion propose layered rgbd scene flow method jointly scene segmentation key observation noisy depth sufficient decide depth ordering computational bottleneck rgb layered depth estimate rigid motion motion middlebury real-world sequences layered approach rgbd scene flow vision estimation intrinsic image object detection depth cues allow scene flow estimation depth cues depth rgbd scene flow figure depth boundaries rgb image motion boundaries depth missing occlusion occlusions errors optical flow formulation rgbd layered models promising approach model occlusions rgb image sequences scene moving layers occlusions motion foreground rgb layered methods promising computationally bottleneck depth ordering requires propose depth rgbd data solve depth ordering problem layered work based observation depth depth figure depth allows estimate rotation translation moving rigid motion motion scene computer vision scene flow estimation problem requires rgb jointly solve stereo depth depth ieee depth image figure region figure depth noisy missing depth boundaries object boundaries image observation depth sufficient decide depth ordering foreground depth solve energy avoid global depth ordering local approach occlusions locally occlusion local image local occlusion depth local depth rgbd observation layered representation rgbd scene flow rgbd data depth common vision single rgbd random single depth depth intrinsic image song depth avoid object detection rgb variations function scene flow scene flow estimation problem compute scene flow optical scene locally moving benchmark method moving rgbd scene flow propose rgbd scene flow method data optical flow formulation horn depth noisy depth spatial allow motion boundaries depth hadfield estimate motion perform dense motion local global rgbd scene flow propose representation rotation translation capture global rigid motion scene local method single moving method captures global motion moving object handle multiple moving estimate scene flow perform motion motion estimation method model errors estimated motion segmentation prior motion random field evaluate method middlebury motion estimation errors semi-rigid scene flow method sequences method obtains better scenes multiple moving objects large atlab code work optical flow layered rgbd data scene flow work optical flow field optical flow estimation methods variational horn motion recover motion boundaries local objects constraint scenes large regions occlusions occlusions constancy optical rgb layered layered approach motion rgb layered methods promising optical flow computationally bottleneck depth ordering requires layer segmentation approach rgb image sequences jointly solve motion segmentation avoid error utk vtk gtk model sequence depth scene moving layers estimate motion assume scene independently moving depth model layer support functions support function layer frame gtk pixel layer frame layer background support motion layer motion image vtk depth wtk solve layer support functions motion image term describes depends current layer term describes motion depends layer support term describes layer support term describes layer support depends solve layer support motion energy term energy function prior term layer layer capture spatial coherence layer support function random field color utk vtk corresponding pixel frame motion field layer current frame vtk layer layer support functions stk gtk stk pixel pixels layers background locally temporal coherence term layer support motion field layer layers motion capture global motion assume pixels layer common rotation rtk translation pixel depth position camera horizontal rotation rtk translation position rtk corresponding position scene flow rtk vtk wtk gtk gtk rigid motion field vtk wtk motion model allow motion layer model horizontal motion rtk spatial pixel constant large color variations temporal coherence utk utk utk utk vtk depth vtk wtk motion rtk rtk rtk utk vtk support robust penalty assume horizontal depth energy functions data constancy occlusion occlusions pixel layer layer current frame stk corresponding pixel frame utk vtk stk pixel constant occlusion constancy image pixel data constancy term vtk estimate global rotation translation corresponding method rgb layered depth ordering layers computationally requires depth energy function motion depth ordering energy rgbd layered depth depth ordering perform energy evaluate method middlebury benchmark real-world intrinsic camera image depth depth reference code work scheme missing depth optical flow method middlebury 2002 test methods middlebury stereo 2002 dataset rgbd scene flow evaluate optical table method semi-rigid scene flow method benchmark rgb layered model rgbd methods figure estimated flow proposed high depth allows method recover motion middlebury 2005 test methods middlebury 2005 sequences 2002 table method error srsf warped image stk stk robust penalty constant penalty note data term layer depends support functions rgbd data constraint variations depth pixels valid depth depth constraint vtk wtk stk stk wtk robust penalty function constant penalty perform energy function support compute occlusion regions based scheme motion function function jointly support functions depth average depth layers decide depth compute optical flow rgb optical flow pixels corresponding pixels frame second frame estimated motion segmentation warped image figure estimated optical flow middlebury motion color scheme segmentation scheme black pixels warped image detected frame second frame estimated motion segmentation warped image figure estimated optical flow middlebury 2005 detected occlusions black warped table average root squared error average error middlebury dataset method rgb rms aae rms aae proposed srsf hadfield quiroga depth table average root squared error average error middlebury 2005 dataset method rgb quiroga proposed rms aae rms aae average image occlusion figure srsf motion field middlebury data single global camera data evaluate rgbd scene flow test method real-world figure sequences method obtains srsf note srsf pixels depth valid output srsf reasonable second figure foreground background srsf table average root squared error middlebury dataset performance single camera translation layers layers pixels recover prior foreground pixels method motion motion figure rgbd tracking test methods sequences rgbd tracking dataset sequences independently moving objects large depth image well figure sequence large regions occlusions errors srsf srsf produces errors single rotation translation moving layered approach obtains reasonable occlusions match image method errors region prior model layer support red sequences occlusion reasonable match scene noisy depth sufficient decide depth layer table performance layers performance layers motion camera layer pixels estimate rotation translation red sequence models objects produces errors figure model scene motion sequence layers atlab code key observation noisy depth cues layer ordering handle occlusions layered approach high computational depth allows estimate common rigid motion layered method obtains middlebury benchmark real-world independently moving objects large work promising depth data motion atlab code work quiroga neural layered representation motion robust estimation mixture models ieee international conference computer optical intrinsic scene single ieee conference computer vision pattern scene flow variational random high optical flow estimation based european conference computer large optical variational motion ieee optical flow european conference computer layered rgbd scene flow estimation method motion estimation frame depth srsf motion figure sequences srsf output black region method obtains motion foreground reasonable estimate background method obtains prior foreground regions method better captures motion second rgb image second depth motion srsf occlusions figure depth segmentation occlusion figure note depth ordering computational bottleneck layered robust estimation layers ieee vision benchmark ieee conference computer vision pattern motion estimation segmentation depth flow ieee conference computer vision pattern hadfield scene based scene flow ieee dense motion estimation color ieee international conference frame second frame srsf motion occlusions depth second depth motion segmentation figure valid output note detection occlusion match well image frame second frame srsf motion occlusions depth second depth motion segmentation motion segmentation motion segmentation figure red method produces promising occlusion note method well captures global motion method produces large moving horn optical reference reference european conference computer mixture models optical flow ieee conference computer vision pattern layered motion optical flow ieee conference computer vision pattern volume local dense semi-rigid scene flow estimation rgbd european conference computer scene flow international conference image stereo depth ieee conference computer vision pattern volume single depth ieee conference computer vision pattern segmentation support rgbd european conference computer song tracking rgbd benchmark ieee international conference computer song object detection depth european conference computer segmentation scenes neural local motion estimation occlusion ieee conference computer vision pattern optical flow estimation ieee conference computer vision pattern current optical flow estimation layered image motion temporal depth neural layered segmentation optical flow estimation ieee conference computer vision pattern layered model foreground background ieee conference computer vision pattern scene ieee international conference computer volume rigid scene ieee international conference computer moving ieee large optical flow ieee international conference computer motion segmentation mixture ieee conference computer vision pattern motion estimation ieee conference computer vision pattern optical motion optical flow ieee robust flow ieee conference computer vision pattern
fast multi-frame stereo scene flow motion segmentation microsoft university multi-frame method efficiently computing scene flow depth optical camera ego-motion dynamic scene moving stereo camera technique segments moving objects rigid estimate disparity map 6-dof camera motion stereo matching visual regions estimated camera motion compute optical flow flow proposal fused camera flow proposal fusion final optical flow motion unified framework benefits tasks optical visual odometry motion segmentation higher accuracy method currently ranked kitti 2015 scene flow implementation frame faster top evaluation challenging sintel sequences fast camera object method outperforms osf currently ranked second kitti left input frame stereo frames ground truth disparity estimated disparity ground truth flow estimated flow ground truth segmentation figure method estimates dense disparity optical flow stereo stereoscopic scene flow camera motion simultaneously moving objects explicitly estimated segmentation scene flow flow dense motion field scene estimated video cameras multiple rgb-d video scene flow estimation tasks computer vision stereo matching optical flow existing methods solve tasks independently stereo optical flow methods computing scene flow exploit tasks additional scene work microsoft university well optical flow consecutive image pairs stationary points 6-dof motion camera existing scene flow additional simultaneously estimating camera motion moving objects stereoscopic scene flow estimation led improved accuracy challenging better optimization well better optimization methods methods computationally practical existing scene flow methods process input flow binocular stereo visual odometry epipolar stereo rigid flow flow fusion disparity disparity non-rigid flow flow figure proposed three estimate disparity camera motion stereo matching visual odometry detect moving object regions rigid flow frig computed optical flow non-rigid flow fnon fused frig final flow segmentation consecutive frames independently efficiently technique estimate scene flow multi-frame calibrated stereo camera moving simultaneously compute dense disparity optical flow maps 6-dof relative camera pose consecutive frames estimated binary mask pixels correspond rigid non-rigid independently moving objects algorithm real-time exploit dynamic pixels correspond static rigid disparity maps estimated stereo robustly compute 6-dof camera motion visual odometry outliers objects ego-motion improve depth estimates occluded pixels epipolar stereo image regions camera motion compute optical flow proposal flow proposal fused camera flow proposal fusion final flow map motion tasks optical visual odometry motion segmentation existing methods solve tasks unified framework solution task benefits joint methods problem optimization problems computational method faster top methods kitti frame methods minutes method faster explicitly camera motion motion unified framework benefits individual optical depth camera flow rigid points rigid flow efficiently compared non-rigid compute non-rigid flow pixels moving corresponding points consecutive rigid flow computed depth camera motion estimates rigid method recover accurate pixels stereo computing camera motions consecutive frames multi-view stereo matching adjacent stereo frames current frame visual motion segmentation camera motion recovery binary mask previous frame pixels current frame outliers downweighted visual odometry motion task essentially final optimization frame rigid non-rigid optical flow proposals mrf fusion binary labeling pixels non-rigid work work task estimating scene flow image sequences variational problem problems optimization methods proposed solution based proposed occlusion large image segmentation recover accurate motion depth multi-view scene problem binocular stereo proposed variational method proposed approach uncertainty proposed efficient variational method scene flow recovery disparity optical flow proposed variational method stereo cameras scene flow evaluated sequences static cameras cameras moving scenes proposed method scene flow scenes moving objects moving stereo kitti benchmark led proposed model dense depth motion proposed piecewise rigid scene model multi-frame settings scenes planar segments rigid prsm current top method joint estimation rigid motions segmentation optimization computationally proposed approach prsm continuous optimization fixed faster practical approach object scene flow segments scenes multiple objects based fixed object set planar model inference computationally minutes faster minutes work scene flow estimation rgb-d sequences based methods camera pose p01 left assume input stereo image pairs size image pixel disparity flow segmentation defined image relative camera motion disparity map source pixels stationary source image points target image rigid function warping pixels source image target image based proposed method input image sequences left cameras calibrated stereo process frames estimate disparity maps flow maps camera motions motion segmentation left moving stationary objects foreground processing frame method image residuals estimating parameters warping function fnon method define relative camera motion images camera points camera source image points target assume calibrated stereo cameras camera warping function flow map binary segmentation reference image frig background fnon foreground frig rigid flow computed disparity map camera motion fnon non-rigid flow defined directly estimating full model computationally rigid motion model computed initial disparity map uncertainty map map consistency uncertainty map computed sgm computational define considered fixed confidence threshold unreliable details supplementary stereo visual odometry occlusion map final disparity map current image disparity map estimate relative camera motion current method existing stereo visual odometry method direct estimates 6-dof camera motion directly minimizing image residuals figure binocular epipolar initial disparity map pixels occlusion map pixels final disparity estimate epipolar model parameters motion model non-rigid motion regions flow fnon directly pixel steps robustly evaluate image residuals truncated normalized cross-correlation tncc ncc normalized cross-correlation computed image centered thresholding set proposed estimate initial disparity disparity map camera motion visual odometry recovery motion estimate epipolar stereo matching improve initial disparity final disparity map estimates compute rigid flow proposal frig recover initial segmentation estimate non-rigid flow proposal fnon moving rigid object regions non-rigid flow proposals fnon final flow map segmentation steps proposed method target pixels rigid warping outliers moving residuals function energy framework modified method exploit motion segmentation weights set based occlusion map downweighted moving object previous mask flow direct methods multiple multiple final estimate minimizes weighted ncc-based residuals tncc previous motion motion estimate forward motions epipolar stereo refinement initial disparity map current stereo pixels occluded multi-view epipolar stereo technique adjacent images final disparity map binocular stereo matching cost volume disparity range dmax epi better cost volume input blending matching costs target images binocular stereo left images estimate left image initial disparity map map uncertainty map estimates standard estimate disparity maps semi-global matching fixed disparity range dmax implementation sgm directions ncc-based matching costs data occlusion relative camera current frame previous current frame estimated visual odometry relative target image estimated p01 p01 recall p01 camera target image compute matching costs points corresponding points pose rigid unreliable moving thresholding ncc higher cost avr matching costs occluded pixels epi compute improved cost volume avr blending epi avr ncc-based map var prior flow fpri depth edge map wpq dep str image edge map wpq initial segmentation epi sgm final disparity map blending weights computed uncertainty map normalized figure initial detect moving object regions image residuals weighted prior depth edge image edge initial compute ncc-based matching costs tncc ncc ncc confidence epi increases increases avr compute initial segmentation initial segmentation reference image binary segmentation rigid flow proposal frig optical flow recall frig estimated disparity map motion define binary correspond foreground segmentation energy eseg defined ncc flo col pri eseg epotts frig ncc tncc values unreliable regions pixels epipolar compute average matching costs ncc ncc var ncc terms data terms values mask epotts pairwise smoothness explain term term term moving objects image residuals matching images increases recall moving improve matching var weighted truncated standard centered var weight map ncc occluded ncc ncc flow term term flow residuals fpri rigid flow prior flow fpri computed var threshold weight flo define flo flo var flo flo flo var flo flo threshold computed pixel flo flo threshold rigid motion frig prior flow fpri bi-directional flo consistency check holes set flo flo prior term term segmentation based previous frames scene ground plane color term standard term color vectors pixels reference image col col non-rigid flow sgm flow consistency check non-rigid flow proposal fnon rigid flow proposal frig col bins histograms color models smoothness term epotts term based potts model defined pairs neighboring pixels pixel col dep str epotts potts figure optical flow flow non-rigid flow proposal sgm consistency filtering weighted median flow proposal fused rigid flow proposal final flow estimate motion final flow map final segmentation mask three edge col col weight computed estimated dep weight computed dep disparity map estimated str edge-based weight edge map fast edge dep str computed edge maps str wpq potts eseg minimizing eseg graph cuts updating color models col segmentation dynamic flow fusion final segmentation rigid non-rigid flow proposals frig fnon final flow estimate fusion step final segmentation fusion process initial binary final segmentation flow proposals fnon final flow estimate energy eseg ncc var ncc tncc optical flow estimate non-rigid flow proposal fnon moving foreground regions estimated initial full flow pose optical flow discrete labeling problem labels range range sgm algorithm discrete flow map sgm filter bi-directional consistency check holes weighted median filtering non-rigid flow proposal fnon flow consistency map sgm supplementary material well refinement rig frig fnon prior flow fpri fnon rig view fnon ncc flo flow occlusion map set fusion step pixels background ground initial segmentation labels graph optimization fusion pixels foreground small implementation details disparity range epipolar disparity range dmax estimating dmax estimated compute dmax robustly histograms bins dmax flow range range sgm flow estimated compute three target region ranges prior flow rigid robustly compute ranges histograms flow vectors bins final range range range estimation sgm individual ncc cost ncc flo segmentation fusion maps steps values segments pri pri segmentation define mask mask mask mask previous mask flow background regions downweighted better mask color term col color models average color ground prior ground plane supplementary material kitti 2015 scene flow benchmark kitti benchmark table method ranked method faster top methods accurate fast methods running method small optical flow size moving motion segmentation accurate table epipolar stereo refinement adjacent stereo frames disparity accuracy visual images camera motion ego-motion evaluation sintel previous scene flow evaluated method sintel compared osf prsm table prsm motion osf prsm accurate method outperforms osf sintel method sintel scenes camera non-rigid object motion kitti moving osf prsm rigid inference piecewise planar method easily parameters modified col ncc visual odometry step scene mountain moving objects field motion segmentation accurate improve temporal consistency motion figure segmentation ground scenes compute ground prior disparity map algorithm images factor optical flow steps image kitti refinement sgm disparity flow maps standard evaluate method kitti 2015 scene flow benchmark evaluate challenging sintel sintel top methods prsm osf prsm multi-frame method osf explicitly moving objects static background rigid motion object background method running times computer parameter settings kitti training data parameters proposed efficient scene flow method dense optical visual motion segmentation optimization methods unified framework led higher accuracy method currently ranked kitti 2015 scene flow benchmark prsm osf faster top challenging sintel method outperforms osf prsm terms efficient method prsm improve practical scene table kitti 2015 scene flow benchmark error rates disparity reference frame second frame optical flow scene flow background foreground disparity flow considered estimated error scene flow considered method prsm osf d1-bg d1-fg d1-all sf-all min reference image motion segmentation disparity map disparity error map figure kitti sequences pixels error maps ground table disparity epipolar pixels d1-fg pixels d1-bg d1-fg d1-all flow map flow error map binocular epipolar d1-bg d1-all table sintel evaluation error rates disparity flow scene flow motion segmentation osf prsm relative method osf average d1-all osf prsm osf prsm sf-all osf prsm osf flow fusion running frame epipolar stereo visual odometry binocular stereo prior figure running times sequences average running edge extraction osf prsm osf prsm osf osf prsm osf prsm osf mountain osf prsm osf prsm osf reference images motion segmentation disparity maps flow maps figure mountain motion segmentation osf ground disparity flow maps estimated prsm osf ground truth direct disparity real-time visual robotics journal computer vision multi-view scene flow view centered variational journal computer vision boykov energy ieee pattern boykov interactive graph cuts boundary region segmentation objects computer vision volume source optical flow european computer vision scene flow estimation correspondence ieee computer vision pattern recognition full optical flow estimation optimization ieee computer vision pattern recognition approach real-time optical flow computation conference pattern fast edge ieee pattern semi-global terms message pattern motion estimation based conference image multiple view computer university second rgb-d dense motion estimation color ieee robotics stereo processing matching ieee pattern optical scene flow rgb-d ieee computer vision pattern recognition variational method scene flow estimation stereo computer vision framework real-time dense rgb-d scene ieee robotics motion rigid scene flow rgb-d conference vision dynamic graph cuts efficient inference ieee pattern message energy ieee pattern computing visual correspondence graph computer vision volume accurate solution journal computer vision scene flow binocular stereo computer vision image image technique stereo joint continuous optimization approach efficient accurate scene european computer vision large optical scene flow ieee computer vision pattern recognition object scene flow ieee computer vision pattern recognition multi-view stereo scene flow estimation matching journal computer vision variational scene flow estimation ieee computer vision pattern recognition dense scene flow estimation european computer vision interactive foreground extraction graph graph based continuous stereo matching ieee computer vision pattern recognition joint estimation stereo european computer vision european computer vision scene computer vision volume scene ieee pattern scene flow estimation multiple european computer vision scene flow estimation rigid motion computer vision piecewise rigid scene computer vision scene flow estimation piecewise rigid scene journal computer vision occlusion large improved rgb-d scene flow ieee video stereoscopic scene flow computation motion journal computer vision motion optical flow ieee pattern scene flow ieee computer vision pattern recognition volume fast multi-frame stereo scene flow motion segmentation supplementary material microsoft university supplementary material details sgm stereo flow well segmentation ground prior main parameter settings effects sgm algorithm proposed algorithm main additional methods supplementary sgm stereo binocular epipolar stereo solve stereo matching problems semi-global matching algorithm stereo matching discrete labeling estimate disparity map dmax disparity minimizes field based energy data term pixel left image corresponding pixel disparity image vpq pairwise smoothness term defined neighboring pixel pairs pixel term defined vpq smoothness inference sgm mrf directions minimizes dynamic updating cost directions image boundary min vpq min normalized scan-line costs min updating min vpq scan-line costs directions disparity estimate pixel sgm algorithm message factor data term sgm proposed uncertainty computed min min individual scan-line costs second term computed computation essentially computational implementation data term defined truncated normalized cross-correlation main smoothness defined sgm col wpq col wpq color edge-based weight parameters sgm disparity range fixed dmax image size kitti images factor disparity range set confidence threshold uncertainty map sgm flow sgm algorithm optical flow problem estimate flow map flow minimizing mrf sgm ncc-based matching cost data term evaluate matching define smoothness term flow second vectors smoothness parameter settings sgm optimization essentially sgm implementation updating scan-line costs pairwise term vpq modified flow vpq labels neighboring flow refinement flow maps optical flow flow maps consistency check weighted median stereo optical flow methods explain estimate forward flow map sgm foreground pixels initial compute mask image flow mask estimate flow map foreground flow map filter outliers bi-directional consistency check flow map holes background rigid flow frig weighted median filtering pixels median filtering foreground pixels non-rigid flow estimate final weighted median filtering filter dpq computed geodesic distance dpq forward flow map flow map consistency check background weighted median filtering figure process flow map disparity map image sf-all error sf-all error figure scene flow reference parameters ncc col error rates evaluated training sequences parameter settings disparity map define distance adjacent pixels geodesic distance dpq computed pixels filter window distance pixel efficiently computed algorithm filter window size median filtering outliers window segmentation ground prior segmentation ground prior term computed detect ground plane disparity map disparity plane defined image assume cameras stereo rig disparity compute disparity residuals ground plane plane ground prior background defined gro gro gro gro increases gro thresholding set dmax gro parameter settings explain parameters effects parameters easily method insensitive parameter effects threshold uncertainty map threshold var str weight image edge-based weight easily direct figure parameters sgm tuned independently weights ncc flo col potts tuned ncc flo potts small ranges ncc term flow term tuned col color term potts col previous work ncc col insensitive kitti image effects parameters kitti training sequences figure threshold ncc ncc-based matching costs sintel images image compared images weight col weight prior color term sintel moving objects moving frames stationary col temporal motion segmentation improve prior color improve temporal consistency motion segmentation col insensitive boykov interactive graph cuts boundary region segmentation objects computer vision volume semi-global terms message pattern stereo processing matching ieee pattern fast filtering visual correspondence ieee pattern stereo stereo matching vision interactive foreground extraction graph optical flow estimation ieee computer vision pattern recognition geodesic distance pattern map estimation ieee times faster weighted median filter ieee computer vision pattern recognition
estimation natural human action recognition simon hadfield degree university surrey centre speech signal processing university surrey surrey 2013 simon hadfield 2013 estimation encoding techniques applicable range vision task natural action favours algorithms generalisation constraints leads intra-class including changes actor viewpoint action algorithms perform well generally well real applications video issues mitigated utilising invariance appearance based remove projective camera exploitation properties feasible microsoft kinecttm growth broadcast footage evaluate provide benchmark future large multi-view action dataset covering action classes comprising high definition data broadcast range staged capture large number existing action recognition techniques extensions allow encoding structural leads improved standard spatiotemporal additional estimation motion fields motion estimation referred contrast image plane previous work scene flow real computational complexity approaches proposed previous state art techniques generally require estimate motion single datasets reasonable lead field scene flow estimation interest monte-carlo based approach motion estimation orders magnitude faster gpu improved accuracy over-smoothing particle based approach underlying assumptions motion estimation behaviour systems optical flow scene flow estimation existing formulations demonstrated require accurate small motions multiscale estimation explores idea functions accurately represent real suitable detection estimation leads non-linear estimation based machine transfer incorporated existing motion estimation schemes single support probabilistic occlusion multi-hypothesis motion smoothing fast accurate approach motion exploited original task natural action range schemes effectively encoding rich based variations popular histogram oriented gradients histogram oriented flow utilising actors undistorted motion action recognition rates improved standard spatiotemporal structural serves demonstrate tractable conjunction growth scene flow estimation valuable computer vision scene motion action interest hand sign scene occlusion probabilistic bilateral histogram motion motion segmentation richard contents thesis ability find addition university contributions andrew provided support years allowed point real face contents nomenclature symbols introduction literature review action recognition interest point detection local feature descriptors sequence encoding motion estimation sparse scene flow estimation dense scene flow estimation action recognition structure action data collection action data automatic extraction manual extraction depth data generation holistic action recognition structure interest point detection interest point detection contents feature descriptors experiments interest point evaluation feature evaluation interest point threshold evaluation detection modes conclusions particle based motion estimation motion field estimation probabilistic scene flow appearance likelihood depth likelihood scene particles ray resampling multiscale iterative estimation iterative diffusion interpretation scene flow evaluation comparison ray resampling analysis sampling density search space analysis iteration analysis propagation robustness noise object tracking application object extraction trajectory estimation hand tracking sign language hand tracking evaluation conclusions contents revisiting motion estimation assumptions smoothed scene particles patch based scene particles image plane smoothing smoothing smoothing evaluation forwards scene particles 104 occlusion aware scene particles 104 assumption variant analysis non-constant velocity improving brightness constancy 113 matching functions function behaviour analysis 116 intelligent transfer functions motion estimation intelligent transfer functions conclusions 131 133 motion features histograms oriented optical flow 133 histograms oriented scene flow undistorted hos 136 independent encoding histogram scene-flow evaluation qualitative action recognition conclusions discussion future work contents pose recognition structure 157 texture features 157 pose recognition framework 159 particle filtering 159 pose data collection discrete head pose 162 semantic hand classes 162 pose recognition evaluation 163 hand pose classification 164 head orientation 166 pose tracking framework 169 interactive system 170 additional motion function bibliography 173 nomenclature lbp human computer local binary pattern parallel tracking multiple mapping rdf hmm slam hog hof hos nrms sift decision forest hidden markov model random field dynamic time warping simultaneous location modelling histogram oriented gradients histogram oriented flow histogram oriented optical flow histogram oriented scene-flow histogram oriented depth gradients bag false rate false rate active appearance model analysis normalised root square average angular error scale-invariant feature speeded robust feature nomenclature sfm sfe svm rmd itf structure motion markov random field scene flow estimation degree active shape model multiple instance learning sign language recognition gaussian model support vector machine kalman filter particle optimisation cross bilateral grid relative motion descriptor bag visual sum squared differences sum absolute differences function probability density function filter forward smoothing intelligent transfer function symbols introduced chapter input observations observations appearance scale observations depth scale dimension spatial position image spatial position image temporal position sequence temporal position sequence number image scales image scales variance gaussian gaussian gaussian function variance output gaussian input variance second matrix volume hessian matrix volume estimating harris corners set harris interest points set hessian interest points set harris interest points e-4d f4d-ha f4d-he symbols f3d-ha f3d-s 4d-ha 4d-he 3d-ha 3d-s godd geve int rmd rmd-4d cont set hessian interest points set separable filter points set harris interest points set separable filter points threshold harris interest points threshold hessian interest points threshold harris interest points threshold hessian interest points threshold separable filter points threshold harris interest points threshold separable filter points weighting depth versus structure interest point estimation half quadrature pair gabor filters half quadrature pair gabor filters number saliency content comparisons response volume harris operator response volume separable filter operator integral saliency volume created interest point detection strengths response volume rmd operator integral saliency created interest point detection strengths depth stream response volume rmd-4d operator dimensional spatial offset dimensions dimensions saliency content spatio-temporal region origin cont-4d saliency content spatio-temporal region origin symbols single descriptor bag visual approach joint values observations local region single descriptor bag visual approach introduced chapter t-1 t-1 spatial position co-ordinates velocity co-ordinates velocity image plane disparity points projection sensors plane velocity terms disparity change frames structure point vector structure point frame motion scene probability distribution prior scene probability scene likelihood scene appearance likelihood scene depth likelihood scene likelihood scene transition model number appearance sensors appearance sensor number depth sensors depth sensor general sensor depth appearance sensor projection functions depth sensor projection functions symbols depth sensor projection functions frame appearance sensor scale frame frame depth sensor scale observed colour point kurtosis parameter measurement model appearance data kurtosis parameter measurement model depth data set scene particles cardinality scene particle set set scene particles time weight scene particle normalised weight scene particle subset scene particles ray pixel sensor number ray resampling subsets particle population set values visible scene values scene diagonal dimension scaled range values scene dimensional offset parameter diffusion output image generated scene particle channels encoding pixel ground truth channels encoding true pixel errof error measure image plane flow magnitude errsf error measure plane flow magnitude errae error measure flow directional symbols errst error measure structure reconstruction normalised error measure image plane flow magnitude errof errsf errae errst normalised error measure plane flow magnitude normalised error measure flow directional normalised error measure structure reconstruction skin region likelihood psk pbg skin probability colour background probability colour introduced chapter co-ordinates sensor patch formed performing pixelwise averaging number patches unit vector ray sensor distance ray prior probability velocity dimensions dimensional pixel offset set pixel offsets dimensional pixel offset comprising local image region variance spatial weighting function bilateral filtering variance weighting function bilateral filtering variance spatial weighting function bilateral filtering variance weighting function bilateral filtering visibility probability point sensor time average visibility probability point sensors frames smoothed version smoothed set scene particles scene particle smoothed set smoothed velocities co-ordinates symbols set offsets dimensional spatial offset comprising local neighbourhood co-ordinates set supporting pixel locations terms set supporting pixel appearance values supporting pixel set supporting pixel gradient values appearance patch sensor abs sum absolute differences matching metric sum square differences matching metric abs abs applied gradient images embody gradient constancy sqg applied gradient images embody gradient constancy flow matching metric fdif fvar fpix fpm fpv fph set pixel difference features set pixel variance features set raw pixel features set patch features set patch variance features set patch histogram features number trees rdf classifier depth trees rdf classifier introduced chapter number spatial neighbourhood split calculating histogram features set offsets cell histogram cell local created concatenating cell histograms bin histogram number orientation bins cell histogram symbols azi ele mag ori number azimuth orientation bins cell histogram number elevation orientation bins cell histogram flow magnitude image flow orientation image mag-p scene flow magnitude created projected scene flow image azi-p scene flow azimuth created projected scene flow image ele-p scene flow elevation created projected scene flow image projected scene flow histogram cell weighted average scene ray sensor mag-3d scene flow magnitude created weighted average scene particle azi-3d scene flow azimuth created weighted average scene particle scene flow elevation created weighted average scene particle scene flow histogram cell 3d-azi histogram scene flow cell histogram scene flow cell 3d-ind scene flow encoded azimuth elevation cell scene flow histogram cell single elevation bin optical 3d-int scene flow cell itf symbols list figures generally applicability kinect staged actions estimated scene flow field histogram scene-flow example example frames hollywood dataset traditional action recognition including depth rmd saliency content comparison precision-recall curves saliency threshold behaviour analysis modes scene flow estimation task comparison brightness constancy functions flow diagram scene particle algorithm example scene flow estimation middlebury examples types scene flow error measure performance runtime search volume performance scales performance diffusion loops performance sequence length performance noise robustness trajectory estimation diagram list figures hand tracking example smoothed flow field examples 100 spatial smoothing performance smoothing performance occlusion rays example occlusion probability example 108 constant velocity violation performance 112 example frame cones sequence 116 function response images response distributions convergence performance intelligent transfer function response distributions contextual itf response distributions 127 itf convergence performance azimuth elevation bins spherical space 136 scene flow hos encoding kinect hos encoding hollywood eat example motion feature performance hos encoding hollywood drive example likelihood tracking hypotheses pose space 161 combining multiple depth maps 161 head hand detection segmentation 163 hand pose examples 164 head pose examples head pose confusion matrices 170 interactive hand pose response distributions response distributions response distributions list figures response distributions itf convergence performance venus itf convergence performance laundry itf convergence performance wood1 180 itf convergence performance rocks1 181 list figures list performance automatic action extraction train test split films number train test clips action dataset interest point detector performance descriptor saliency performance scene flow estimation performance range algorithms ray resampling performance hand tracking performance smoothed scene particle performance scene particle performance constant velocity occlusions single view motion estimation performance itfs multi view motion estimation performance itfs 131 motion feature class performance hand pose classification performance 165 hand classification confusion matrix 165 head pose performance continuous head pose estimation 169 list work presented thesis simon hadfield richard scene particle based scene flow pattern analysis machine simon hadfield richard hollywood recognizing actions natural conference computer vision pattern recognition simon hadfield richard hand trajectories clustered scene international conference image analysis simon hadfield richard particle based scene flow depth international conference computer simon hadfield richard pose estimation international workshop september chapter introduction work presented thesis motivated rise video efficient order data fully exploited computer vision focus estimation structure motion recognition human greatest challenge application area level generalisation features presented mitigate invariance number common sources changes lighting viewpoint differences actor algorithms capable operating combination appearance sensors depth including appearance employing multiple depth camera providing calibration techniques restricted narrow baseline initial chapter highlights aims associated thesis structure contributions remaining human action recognition applicable areas computer human computer techniques generation natural employed well chapter introduction surveillance applications outdoor vast video tasks applied unconstrained data addition providing largest application tasks involve greatest levels action performed semantic attempting vision techniques order solve unique rarely considered lack control favours highly invariant complex features lead restricted microsoft kinecttm work computer vision living operate unconstrained structural employed simple features based depth comparisons provided discussed simplifying segmentation objects detection work presented thesis general extracted invariant data application unconstrained natural producing unconstrained referred datasets action recognition examples figures compared datasets previous structural ability structural work presented thesis newly captured dataset community support future chapter discusses current state fields action recognition motion order introduced remainder position thesis previous work chapter explores structural recognising unconstrained kinect living output depth map depth comparison features figure generally applicability depth based features allows operation extremely varied living images initially collection dataset including automatic manual existing state art techniques explained proposed extensions enable encoding structural proposed extensions include interest point detection feature analysis proposed approaches lead improved performance simple appearance based techniques chapter improvement motion features chapter incorporating structural estimation motion fields explored scene flow contrast optical flow figure example estimated motion existing scene flow estimation techniques data chapter introduction kth running example hollywood2 example kth running example hollywood2 example kth running example hollywood2 example figure staged examples class staged kth dataset hollywood2 dataset oversmoothing artefacts techniques reduce accuracy regions tend novel probabilistic approach scene flow estimation inspired monte-carlo estimation particle approach capable maintaining multiple motion hypotheses smoothness constraints accumulate time resolve discussed techniques formulated operate combination appearance depth sensors depth sensors algorithm figure estimated motion field sequence performing action motion move red forms simultaneous motion structure speed flexibility vast applications exploit motion feature descriptor action example applications hand trajectories extracted frames sign language clustering estimated motion chapter improves scene flow estimation assumptions underlying motion techniques explored include smoothness maintaining properties inclusion constant velocity probabilistic handling finally analysis fundamental brightness analysis light number issues motion estimation explains number including multi-scale iterative motion robust formulation based machine incorporated motion estimation excellent chapter feature descriptor developed encode estimated motion figure descriptor inspired work based motion descriptor incorporated action chapter introduction figure hos descriptor example sequence local motions encoded orientation length relating strength motion images relate levels plane motion left involves motions bottom motions images green squares normalised recognition systems proposed chapter inclusion motion features improve recognition image plane motion chapter contributions structure motion unconstrained recognition future work including improvements proposed action recognition application scene flow estimation areas computer contributions work presented thesis publicly dataset unconstrained action including structural publicly baseline interest point detection employing structure determining feature descriptors based current state art action capable encoding structural extremely fast accurate approach motion over-smoothing multi-modal framework application motion estimation combination appearance depth approach estimating occlusion maps motion robust brightness constancy formulation based machine automatic tracking based estimated motion general feature encoding motion chapter introduction chapter literature review chapter discusses current state art range general techniques human action recognition interest point local feature encoding holistic addition existing work simultaneous estimation structure motion sparse dense action recognition state art approaches action recognition follow general consisting sequence sampled number spatio-temporal second local feature descriptors extracted sample third collection local features encoded single holistic descriptor finally sequence descriptor discriminative order approach highly successful techniques object recognition additional temporal simplest approach sampling dense sampling scheme sampling spatio-temporal case samples lie background objects involved reduce chapter literature review authors extend dense sampling uniform grid features calculated integration entire spatio-temporal contributions point weighted features extracted background regions provide context densely sampled incorporate action context outperform sparse features higher authors idea step attempt model context objects object parts create additional feature based number configuration perform separate scene classification prior knowledge probable action example action occur improved recognition dense sampling sparse sampling schemes large scale action recognition sparse sampling relies detection salient regions serves reduce assuming correlation interest point detector object performing interest point detection literature wide variety interest point detection based detectors common corner detector introduced harris extremely fast detector local image gradients positions strong intensity changes distinct points relate edge directions restricted lie corners considered salient edges solve aperture allowing originally designed detection interest points approach applied temporal examining action recognition harris detector extended laptev operate directly space-time volumes detected interest points strong intensity gradients spatio-temporal directions restricted computational time increased detected points assumed dollar proposed scheme faster original harris referred linear spatial temporal dimensions treated filtering resultant interest points correspond spatial regions exhibit intensity edges points correspond moving regions high moving object willems extended saliency measure spatiotemporal purpose action recognition salient points defined locations strong second order intensity including spatio-temporal difficult determine saliency valid saliency performance comparisons action recognition presented discussion comparisons review paper tuytelaars mikolajczyk specific interest point detection scheme scale salient features remains considering interest points scales provide increased number scale salient region subsequent feature simplest commonly employed interest point detection small set discrete scales work automatic detection optimal scale detected interest proposed approach examining saliency response function scale parameters idea applied determine spatial temporal scales interest points interest point chapter literature review detectors scale positions exhibit maxima saliency measure longer maxima saliency measured leads iterative position scale repeated detected interest point approaches proposed avoid iterative scale-invariant interest point detectors operator mikolajczyk hessian points willems single position scale local feature descriptors sampling points local features descriptors descriptors exploited previous simplest easily computed descriptor proposed dollar based histogram pixel gradient spatio-temporal surrounding interest point gradient calculating gradients centre form descriptor proposed motion based optical produce volumetric features laptev incorporated appearance motion previous utilising hog hof provided complimentary action large number spatio-temporal feature developed extending traditional features field object spatial scale-invariant feature descriptor extended encode temporal willems extended speeded robust feature descriptor extended hog spatio-temporal work presented chapter hof descriptor spatio-temporal extended utilising dimensional flow action recognition authors explored interest point detections feature oshin descriptor based interest point detection strengths spatio-temporal spatiotemporal features based relative saliency distributions demonstrate excellent spatio-temporal interest points sequence encoding stage accumulate local features sample order form holistic descriptor entire local features accumulated directly work oshin common case continuously codebook approach object generally approach attempts cluster samples small subset sequences matching sample closest producing histogram performing accumulation spatially temporally implies holistic descriptor invariant range spatial temporal valuable spatial configuration temporal laptev attempt mitigate spatio-temporal volume creating descriptor concatenating create sequence descriptor follow encoding concatenating create single employ multiple instance learning parts sequence example purpose encoding features sequence level allow subsequent machine learning techniques sequence approaches action recognition avoid machine learning chapter literature review holistic sequence descriptor voting scheme sample providing advantage videos detecting multiple peaks voting holistic descriptor second advantage actions spatially examining samples contribute employing learn descriptors action class weighted voting scheme weight patterns vote determined proposed alternative scheme fast tracked sift features demonstrated motion compensation remove noise features camera third possibility examine frame video based sequence example frame state hidden markov model hmm recognition performed test determining hmm produced observed state sequence advantage temporal difficult determine correct structure hmm order provide good amount training data motion estimation motion features proved valuable action unsurprising temporal nature obvious definition terms image plane action performed depth broadcast natural rich motion earliest work estimating motion scene motion estimation field structure motion work monocular moving exploiting order estimate structure scene techniques equally applicable moving observed static camera work non-rigid conjunction non-linear optimization markov random fields sfm approaches estimate dense field simultaneous location modelling focused sparse reconstruction tracking feature slam tracked features relate interest points action recognition including harris corners detector fast corners authors slam techniques generally perform kalman filter variants techniques demonstrated optimisation techniques low accuracy systems sfm slam focus slam moves dense utilising modern field scene flow estimation introduced vedula attempts structure sfe differs sfm slam removing assumption reconstruction fully order task sfe techniques focus multiple employing monocular sfe algorithms focused multi-view appearance proposed technique based depth termed combined appearance developed work presented chapter applicable number appearance depth previous discussion sfm approaches sfe exhibit sparse dense chapter literature review sparse scene flow estimation local scene flow approaches closely field surface tracking conjunction underlying structure difference temporal scene flow estimates motion points based parts scene pixel assumption scene points tracked correspond points tracked contrast surface tracking based detecting areas areas difference implies surface tracking techniques surface points tracked sequence correspond initially tracked surface tracking algorithms reliable estimating motion fields longer contrast scene flow algorithms suffer provide long term trajectories mesh earliest work field collection tracked multi-view appearance sensors relate surface associated original formulation required configuration light sources respect extended technique perform point tracking scenes provide small number reliable suitable motion estimation motion estimate locations general motion estimate proposed technique space carving object tracking mesh frames brightness allows motion estimate produced point mesh motion points respect tracking mesh motion estimation previous mesh fit approach developed variational framework tracking approaches required initial mesh provided dense scene flow estimation dense scene flow field optical flow addition plane motion initial work sfe vedula focused combination optical flow multiple order estimate mutually consistent flow field work dense sfe attempts estimate motion points finding exact correspondences frame aperture problem well optical flow edge point motion motion edge motion parallel edge assuming edge aperture local patch corner points considered edge constraints locations allow motion uniquely similarity issues occur sfe demonstrate points lying motion determined directions point lies edge formed full dimensional scene flow uniquely determined regions unique estimation scene flow entire scene highly vast majority points authors generally assumption assuming motion field constraints combined neighbouring order determine schemes proposed optical flow generally local patch based inspired work lucas kanade motion pixel chapter literature review estimated constraints subset neighbouring second variational inspired work horn motion field including term total variance sfe approach property motion regions local approaches total range optimisation schemes employed estimate final motion including gradient dynamic energy function highly dimensionality task data matching attempt reduce local coarse fine estimate wide range smoothness terms huguet based spatial gradient motion image plane basha proposed point cloud smoothness assumptions valid smooth motion field project image object boundaries produce discontinuities motion fit underlying assumption erroneous assumption combination constraints leads referred oversmoothing basha source errors modern sfe wide range approaches proposed mitigating over-smoothing simplest employed wedel reduce weight smoothness term based local intensity gradient assumption object boundaries produce strong edges lead difficulties estimating motion highly zhang extend lucas kanade optical flow approach explicit smoothness motion motion estimation estimated local match viewpoints smoothness large overlap close extent oversmoothing limited size employed neighbourhood matching probabilistic framework belief propagation work zhang combined idea segmentation consistent motions determined propagating constraints problem matching local region size shape projection consistent matching implies assumption patch observation orientation relative image approach assumption introduced work zhang fitting parameters motion model approach range motion models patch structure viewpoints global level current structural matching score employed variational update estimated structure motion accuracy depends accurate knowledge camera case approaches proposed inclusion stereo parameters camera parameters scene flow estimated number techniques dense sfe traditional perform traditional variational scene flow integrate temporal applying kalman filtering pixel avoid optimisation estimating small number high accuracy region growing approach motion chapter literature review explicit region growing produce smooth motion estimated motion fields depending distribution smaller based extending space carving voxel scene flow estimation described smoothness assumptions mitigating over-smoothing idea introduced vedula proposed covering structure based simplified standard shape combined optical flow estimation viewpoint exploration generally heavily associated loss reduce included number heuristic including background optical flow thresholding basha developed voxel based scalable large structure selection motion estimated voxel based methods greatest similarity work presented chapter explores original continuous space voxel collection discrete smoothness allowing aperture problem accumulation constraints combining constraints basha allows improve estimation future additional wedel introduction scene flow focus variational schemes chapter action recognition structure performing action recognition tasks natural extremely relevant areas scene appearance orientation definition action high level semantic assigned wide range lead deal intra-class order task achieved making structural highlights occlusions inherent invariance actor appearance lighting approach demonstrated vision suffer high intra-class variations example general pose estimation discussed appendix natural action dataset large number existing action recognition extended account newly structural saliency measures feature finally depth analysis comparing combinations exploring behaviour density features chapter action recognition structure action data collection commercially displays introduced leading rise commercially including films broadcast microsoft kinecttm greatly simplified capture despite work action recognition focussed spatiotemporal sequences spatial dimensions temporal small number datasets produced gesture dataset human actions extremely limited cases publicly currently best exploit action recognition dataset purposes dataset community cvpr 2013 source baseline action data action recognition datasets kth staged datasets generally considered techniques performance action recognition datasets action recognition unconstrained variations camera actor action extremely difficult incorporate level variation staged captured methods designed trained datasets translate well natural surveillance video applications point staged capture limited operating range sensors capture direct outdoor alternative obtaining data including videos clips extracted commercially films action data collection automatic extraction large size hollywood dataset extraction process employed techniques data approach relies extraction time film describe behaviour actors addition alignment produces weak labels training support vector machine action automatically extract films publicly audio description secondary audio films audio description scene contents employing speech recognition description converted suitable automatic extraction table number actions extracted technique conjunction range speech recognition compared manual performance training order addition difficulties involving multiple manual extraction approach original hollywood dataset additional advantage actions accurately manual extraction fraction constructed original depth data extracted films depth variations chapter action recognition structure speech recognition naturally microsoft api api api true positive rate total false table performance automatic action fraction correct automatically extracted range speech recognition compared manual total number false detections leads scenes fundamentally created collection films generated entirely provide real human clips films captured camera james fusion camera 3ality produce content real stereo true depth amount footage greatly leaving drive time step three final resultant dataset referred video covering action dataset level original hollywood increased number action set sequences chosen actions randomly extracted overlap positive films split train test action action tested actors action training actor scene observed action data collection training table training test split data films table number training test clips addition example images dataset figure manual extraction action sequences films sequences sufficient included included sequences frames half field view sufficient majority action videos actions actors head included sequences separate sufficient manual extraction allowed sequences temporally frame removing irrelevant data average sequences seconds sample dataset comprises video left resolution frames amounts video addition left depth video provided resolution frame simulate hybrid viewpoint left viewpoint dataset higher spatial depth true kinect artefacts introduced depth data post processing assumed changes actions appear actions common task action recognition point lead version dataset randomly chosen samples equal dataset experiments chapter action recognition structure usephone standup sitdown punch film swim table train test split film empty entries indicate actions type blue entries train red entries test usephone dance drive shoot kick kiss hug eat standup sitdown punch shoot swim noaction dance drive film train samples test samples table number train test clips action dataset total kick kiss eat hug action data collection figure example frames hollywood left view depth streams examples action classes kiss drive darker regions depth images closer chapter action recognition structure depth data generation order examine action stereo reconstruction performed left viewpoint videos extracted order structural estimate size hollywood state art reconstruction algorithms prove gpu approach allowed depth data generated tractable scheme cross bilateral grid inspired bilateral discontinuity preserving smoothing discussed greater detail generate hollywood temporal variant favours consistency reconstruction remove scheme high memory memory restricted disparity system operate close real process dataset holistic action recognition structure order determine action range traditional spatiotemporal action recognition extended account structural figure flow detection salient spatiotemporal extraction local feature finally encoding sequence single holistic approaches flexible iterative extension pose estimation system appendix difficult perform online estimate location actions temporally holistic accumulation improved robustness interest points leads noisy holistic action recognition structure figure traditional action recognition including depth number salient points detected local feature descriptors extracted local descriptors accumulated holistic descriptor entire video remainder chapter discusses extensions traditional saliency incorporating addition popular feature descriptors action recognition extensions encode structural interest point detection performing dense sampling pose system appendix generally leads number features action discriminative temporal system irrelevant frames estimating salient irrelevant features saliency measures discussed originally formulated static extended operate naive approach extend techniques specific spatiotemporal saliency measures discussed harris corner extension laptev hessian points approach willems chapter action recognition structure separable filters technique dollar depth comparison spatiotemporal interest point detectors traditional spatiotemporal paper tuytelaars mikolajczyk harris corners harris corner popular saliency extended spatiotemporal laptev detector operates spatiotemporal volume applying gaussian smoothing convolution gaussian function calculating second-moment-matrix spatial location temporal location salient points defined locations large strong intensity variation distinct spatiotemporal order improve avoid calculation eigenvalues maxima detected volume calculated equation det evaluate equation explicitly calculating eigenvalues points large eigenvalues maxima maxima large extending operator additional dimension increased include gradients dimension equation gradients equation increased spatiotemporal separate matrix calculated space-time local gradients holistic action recognition structure combination appearance depth streams volumetric data intensity measurements dense measurements form result gradient estimated directly convolution filter relationship gradients depth appearance exploited equation observation appearance gradients spatial temporal depth gradients estimated second-moment-matrix calculated set harris interest points f4d-ha defined set spatiotemporal locations response threshold 4d-ha equation lower threshold values lead extracted increase likelihood noise threshold recognition examined detail f3d-ha 3d-ha f4d-ha 4d-ha hessian points willems extended saliency measure spatiotemporal second-moment-matrix laptev calculated hessian e-4d spatiotemporal volume gaussian definition saliency differs hessian chapter action recognition structure implying interest points exhibit second order gradients roughly contrast harris orientations long eigenvalues large highlights hessian points extension sign second order leading detection spatiotemporal harris gradients estimated relationship depth intensity stream gradients equation allows hessian e-4d calculated equation set interest points f4d-he calculated set spatiotemporal e-4d greater threshold 4d-he equation e-4d det f4d-he det e-4d 4d-he interest point detection original harris hessian interest point operators motivated idea object boundary points highly intensity gradients generally relate depth data directly boundary estimation intensity gradient alternative approach estimating full intensity employ complimentary pair spatiotemporal appearance depth object holistic action recognition structure boundaries colour detecting boundary objects harris corners equation defines set harris points volume created applying equation appearance converted depth relative weighting appearance depth set remainder implying equal weighting appearance depth salient points detected scheme strong intensity gradients spatiotemporal directions point high strong depth discontinuity spatiotemporal directions moving object hessian points equation relates set hessian points terms hessian matrices appearance depth volumes det det mentioned hessian points relate image regions second order detected points relate moving regions chapter action recognition structure separable filters third highly successful approach interest point separable linear filters technique dollar harris hessian dimension treated differently separable filters approach suitable direct technique easily original formulation separable filters create spatiotemporal response volume filtering input gaussian filter spatial quadrature pair gabor filters geve godd temporal equation geve godd gabor filters applied temporal relate single interest point detector regions low intensity regions intensity peak low intensity points relate moving edge high contrast single high contrast gaussian filtering applied spatial simply relates weighted accumulation responses equation formulation separable filters response volume created equation applied appearance depth set detections relates moving high contrast boundaries described moving structural f3d-s 3d-s holistic action recognition structure feature descriptors interest point schemes reduce features irrelevant utilising depth data salient features encode structural highly successful feature descriptors original descriptors include spatiotemporal extensions allow descriptors incorporate structural relative motion descriptor relative motion descriptor oshin perform well large range natural action recognition making saliency interest point calculate spatiotemporal volume int created interest point detector response equation saliency content cont spatiotemporal region origin dimensions region calculated rapidly creating integral small number descriptor rmd saliency distribution location performing pairs randomly offset spatiotemporal figure illustrates relationship location random offsets associated note collections offsets randomly selected set applied subsequent training test subcuboids cont int chapter action recognition structure figure rmd saliency content single saliency content spatiotemporal volume int comparison performed location offsets subcuboids dimensions interest points cont cont rmd example figure subcuboids offsets interest interest points weight cont cont comparison descriptor rmd response rmd computed spatiotemporal location accumulated encodes relative saliency distributions occur descriptor applicable form saliency require appearance motion data amount encoded descriptor number comparisons increasing leads sparser common rmd calculated collections random allows descriptor encode holistic action recognition structure independence bins encoded relative motion descriptor rmd descriptor incorporate structural measurements interest point dimensional integral behaviour saliency image rmd-4d descriptor rmd-4d equation comparing saliency content randomly offset cont-4d int cont-4d cont-4d rmd-4d original rmd descriptor operate conjunction definition saliency interest point detection rmd-4d descriptor restricted operating depth aware interest point detectors feature descriptor equally applicable standard spatiotemporal interest provided depth stream bag visual popular approach feature extraction action bag visual employed laptev inspired highly successful work field object collection distinctive features selected training chapter action recognition structure form sequences described frequency histogram bin relating feature space generate object recognition sift points commonly highly successful field action employ combined hog hof feature descriptor directly encodes appearance motion making equation defines descriptor location creates joint histogram values local region centred point function applied gradient output histogram relates hog output flow images hof spatial neighbourhood concatenating calculated interest points training clustering performed find distinctive subset descriptors range features order form clustering standard distance function contents codebook training sequences subsequent extracted closest cluster accumulated histogram sequence interest single equates single entry prove robust rmd saliency comparisons performed point irrelevant interest points affect histogram idea explored experiments bag visual extend encode structural simplest approach include histogram oriented depth gradients structure appearance motion equation natural equivalent hof depth standard optical flow algorithms tend poor applied depth performing accurate motion estimation trivial estimation motion explored depth bag encoding scheme employed features features previous rmd-4d dependent interest point detector require depth aware interest depth video calculating experiments classification sequence descriptors performed support vector machine function facilitate comparisons hollywood hollywood average precision calculated error metric explained detail relates area precision recall curve detection integration error measure extremely cross allows specific train test set defined hollywood improves comparisons future computation rmd region comparisons performed histogram histograms form bag chapter action recognition structure visual descriptor generated codebook laptev interest point evaluation incorporation structural interest point detection analysis aims determine points chosen salient relevant actions bag visual feature descriptor laptev table performance breakdown class interest point detection type saliency measure large performance average precision best scheme roughly feature standard spatiotemporal harris points outperform separable filter points depth aware separable filters designed computational hessian based interest points prove harris operators scheme standard spatiotemporal approaches prove belief calculation intensity gradients combination intensity structure measure actions perform described depth aware interest actions drive depth aware interest points better actions dance shoot performed depth inclusion depth saliency measure actions punch kick original spatiotemporal formulation best suggests combination standard depth aware prove complexity depth aware interest point detectors remains order experiments action noaction punch kick shoot eat drive usephone kiss hug standup sitdown swim dance f3d-s f3d-ha f4d-he f4d-ha table interest point detector values average class hollywood range interest point detectors including simple spatiotemporal interest depth aware bag visual feature descriptor classes depth aware interest points outperform chapter action recognition structure spatio-temporal respect naturally factor increased techniques roughly techniques times feature evaluation including structure feature encoding average performance classes correct classification rate presented addition average performance measure relevant tasks sequence belong class video clustering table combination interest point feature descriptor best performing feature saliency measure relationship saliency appears feature descriptor cases fast f3d-s performed hessian based extended harris operators provided best spatiotemporal interest points offer worst performance scheme prove effective incorporate depth types descriptor consistent improvement incorporating structural increases roughly average precision correct bag descriptors outperform rmd unsurprising rmd relies interest point inclusion visual motion employed bag reasonable including structural features prove valuable standard saliency depth fact proves features provide gains 3d-s 3d-ha combined extended saliency measures demonstrates depth aware saliency measures capable regions structural features experiments interest points f3d-s f3d-s f3d-s f3d-s f3d-ha f3d-ha f3d-ha f3d-ha f4d-he f4d-he f4d-he f4d-he f4d-ha f4d-ha f4d-ha f4d-ha descriptor rmd rmd-4d rmd rmd-4d rmd rmd-4d rmd rmd-4d rmd rmd-4d rmd rmd-4d rmd rmd-4d rate table descriptor saliency combination descriptor saliency correct classification rate average precision best feature saliency measure chapter action recognition structure figure precision-recall curve feature saliency figure precision-recall curves feature descriptor saliency bag approaches tend offer greatest precision lower recall higher recall levels features converge level feature descriptors offer better spatiotemporal complexity rmd-4d greater standard rmd linear range depth well mitigated integral volumes meaning runtimes order seconds single contrast extraction features relates increase remaining increased feature vector length lead increased cost codebook generally number experiments interest point threshold evaluation generate precision-recall curve figure detection threshold svm interest point detection stage involves separate controls density chance regions included interest point operators produce response meaning optimal threshold salient points previous work threshold selected experiments previous sections employed saliency thresholds based previous figure relationship saliency threshold action recognition interest point separable filter saliency responses appear lie range changing threshold order leads interest points performance saliency standard feature descriptor depth aware exhibit positive rmd based higher saliency thresholds lead increased accuracy harris interest bag approaches provide greatest accuracy lower saliency suggests rmd features sensitive bag features better weak interest point relates single histogram entry bag poor interest points affect rmd descriptor surrounding demonstrates interest point detection reducing computation improving signal noise ratio features lead improved detection modes figure average precision interest point feature calculated detection figure static chapter action recognition structure separable filter separable filter hessian hessian harris harris harris figure saliency threshold average precision interest point detector descriptor saliency threshold experiments static thresholding multiclass multiclass multiclass figure analysis average precision classes hollywood analysed classification modes thresholding relates threshold order class mode requires greater likelihood noaction best combinations interest point feature simpler static difficulties learning noaction remaining detection modes analysed figures standard multiclass compared versus classifiers pair versus versus figure single class vote versus chapter action recognition structure scheme figure allows classifier weighted vote estimated best performance provided versus voting problem simpler tasks scalable training testing times increasing number classes number conclusions high class variation inherent natural action mitigated making removes variations actors lighting demonstrate natural action recognition dataset number existing provide improved performance extended incorporate depth included saliency feature gains performance saliency measure features included depth gain average demonstrates points detected salient operators structural effective measure saliency action recognition proved harris complimentary appearance structural saliency best bag feature descriptors proved increases saliency thresholding leads improved performance rmd reduced performance bag implying rmd scheme sensitive noisy highlight fact dense features comparisons performed selected saliency thresholds dataset future development conclusions complex structural encoding action scale selection methods interest point allow greater performance high resolution encoding depth descriptors structural directly motion features image despite fact estimation out-of-plane motions chapter action recognition structure chapter particle based motion estimation addition direct structural data hollywood motion account plane motions unconstrained order integrate interest point based approach chapter instantaneous motion space-time location salient estimated motion field size existing techniques estimating dense motion approaches require hours process single addition speed existing techniques calculating scene flow rarely consider estimation single formulations allow propagation leading temporal producing motion motion fields reliable recognising third difficulty current motion estimation discussed chapter particle based motion estimation ambiguity inherent task aperture lead previous work focus accumulation constraints spatially increase aperture assuming smoothness motion field local global variational set ambiguous single result rise motion discontinuities motion object assumption leads selection preserving occurs referred over-smoothing basha artefacts source errors modern scene flow estimation techniques context action recognition system chapter artefacts motion object vast majority salient points introduction problem sfe presented remainder chapter explores novel probabilistic approach sfe iccv temporal consistency propagating oversmoothing artefacts multiple approach presented orders magnitude faster previous making suitable application action recognition dataset chapter motion field estimation consider observed cameras points task estimating motion point considered correspondence figure illustrates examples correspondence combinations involved correspondence indicated bottom figure correspondence point specifies location point frame stereo matching correspondences frames camera optical flow figure defines plane motions motion field estimation correspondence stereo match optical flow optical flow stereo match full correspondence figure scene flow estimation represented boxes forward edge image plane cross focal indicate points project point detected multiple sensors defined bottom detections point frame start combination single stereo match single optical flow match meaning point detected frames sensor single frame leads optical flow start point combination three constraints stereo matches optical optical flow stereo sufficient uniquely determine motion point fact combination three correspondences uniquely defines fourth optical flow correspondence offset stereo matches stereo correspondence determined points flow correspondences chapter particle based motion estimation examining figure natural consider scenario stereo optical flow scenario involve difference sfe stereo reconstruction optical flow stereo correspondences position point create separate structural reconstruction parts structure optical flow constraints moving points image plane correspond point result case pair motion knowledge structure rise sfe aims fully describe scene obtaining single unified figure extended include additional sensors making problem changing three stereo matching correspondences single define larger optical flow fields define underlying constraints determining scene point covering sensors points depth sensor stereo correspondences meaning single optical flow correspondence required uniquely determine optical flow correspondences providing scene techniques referred difficult determine motion correspondences depth data appearance sensor optical flow creating probabilistic scene flow probabilistic scene flow continuous dimensional space structure scene point space vector structure continuous dimensional space points space referred vector sfe extract set points motions input observations set images number appearance depth dimensional scene probability distribution probable combination structure motion observed peaks distribution generate estimate scenes structure motion note limiting distribution single peak optical rise multiple hypothesis properties mentioned bayes define posterior distribution terms prior distribution likelihood bayes extended joint estimation time probability sequence structure motion sequence leading equation transition function t-1 t-1 constant velocity motion point t-1 t-1 t-1 motion models incorporation require increase dimensionality particle transition function represents scene chapter particle based motion estimation flow field generate prior probabilities equation note uniform distribution prior frame system exponential meaning initial likelihood sequence sequence scene flow fields equal product analysed conjunction equation allows transition likelihood terms frame equation t-1 t-1 terms combined equation t-1 t-1 formulation requires likelihood term current conjunction posterior previous transition formulation suitable online complexity increase additional term equation remains likelihood chance set observations scene structure point moving velocity sections define based provide likelihood based observations collection appearance based observations probabilistic scene flow collection depth likelihood product assumes independence formulation trivial incorporate sources independence appearance likelihood appearance observations output appearance sensors rgb projection functions pixel likelihood structure point brightness constancy assumption brightness remains constant direction fundamental assumption explored improved rgb assumption extended colour assume true appearance point average appearance observed sensor squared error brightness equivalent variance projected previous current frame cost likelihood t-1 likelihood depends current values likelihood formulation note previous assume gaussian measurement noise function referred approximation chapter particle based motion estimation figure comparison brightness constancy yellow equation values green blue gaussian formulation red exponential cost brightness figure illustrates behaviour robust estimation function suitable energy tasks probability gaussian assumption reflect true motion estimation noise motion estimation peaks heavier formulation presented produces well formed likelihood constancy peaks heavier gaussian exponential parameter control kurtosis set remainder depth likelihood depth task ambiguity discussed detail additionally depth sensors scene particles contribute likelihood appearance observations collection depth projection functions projection functions likelihood structural point determined closely matches depth observations frame t-1 fit previous depth observations frame quantify average square position sensors calculated equation note parameters control relative contribution appearance depth based expected noise note type sensor relevant likelihood term depends entirely remaining sensor sum empty set defined approach combination including appearance depth hybrid images project likelihood point project including current frame previous motion likelihood set constraints match explained uniquely define sensor configuration depth sensors provide equivalent stereo match constraints point visible sensor colour assigned likelihood scene particles continuous number structure number applying coarse chapter particle based motion estimation remains dimensional simple pair cameras leads structure equivalent number monte-carlo sampling approach analyse small subset points provide approximation continuous probabilistic remaining particle filtering samples associated resampling techniques employed sampling density promising areas particle filtering approaches offer convergence dimensionality space note number fundamental differences detail algorithm presented standard particle filtering remainder samples referred scene particles specifies spatial location velocity dot represents scene image form particle population weight scene particle analysing likelihood observations scene particles previous frame constant velocity motion sampled approximation prior probability equation formulation leads principled propagation temporally consistent sfe prior distribution initialised randomly uniformly weighted collection scene figure flow diagram number depth sensors additional stage performed account reduction spatial scene particle calculated depth images subset sensors valid depth estimate projected location sensor randomly selected provide spatial location note multiple samples case sensors structure scene particles figure flow diagram scene particle iterative discussed note previous estimate input occluded random selection process ensures particle population reflects sensor visibility velocity values scene particles remain multiple hypotheses sample velocities spatial location distribution complex lying original ray resampling particle filtering techniques involve additional step samples probability solve problem resampling serves focus hypotheses promising order best number initial particle cloud directly transition samples high probability termed particles common resampling schemes resampling sampling density determined chapter particle based motion estimation current estimate high probability regions densely achieved set drawing particles previous set t-1 probability drawing particle normalised weight residual resampling scheme resampling particle randomly drawing remaining samples based residual scheme density samples point space determined estimated posterior probability particles assigned uniform population reflects underlying scene particles algorithm fundamentally standard particle filtering general particle hypothesis relates complete standard particle filter implementation based joint space dimensional simply number samples required particle filter number scene particles algorithm formulated output motion field comprises large collection high probability focus system valid hypothesis well detail particle population resampling iterations performing local exploration largest secondary modes scene particle algorithm aims estimate large number local order estimate motion loss secondary peaks leads reduced scene coverage estimated motion leading motion estimated single point scene particles referred avoid modified resampling scheme termed particle population based optical rays particles lie subset sensor ray pixel defined note subsets single sensor scene particle lies ray subsets sensors mutually scene particle multiple number sensors scene particle population residual resampling performed particle population constructed drawing scene particles probability scene particle drawn defined normalised equation process ensures minimum scene particles estimate motion point loss allowing chapter particle based motion estimation convergence particles motion spatially subsets extracted resultant motion valuable property dense contrast majority sfe stereo reconstruction estimate densely scene particles lie optical leaving subset subset filled scene particles randomly selected neighbouring drawn particles position lie long empty leaving motion parameters distance ray random selection scheme ensures modes surrounding data empty smoothing scene particle cloud instance ensures fully dense motion multiscale iterative estimation improve accuracy estimated motion iterative refinement techniques set observations likelihood scales coarse fine scaled input observations convolution gaussian variance scaled observations constructed half scale equation leads define likelihood term product terms scale scene particles multi-scale approaches commonly variational optimisation local minima closest initial multi-scale difficult algorithms estimate large moving cost surface increases local performing local minima smoothed allowing initial estimate scale closer global scene particle multi-scale iteration serves multi-hypothesis nature local minima large motions probable small multi-scale estimation allows observations point observed iterative diffusion second iterative inspired particle optimisation involves change behaviour scene particles diffusion particle population particles multiple diffusion stage variance approach equation random drawn dimensional gaussian vector matrix gaussian distribution determined range values visible scene diagonal matrix dimension based chapter particle based motion estimation mine mine note mine represent vector formed maximum minimum equation defines range values project sensor current previous note property scene camera stationary cameras dependent particle t-1 depth sensors exploration velocity space result elements diagonal set chance diffusion occurs note variance distribution drawn scaled parameter equation parameter controls exploration higher levels lead behaviour low levels lead refinement current second iterative estimation involves repeated sampling likelihood equation resampling diffusion reduced values interpretation scene particle cloud multi-hypothesis representation scenes structure representation evaluation ground truth existing scene flow provided optical flow field conjunction stereo flow field disparity scene flow evaluation traditional approaches formulated dense reference order scene particle cloud representation multiple hypotheses associated probabilities image generated pixel weighted average scene particles rays diffusion system variance lead weighted average proves resampling scheme clustered scene particles high probability output image encoding image plane motions start points flow plane motion terms disparity change disparity projected point interpretation stage original scene particle performed frame weighted averaging simple interpretation process removes multiple modes multiple simplified view scene particle point view reference sensor reasonable multiple modes modes implies interpretation scene particle cloud prove tasks sensor required optimal example collection scene flow evaluation performance scene particle algorithm compared previous state art sfe properties algorithm qualitative evaluation performed number including long appearance depth microsoft kinecttm figure example frame chapter particle based motion estimation rgb depth motion field figure example scene flow images frame kinect images appearance depth input data image estimated motion motion direction red black performing comparisons sfe techniques difficulty ground truth point synthetic data sequences dataset provide possibility obtaining dense ground truth demonstrated stereo motion estimation techniques exhibit fundamentally tested real synthetic datasets explored comparisons chapter performed datasets middlebury system scene flow datasets existing state art sfe middlebury datasets originally designed benchmarks stereo reconstruction example images figure order analysis developed originally huguet dataset comprises complex single scene flow evaluation figure middlebury images middlebury cones teddy datasets note scenes highly include equally parallel rgb ground truth disparity maps accurate disparity captured light data construct collection equivalent moving scene smaller number specific images form frames sequence frames sensors appears translation equal separation sensors original sequences represents static translating complex combinations images simulate including videos high computational cost existing sfe comparison evaluate normalised root square error metrics proposed basha normalise error scores range values ground order facilitate comparisons previous work types error measured figure error green chapter particle based motion estimation directional optical flow errors stereo flow error structural error figure types scene flow error ground truth structure point flow estimated point flow error measures note reality errof errsf error components structural estimation measured entirely errst figure relates image plane motion pixelwise error equation based output interpretation stage equivalent channel image ground truth note second images channel velocity channel ground truth errof calculated averaging normalised rms form error errof squared pixelwise errors square root ground truth scene flow evaluation errof min max second error measure figure relates plane motion pixelwise form defined errsf total nrms error score errsf max min third error measurement figure relates accuracy structural error negligible operating mode structural input analysing multi-view pixel error defined equation errst leading nrms error score errst max min previous authors final error measure represented green figure defined pixel directional error defined equation chapter particle based motion estimation errae directional average angular error measure equation lack scene specific implies assumption scene range motion errae experiments sections performed scene particles ray particle filter equal maximum values number image scales set diffusion variance set iterations reducing third comparison comparison existing scene particles algorithm appearance hybrid mode appearance depth data single viewpoint output microsoft kinect compared multi-scale variational approach huguet variational formulation basha appearance hybrid techniques tested publicly comparison alternative hybrid technique technique operates running state art optical flow algorithm appearance combining ground truth order table technique referred examining performance hybrid mode algorithm faster accuracy hybrid mode scene particles approach greater motion magnitude accuracy greater directional accuracy scene flow evaluation algorithm dataset sensors depth sensors errof errsf errst errae coverage runtime frame scene particles scene particles basha basha huguet scene particles scene particles basha basha huguet scene particles scene particles basha basha huguet cones secs cones cones secs 213 secs cones cones cones hours teddy secs teddy teddy secs 207 secs teddy teddy teddy hours venus secs venus venus secs 213 secs venus venus venus hours table scene flow estimation performance range hybrid appearance mode note runtimes structure errors negligible depth entries highlight best performance chapter particle based motion estimation interesting note approach dedicated optical flow motion accuracy image worse scene particles implies incorporation depth data stage allows accurate flow additionally technique scene coverage estimated motion field cases approach final difficulty basha incorporate number appearance scene particles system incorporate combination appearance depth multiview appearance performance scene particles algorithm appears improve hybrid mode suggests larger number input images provided multiple increased difficulty estimating structure motion structure estimation multiview scene particle algorithm improved dedicated multiview motion magnitude accuracy scene particle algorithm image directional estimation accuracy stochastic nature scene particles leads estimated motion fields low level terms motion noise generally regions low motion background small change absolute leads large change runtime comparisons speed listed sequential computation single exploiting possibility provided independence scene computation time provided basha optimization scheme assumed order huguet scene particles approach operates roughly 100 times previous state art analysed single frame sequences scene flow evaluation algorithm dataset sensors depth sensors errof errsf errae coverage scene particles non-rr scene particles scene particles non-rr scene particles scene particles non-rr scene particles cones cones teddy teddy venus venus table ray resampling scene flow estimation hybrid sensor scene particles ray resampling terms accuracy propagation frames scene particles improvements performance multiple principled fusion lack oversmoothing ray resampling analysis demonstrate issues discussed quantitatively examine ray resampling table compares performance scene particles algorithm standard resampling scheme proposed ray resampling reasonable assume lead accumulation particles regions high reduced coverage increased accuracy regions remain appear fact ray approach produces chapter particle based motion estimation accurate directional estimates datasets roughly equivalent motion magnitude implies subset local maxima higher probabilities automatically correspond reduced sampling density monte-carlo based sampling density scene particles algorithm accurately underlying distribution specifies effectively local maxima sampling implies greater computation scene particles define sampling density terms scene particles relates input scenes range values observed sets set high resolution second low number scene particles sampling density terms analysis systems performance increased error rates high resolution accuracy resultant motion examined fewer hypotheses number hypotheses sampling density defined invariant sensors ability sampling density feature scene particles parameter speed depending implementation reduces allowing additional samples analysed sampling density controls second accuracy memory relevant gpu figure demonstrates effects varying sampling values hypotheses error measures exhibit exponential relationship sampling directional accuracy angular error scene flow evaluation optical flow error speed stereo flow error speed structural error speed directional error speed figure performance varying number scene particles cones highlights fundamental limit stochastic estimation systems error motion magnitude structural error rates appear completely runtimes greater indicating limit lower error search space analysis state space dimensions scene particles range structure velocity values observations sensor scene particles range chapter particle based motion estimation figure search volume search volume listed error measures fraction performance measure errof linear cones comparison approaches transition likelihood removed utilising smaller state space requires fewer scene particles achieve sampling application specific knowledge example objects cover scene single state space number scene particles achieve reduction runtime cost reducing size state space maintaining constant number scene particles leads improved figure demonstrates scene particles algorithm previous state art exploring velocities maximum state space scene particles velocity values cover scene single visible realistic higher velocity space search areas state additional scene particles required increasing computational cost approach orders magnitude faster ray resampling leads scene flow evaluation figure scales relationship cones number image scales optical flow error errof error measures exhibit reduction growth increasing search implies ray resampling cover state iteration analysis iterative estimation schemes multiscale approach employed variational motion second iterative based diffusion benefit schemes quantitatively analysed figures forms iterative refinement number iterations error rate multiple scales proves unsurprising addition scene particles time additional number iterations performance gains implies scene particles maxima remaining errors likelihood function true motion field brightness sensor computational cost increases linearly number chapter particle based motion estimation figure diffusion loops relationship cones number diffusion loops optical flow error errof error measures exhibit propagation scene particle algorithm orders magnitude faster approaches analyse behaviour approach sequences longer application algorithm real vision relevant action recognition application chapter figure performance algorithm sequences varying error scores single frame sequence considerable reduction directional plane motion error subsequent frames provide improved behaviour motion hypotheses rarely remain ambiguous sets observed lower limit performance regions prior provide optical flow error measure rapidly requiring sequences frames improvements suggests image plane motions suffer require accumulation observations behaviour serves estimation scheme scene flow evaluation optical flow sequence length structural error sequence length directional error sequence length figure sequence length analysis error measurements sequences experiments performed single appearance depth standard scene particles chapter particle based motion estimation capable propagating robustness noise algorithm applicable real data action recognition dataset chapter robust noise range including sensor noise quantitatively analyse input sequences corrupted varying levels gaussian noise figure gaussian noise pixels modified gaussian performance analysed varying standard deviation noise salt pepper noise varying fraction pixels randomly set viewpoints treated algorithm performs well salt pepper error metrics increase linearly number corrupted full range noise levels independent nature scene estimate motion corrupted pixels remain variational patch based approach expected linear relationship low noise corrupted pixels estimated correctly smoothness higher levels global approaches suffer performance system gaussian noise generally linearly increasing absolute gaussian noise estimated motion salt pepper standard deviation intensity values equivalent salt pepper noise multiscale approach smoothing applied create image reduces scene flow evaluation optical flow error salt pepper noise optical flow error gaussian noise structure error salt pepper noise structure error gaussian noise directional error salt pepper noise directional error gaussian noise figure noise three error function noise left salt pepper noise gaussian noise tests performed single appearance depth cones standard scene particles chapter particle based motion estimation object tracking application scene particle algorithm allows motion fields estimated rapidly existing variational approaches orders providing temporally consistent long features sfe feasible computer vision interest example application sfe perform object tracking segmentation lack oversmoothing artefacts scene particle algorithm additional benefit providing object generally approaches fitting model object tracking collection distinctive feature points approach representation objects interest naturally work third property objects approach tracking colour distributions common hand work tracking optical flow comparison tracking scene flow output object moving moves image plane projective object extraction order extract objects scene flow scene particles clustered location motion creates clusters consistent collections structure separate objects close moving distinct purposes example clustering performed fitting collection dimensional gaussians scene flow advantage approach simpler allows objects differing object tracking application clusters number clusters naturally trajectory estimation order construct trajectories clusters consistent set labels achieve frame predicted cluster generated clusters previous constant velocity motion performed match labels newly estimated cluster clustering frame clusters initialised task specific clusters cluster centre defines centre object average algorithms motion estimates common estimate velocity difference previous reliable objects change shape making hand tracking sign language analyse performance tracking system based scene flow specific task tracking hands head sign language application trajectories expected provide valuable sign language number objects tracked model based approaches extremely difficult high levels figure scene flow estimation system input data multi-camera produces estimate scene estimated motion additional task specific included form term likelihood chapter particle based motion estimation figure trajectory estimation original scene particle algorithm darker interpretation stage trajectory estimation equation accuracy relevant term defined equation relates likelihood combination belong object based skin colour scene flow estimation regions reduced number scene particles improving computation clustering frame performed motion random selection sign language skin colour model comprises probability distributions rgb colour background colour pbg point scene skin colour psk likelihood term calculated space colour vector ratio skin background averaging responses system single view min psk pbg object tracking application pbg initialised colour models psk allowed based face colour central region face detection update psk distribution remainder image update pbg hand tracking evaluation performance tracking system ground truth position moving objects real scenes rarely true sign language objects move currently sign language dataset ground newly captured multi-view trajectories image plane figure example frames parts trajectories projected estimated trajectories objects consistent observations frames dataset relating hours sign language application traditional sfe requiring years contrast scene particle algorithm required years sequential making feasible split processing small number order scene particles algorithm faster table seconds skin colour prior good tracking particles scales diffusion calibration provided labelled correspondence points estimate camera serves demonstrates technique degree robustness figure parts narrow figure illustrates narrow baseline creates ambiguity trajectory matches data well projected left projection sensor erroneous frame chapter particle based motion estimation object head hand left hand agreement rms error rms error table hand tracking agreement projection estimated trajectories alternative system palm wider baseline systems figure resolve additionally third viewpoint case allows system operate case despite completely occluded side view estimated trajectory hand consistent observations system analysed comparing projection trajectories high accuracy tracking provided data tests consistency providing lower inconsistencies errors tracking trajectories assumed separation predicted location third palm frames trajectories agreement listed object table rms distance terms palm compared tracked close agreement systems indicates tracking system system greater accuracy head trajectory moves accuracy left hand higher viewpoint hand occluded object tracking application figure hand tracking example narrow showing projection trajectories consistent illustrates case narrow baseline rise ambiguity frame wide robustness object chapter particle based motion estimation conclusions chapter focussed particle filter based technique estimation dense motion providing rich description dynamic contrast existing work chapter focussed maintaining multiple approach avoid common oversmoothing artefacts estimated motion boundary regions tend salient vision including action technique provide quantitatively improved benchmark scene flow formulation scene particles algorithm allowing combination appearance depth coupled runtimes orders magnitude faster previous state art motion estimation feasible wider variety vision applicable realistic hollywood good robustness types sensor noise propagation leading temporally consistent demonstrate suitability approach real vision example application tracking hands sign speed motion estimation tracking performed vast amount data hours consisting multiple camera combinations demonstrates scene particles algorithm flexible wide range scenes sensor action recognition hollywood despite flexibility suitability real vision improvement current formulation based naive assumption brightness violated real smoothness constraints despite generally leads valuable producing object boundaries reasonable conclusions constraints improve accuracy preserving chapter particle based motion estimation chapter revisiting motion estimation assumptions despite flexibility speed scene particles algorithm chapter improvements accuracy examining fundamental assumptions improvements based naive linear relationship brightness constancy additional assumptions smoothness constant velocity smoothed scene particles original scene particles avoid oversmoothing artefacts removing smoothness constraints utilising multiple temporal accumulation assumption motion field smoothness generally large areas approaches incorporating smoothness scene particle chapter revisiting motion estimation assumptions patch based scene particles lucas kanade formulated earliest optical flow accumulated flow constraints pixel local neighbourhood technique leads smoothness motion field neighbouring patches variational approaches developed work horn motion estimates discontinuities local extent limited patch variational schemes affect entire motion scene particles algorithm follow local likelihood term equation sum pixel-wise likelihoods neighbourhood neighbourhood extracted projection particle scores calculated conjunction algorithm consider spatial patch addition temporal constraints prior modification depth likelihood term equation function extension sum squared differences similarity standard metrics encode pixel-wise differences equation compares pixel-wise consistency number patches allows metric employed number approach assumes sensors parallel flexibility inherent assumption small regions scene patches image plane correspond patches smoothed scene particles image plane smoothing patch based scheme directly smoothing scene particle oversmoothing artefacts accumulate time propagation smoothness alternative approach employ smoothness constraints interpretation stage averaging multiple leaving underlying scene particle cloud simplest achieve follow previous authors apply smoothing image plane projection scene achieved smoothing channel output image produced described values channel treated order oversmoothing edge preserving bilateral filter bilateral filtering inspired operation human visual bilateral filter performs smoothing averaging values pixel determined spatial gaussian spatial gaussian equivalent standard convolution gaussian gaussian based pixels values central contribute smoothing applied values smoothing kernel overlaps values side discontinuity central pixel increased values side pixel equation result weighted averaging pixels offsets lying neighbourhood contribution pixel determined magnitude offset gaussian weighting function variance provide spatial second term contribution based difference pixels central second gaussian variance total chapter revisiting motion estimation assumptions weight neighbourhood normalised filter bilateral filtering greatly reduces smoothing compared simple gaussian relates level level high frequency standard gaussian level expected edges change suffer lower contrast edges heavily improvement standard gaussian requires selection small details low smoothing applying smoothing post processing smooth scene particle cloud projection smoothness assumptions valid original projective points longer affect simply project neighbouring achieve bilateral filtering perform weighted averaging scene dimensional achieve oversmoothing unsmoothed scene particle cloud generate prior frames input scene particle cloud interpretation scene particle original cloud consists smoothed equivalent consisting smoothing evaluation equation smoothed calculated weighted averaging scene offset lying particle neighbourhood bilateral filtering weighting neighbouring scene particle magnitude gaussian weighted variance velocity dimensions integrated motion hypotheses position addition standard gaussian weighting variance weight hypothesis sample order simulate weighted averaging original equates total weighting normalise filter equations created provide smoothed values case variance relates strength construct smoothing amount high frequency features variance size smoothing evaluation table compares smoothing figure post smoothing motion low level figures removed smoothed estimate figures leaving general motion formulation bilateral oversmoothing visible chapter revisiting motion estimation assumptions smoothing removes fine motion details example areas longer move longer patch based matching post improvement terms motion magnitude applying post filtering increases magnitude directional accuracy cost computation post filtering techniques applied motion structural performance patch based matching cost reduces accuracy structural degree comparable previous state art approaches table discussed previous loss fine structure introduction over-smoothing artefacts directional accuracy closer levels previous discussed produced removal low level occurs unsmoothed motion small amount motion noise leads large directional regions low motion background extremely implying fundamental similarity global variational schemes local patch based smoothing employed demonstrates current incorporation smoothness constraints optimisation limiting factor current motion estimation remains true incorporated scene particle despite addition multiple hypothesis examine scale smoothing table local neighbourhood spatial variance set area gaussian represented filtering neighbourhood set cover region equivalent scene range spatial variance determined figure explores effects varying figure displays varying scale smoothing evaluation algorithm dataset errof errsf errst errae runtime post post post post post post venus venus venus venus teddy teddy teddy teddy cones cones cones cones 213 secs secs secs secs 207 secs secs secs secs 213 secs secs secs secs table smoothed scene particle performance scene particles algorithm incorporating smoothness constraints variety tests performed occlusion aware scene appearance 100 chapter revisiting motion estimation assumptions frame 108 unsmoothed frame 108 smoothed frame appearance frame 108 appearance frame 113 unsmoothed frame 113 smoothed frame 112 appearance frame 113 appearance figure smoothed flow field comparison estimated flow frames kinect standard scene particles algorithm bilateral estimated motions light red black frames smoothing evaluation gaussians originally set scene range spatial smoothing scale behaviour patch based system structural estimation directional accuracy performance closer previous state art directional error post filtering increases scale post filtering provide directional accuracy motion magnitude error measures exhibit optimal performance neighbourhood size smoothing subsequent increases scale lead reduced additional spatial constraints oversmoothing point increasing neighbourhood size leads exponential increase computational patch based smoothing behaviour observed spatial smoothing scale performance improves smoothing scale increasing scale leads reduced behaviour low small difference neighbouring motions sufficient additional constraints meaning smoothing scale neighbouring motions improving scale properties bilateral filter discontinuities considering rise oversmoothing general peak performance times initially tested scene range filtering appears gain benefit changes smoothing reflecting table changes smoothing changes spatial note increasing smoothing scale additional computational spatial chapter revisiting motion estimation assumptions optical flow error spatial smoothing scale stereo flow error spatial smoothing scale structure error spatial smoothing scale post filtering affect directional error spatial smoothing scale figure spatial smoothing change scene flow estimation function spatial smoothing patch bilateral bilateral smoothing axis specifies kernel patch variance spatial gaussians set standard deviations note lower error scores relates better smoothing evaluation optical flow error smoothing scale stereo flow error smoothing scale directional error smoothing scale figure smoothing change scene flow estimation function smoothing bilateral axis specifies variance gaussian original defined spatial smoothing neighbourhood note lower error scores relates better structure reconstruction performance post 104 chapter revisiting motion estimation assumptions forwards scene particles addition incorporating smoothness simple modification original likelihood formulation allows assumption constant velocity modified favours motions consistent previous current consistent current future constant velocity assumption tracking generally assumed high changes velocity formulation additional reduces ambiguity serves purpose propagation prior explored formulation allows gains requiring number frames reliable system frame achieve frame input image motion field applications assumption constant velocity implies high acceleration note modification depth likelihood equation effects performance varying degrees violation constant velocity explored occlusion aware scene particles addition assumptions improvements brightness constancy motion estimation occlusion aware scene particles explicitly account points cases camera motion occluded points scene particles points occluded visible sensors allow reconstruction quantify visibility integrated likelihood create occlusion aware version scene particles order likelihood occlusion maps estimated prior probability point visible sensor time probability point occluding occluding points lie camera centre optical unit vector occluding points distance sensor lower probability occluding point defined probability probability visibility function equation figure point origin relationship co-ordinates points relate structure point origin camera occlusion probability ray sensor point unit vector integration equation performed ray offset sensor note previous approaches motion estimation evaluation points occlusion common occlusions determined simply lack consistency scene particles algorithm likelihood proposed formulation requires occlusions justified estimated structure motion points labelled occluded point occlude chapter revisiting motion estimation assumptions figure occlusion rays relationship points dimensional point origin co-ordinates origin reference structure point scene unit vector ray sensor origin sensor point -uc note velocity dimensions prior produce visibility point time dependent instantaneous velocity prior function output probability normalised range input particle sum obvious normalise total weight particles equation occlusion aware scene particles -uc formulation leads nonzero point ray low probability occlude point extend example visibility particle dependent weight particles front occlusion probability normalised probability -uc integral relates probability points including visibility probability correctly lies range nonzero point ray assigned probability point probability points lying occlusion probability defined figure illustrates behaviour single discrete case involving particles continuous case assuming constant gradient sample note visibility probability particle despite low occlude particle high probability visibility despite particle front higher occlusion probability probability point probability points front complete visibility maps probability distributions dimensional binary employed handling viewpoint separate visibility vision tracking object 108 chapter revisiting motion estimation assumptions discrete case continuous case figure occlusion probability cases continuous discrete discrete example scene particles varying continuous case assumes constant gradient note probability occlusion particle weight greater particles front visibility maps integrated scene flow estimation modification original likelihood formulation equation contribution sensor variance calculation weighted based visibility consistency viewpoints assigned greater modification applied depth likelihood equation note visibility probabilities total visibility point times average visibility assumption variant analysis visibility probabilities directly increase likelihood viewpoints visible allows points high appearance consistent subset provided subset justified visibility analysing likelihood visibility described leads converge mutually consistent formulation point occluded single consistency sensor original likelihood discussed constraints figure number sensors greater average visibility including current previous constraints assumption variant analysis table performance standard scene particles algorithm previous compared forwards-backwards formulation chapter revisiting motion estimation assumptions algorithm basha basha huguet basha huguet dataset cones cones cones cones cones teddy teddy teddy teddy teddy venus venus venus venus venus errof errsf errst errae runtime 213 secs secs secs hours 207 secs 213 secs secs hours 213 secs secs secs hours table scene particle performance constant velocity scene flow estimation appearance scene particles embodying constant velocity probabilistic compared basha huguet non-constant velocity occlusion aware system previous state art techniques table included variant appears provide improved terms negligible increase computational structural accuracy errst occlusion aware approach larger error cost increased non-constant velocity forwards-backwards formulation appears offer improved performance negligible analysis dataset newly incorporated constant real applications action recognition task focus assumption examine sensitivity system trivial middlebury create test sequences levels frame sequence comprising relates increase second third frames motion increases times sensor approach sequences generated levels figure performance standard scene particle algorithm comparison error metrics display small deviations lead forwards-backwards system provide gains original sensitive constant velocity assumption action recognition 112 chapter revisiting motion estimation assumptions optical flow error scene acceleration stereo flow error scene acceleration structure error scene acceleration directional error scene acceleration figure constant velocity violation analysis scene flow estimation forwards-backwards scene particles violations constant velocity improving brightness constancy 113 improving brightness constancy sensitivity forwards-backwards deviations constant velocity brightness constancy motion estimation techniques optical flow scene well real brightness constancy assumption examples include specular occlusions directional case scene flow issues multiple response previous sensitivity effects rarely motion estimation schemes analysed synthetic ground truth motion fields real data scene flow synthetic datasets rarely equivalent tests forwards-backwards system analysed artificial assumption existing motion estimation techniques exhibit real data conclusions drawn equally applicable multiview scene flow hybrid scene flow appearance brightness constancy assumption involves appearance hybrid scene flow scenario equivalent single view optical flow unified exploring behaviour brightness constancy motion vector image define set supporting pixel set image position frame number pixels involved optical flow entry current frame previous chapter revisiting motion estimation assumptions scene flow estimation motion associated pixel set pixel locations sensor start points motion sets supporting pixel locations sensor set associated pixel values defined matching functions unified behaviour brightness constancy assumption small number matching functions proposed embody brightness constancy comparison vast number smoothness simplest based absolute deviation brightness constancy function defined abs equation supporting abs approach authors employ define equation relates sum squared improving brightness constancy supporting function scene particle presented general equation functions applied rgb well creating separate set supporting pixel values input leads function based colour constancy gradient images leading set supporting gradient values functions abs sqg based gradient constancy assumption functions based linear proposed gradient constancy abs sqg final function flow complex supporting pixel values time temporal gradient image function equates linearised brightness constancy assumption terms quadratic higher function defined equation spatial gradient 116 chapter revisiting motion estimation assumptions figure example frame cones sequence function behaviour analysis analyse embodying appearance constancy responses examined ground truth motion fields optical flow scene real cones sequence figure figure example frame remainder performed additional sequences middlebury including range illumination additional presented appendix blue regions ground regions indicate areas appearance constancy assumption dark red scene displays general behaviour brightness constancy functions abs well prove sensitive regions brightness constancy assumption valid specular scene flow case fine pattern details close pixel level improving brightness constancy optical flow scene flow abs abs sqg figure function response response motion estimation ground truth motion real dark red regions indicate deviations constancy blue regions lack ground chapter revisiting motion estimation assumptions differently varying gradient constancy measures abs sqg prove robust functions optical performing well occlusions fine scene flow gradient constancy functions comparable brightness linearised brightness constancy function appears robust occlusions displays greatest inconsistency edges fine areas provide regions maximum response consistency lack truth errors function inconsistencies generally providing high response true motions half task good second provide low response ideal function produce continuously error input figure probability density function fraction scene assigned response function cones displays ground truth motion field red values figure motion field motion magnitude location note response function normalised facilitate response implies inconsistency associated constancy response strong ground truth motions lie lower artefacts erroneous motions considered lead distribution linearised brightness constancy function overlap gradient based functions abs sqg produce heavier distributions erroneous improving brightness constancy optical flow scene flow abs abs sqg figure response distribution responses motion estimation applied ground truth error motions real responses normalized range chapter revisiting motion estimation assumptions shape locations peak increase number best separation occurs simple brightness constancy functions abs performance large motion error large overlap response distributions implies erroneous cases scene constancy assumption directional lighting attempting function response scene result correct motions convergence behaviour motion estimation schemes employ follow gradient function assuming reducing inconsistency leads reduced motion scene particles algorithm explicitly response iterative resampling diffusion lead shift sampling density prior response figure axis displays average response motion amount motion error varied true brightness constancy based schemes display general reduced inconsistency true despite overlap response functions based gradient constancy sqg perform roughly response produced motions ground implies function employed motion convergence occur extremely close true linearised brightness constancy worst function favours smaller average displays correct explains multi-scale approaches commonly attempting enable function allow larger improving brightness constancy optical flow scene flow abs abs sqg figure convergence average constancy deviation analysed varying degrees motion chapter revisiting motion estimation assumptions intelligent transfer functions proposed functions motion assume linear quadratic relationship deviation appearance deviations occur properties reflect errors input functions account perform motion errors scene employ machine learning find matching unconstrained robust appearance inconsistencies real sensitive inconsistencies motion learning nonlinear embody complex expected light dark parts image contrast variation expected appearance deviations learning response observed regions figure second specular effects large change appearance colour changes appearance channel relate erroneous order produce nonlinear decision forest classifier rdf capable estimating likelihood probabilistic framework scene particles removes normalisation step produce normalisation fit online estimation schemes motion classifier trained true motions erroneous estimated likelihood equivalent previous constancy deviation functions response indicates motion response indicates motion presented appendix itfs trained extracted improving brightness constancy sequence teddy chapter generalisation functions data middlebury sequences figure performance based range input feature variance feature set fvar defined equation set single equivalent output squared differences function defined equation learning nonlinear mapping improved separation direct optical flow difficult scene flow estimation improvement original fvar second feature set differences features fdif calculated distance supporting pixel supporting equation equivalent input absolute difference function abs defined equation learning naive functions attempting itf provide greatly improved considering nonlinear combinations improvement scene flow number features fdif third feature set fpix attempts learn directly raw pixel optical flow case leads improved differences feature fdif scene flow estimation fdif proves rdf assumption features fpix contained correlation note ideal true difference feature fdif examined pixel features fpix ideal long feature takes obvious chapter revisiting motion estimation assumptions features fpix difficult rdf learn correlation explains performance lower scene flow case consists optical flow case despite raw pixel features additional allow regions treated differently depending intensity dark light normalized fdif explains improvement optical flow fpix itfs display reduction standard vast majority motion fields true bottom response remaining response small number addition complex nonlinear combinations supporting include higher level features based local context local image variance fpv supporting indicate pixel lies boundary features defined equation patch centred set relating local neighbourhood patch fpv local supporting pixel create feature set fpm light dark regions supporting pixels improving brightness constancy optical flow scene flow fvar fdif fpix figure itf response distribution responses applied ground truth erroneous motions real chapter revisiting motion estimation assumptions robust version fpix proved valuable optical flow fpm complex form contextual feature defined fph equation relates coarse bin histogram local values supporting contextual variance features fpm fpv light versus dark regions boundaries versus additional fph note contextual features general direct commonly employed local motion estimation contextual explicit correspondence creates robustness patch projective separate warping itfs employed local schemes pixel-wise comparison figure inclusion local variance feature fpv benefit fdif implies motion object determining complex local histogram features fph provide improved performance optical flow prove scene flow larger number features scene flow difficult determine accurate class valuable contextual features prove local fpm simplest figure average response varying levels motion response greater improving brightness constancy 127 optical flow scene flow fdif fdif fpv fdif fph fdif fpm figure contextual itf response distribution responses itfs based fdif including types contextual chapter revisiting motion estimation assumptions original brightness constancy functions scale axis modified figure order curves suggests itfs provide convergence optimization motion addition expected gains inclusion contextual improved convergence addition improvements motion estimation intelligent transfer functions itfs display robustness appearance constancy motion estimation including scene particles multiscale estimation iterative warping designed mitigate standard functions quantify gains robustness translate improved accuracy motion original scene particles sampling likelihood distribution weight scene achieved analysing equation supporting pixel set scene particle itf note scene particles algorithm formulated probabilistic standard energy response itf high values relate true low values relate erroneous motion estimation accuracy number itfs compared original testing performed multi-view hybrid equates optical flow purposes brightness constancy addition motion estimation computational complexity function table described terms number supporting pixels cardinality hybrid scenario multi view independent colour number pixels local context itfs additional rdf analyse improving brightness constancy optical flow scene flow fdif fdif fpm figure itf convergence average function response varying levels motion best itfs contextual previous function rescaled comparison chapter revisiting motion estimation assumptions function fvar fdif fpix fdif fpv fdif fph fdif fpm errof errae complexity log log log log log table single view motion estimation performance performance hybrid scene flow based standard range computational complexity linear implementation exploiting independence trees cost equal number trees times log depth complexity parameter linear functions neighbourhood cardinality quadratic runtime cost function roughly order experiments complexity lead simpler itfs roughly computation time multiview computation hybrid scenario roughly itfs utilising contextual features times increased roughly factor interesting faster fpix simply calculate variance estimating multiview scene changes computational affect stage scene particle motion estimation reflect behaviour initial sensitivity demonstrates despite multiscale estimation scheme mitigating scene particle algorithm sensitive brightness constancy improving robustness improved accuracy single view motion worst performing conclusions 131 function fvar fdif fpix fdif fpv fdif fph fdif fpm errof errsf errst errae table multi view motion estimation performance performance scene flow based original range leads reduction motion magnitude raw pixel improvement features fpix prove effective reduced errof directional closely combination difference context features fpm multi-view fvar improvement original simple nonlinear mapping complex itfs provide performance best difference features coupled context fdif fpm improvement magnitude improvement directional coupled reduction structural conclusions fundamental assumptions scene particle algorithm initially inclusion additional embodying constant velocity assumption approaches employed incorporate assumption smooth motion approach leads motion estimation previous state art performance improved directional performance error previous chapter revisiting motion estimation assumptions smoothness constraints limiting factor performance scene flow constant velocity assumption computational analysis demonstrated sensitivity assumption real vision motivated sensitivity original brightness constancy assumption context number proposed matching function artefacts real standard functions violated degree erroneous scene functions exhibit poor convergence optical flow smaller gradient constancy assumption required accurate itf machine learning techniques create general robust non-linear version brightness constancy itfs based range including number types contextual functions offer robust representation behaviour real scene artefacts heavily erroneous improved convergence incorporating itfs scene particles improvements accuracy hybrid multiview negligible computational future application leads improved performance alternative motion estimation addition probabilistic framework examined additionally approach smoothness depth constancy assumption range flow hybrid scene flow prove valuable learn joint single function embody brightness constancy smoothness finally application itfs areas image image registration chapter motion features scene particles algorithm allows generation accurate motion speed tractable real vision hollywood action recognition introduced chapter chapter explores techniques encoding feature descriptor structure explored operator create joint histogram values local hog defined application operator gradient images hof descriptors defined operator applied optical flow images description hof case formulation scene flow histograms oriented optical flow hof descriptor referred oriented optical encodes local motion salient point descriptor calculated defined set pixel degree spatial neighbourhood split grid separate histogram computed 133 chapter motion features result normalised equations normalising neighbourhood descriptor gains scale well invariance changes speed action differences relative motion histogram spatial computed flow magnitude orientation defined equations number bins flow bin cell histogram determined equation set pixel locations cell relates pixel weighted bin determined mag ori arctan mag ori number orientation bins set bin pixels orientation second bin filled magnitude orientation result pixel bin histograms oriented scene flow histograms oriented scene flow approach easily employed encode scene output image defined image encode image plane motion images third channel encodes plane motion fourth channel disparity three dimensional single orientation pair spherical azimuth defined equations figure illustrates spherical orientation start point unit magnitude flow vector origin system centre point lie surface discrete azimuth specifies rotation plane values diagram relates motion flow point concentric azimuth plane orientation elevation specifies rotation takes values reduced range ambiguity ensures point defined single unique pair azimuth elevation point azimuth 180 elevation magnitude flow controls voting weight relevant defined equation azi-p arctan ele-p arctan mag-p orientation joint histogram combination azimuth bin elevation bin defined 136 chapter motion features figure azimuth elevation bins spherical azimuth rotation relating moving point concentric elevation rotation axis moving concentric example bins azimuth elevation cell equation optical flow normalising concatenating cell histogram produces full hos descriptor mag-p azi-p azi azi ele-p ele ele undistorted hos descriptor original hof descriptor encode additional plane formulation closely histograms oriented scene flow velocities projective scene flow velocities computed orientation magnitude alternative weighted set scene particles lie ray sensor weighted average scene defined equation particle normalised scene particle total weight ray elements scene particle relate velocities orientation magnitude images equations images equation projected produces undistorted hos descriptor azi-3d arctan arctan mag-3d independent encoding moving dimensional joint greater optical flow hof descriptor neighbourhood size cell left shift lead sparsity ratio pixels number orientation greatly descriptors independent encoding chapter motion features orientation allows azimuth elevation encoded pixels contained correlation orientations approach leads smaller descriptors bins cell improving train test simple modification cell histogram equation leads independent histogram azimuth orientations azi equation definition distribution elevation orientations descriptor formed normalisation equation 3d-azi mag-3d azi-3d 3d-ind 3d-azi 3d-alt 3d-azi 3d-alt 3d-azi 3d-alt 3d-ind 3d-ind 3d-ind histogram scene-flow evaluation applicability scene particles algorithm hos feature encoding examined context action scene flow field calculated particles image scales diffusion entire hollywood dataset years traditional scene flow hos encoding azimuth bins elevation bins azi ele histogram scene-flow evaluation scene particles algorithm requires estimates parameters relative order determined films hollywood dataset fusion camera 3ality remaining films camera 3ality extremely flexible capable operating camera parameters fusion camera system approximation system ratio implying pixel dimensions focal order relative configuration cameras create varying depth experiments assumed cameras close separation equal distance naturally parameters estimates camera changing focal effects mitigated extent scale invariance hos relative distribution motion strength absolute qualitative figure illustrates estimated scene flow field subsequent hos encoding simple kinect showing hand histogram slices relating differing levels out-of-plane motion rgb figures correspond slices elevation bins degrees chapter motion features rgb scene flow depth hos encoding bin hos encoding bin hos encoding bin hos encoding bin figure scene flow hos encoding rgb depth images single frame sequence estimated scene flow field background removed remaining images elevation slices subsequent hos feature performed densely scene background green squares normalised histogram scene-flow evaluation scene flow field figure majority motion occurs arm small amount moving hos encoding images reflect vast majority motion fourth azimuth bins bins oriented positive axis interesting note completely overlap encoded amount despite stationary motion scale invariance hos small amount motion faster moving region low level background motions hollywood spatio-temporal saliency measures hos features extracted stationary hos images elevation plane motion motion arm regions occur figures relate motion camera reflects contents motions head appear images indicating small amounts motion order plane image correctly indicating scene moving figure example hos encoding eat hollywood figures indicate motion figure relating motions displays motion arm interest motion uniformly image correctly reflecting motion moving hand face regions small nonzero areas actors display motion scale figure motion bottom small head chapter motion features hos encoding bin hos encoding bin hos encoding bin hos encoding bin zoomed head region elevation bin zoomed arm region elevation bin figure hos encoding hollywood eat small number interest points overlaps local green squares represent red dot centre images zoomed regions indicated yellow boxes original histogram scene-flow evaluation action recognition hos features analysed quantitatively hollywood action recognition approach chapter harris saliency measure descriptors offer best performance table comparison optical flow integrate motion hof elements descriptors equation selection hos based interest point codebook holistic svm remain including appearance structure elements feature parameters remain previous hollywood experiments chapter codebook size conjunction grid cell consists figure performance type motion multiclass detection scalable feature described number elevation bins ele set plane motions feature standard depth maps provided hollywood dataset feature 3d-int encoding employing itf scene flow estimation fdif features trained teddy venus middlebury relationship detection modes scalable approach generally providing improved performance standard multiclass classification interesting note motion features based scene flow provide improved performance optical flow based scene particles algorithm simulate hof performance worse standard implies estimated scene flow field motion chapter motion features field temporal inconsistencies full scene flow histogram offer gains hypothesis plane motions valuable action figure motion feature standard hof features compared range scene-flow encoding effects projective performance compared undistorted independent encoding scheme 3d-ind difference multiclass detection independent encoding lead performance histograms mitigated extent learning large number training simpler tasks scheme fewer samples increasing sensitivity sample interesting note independent encoding scheme histogram scene-flow evaluation implies contained correlation encoding orientations leads greatly reduced feature vector efficient reduction depth maps scene flow estimation recognition sparsity depth combined formulation scene particles based dense output depth suggests combined scheme prove structure estimated regions scene regions provided largest gain performance employing itfs scene flow estimation 3d-int allows reasonable motion fields calibration issues leading increase performance optical flow based features implies improved accuracy convergence properties lead increased robustness camera alignment demonstrates itfs trained middlebury datasets well varied including outdoor scenes variety illumination breakdown performance type motion table note performance original features figure directly comparable multiclass table types action drive swim scene-flow leading average example drive sequence figure obvious difference eat sequence figure scene interest points second observation majority motion scene occurs elevation bins relating small amounts out-of-plane motion occurs moving majority motion azimuth relates motion left global chapter motion features action noaction punch kick shoot eat drive usephone kiss hug standup sitdown swim dance 3d-ind 3d-int table motion feature class average precision class type motion feature best performing feature class previous features figure translation occur swim depending camera occur camera camera explains plane motions recognising knowledge strong global discriminative motion motion head left image interesting note performance scene-flow features differs hof based despite fact histogram scene-flow evaluation hos encoding bin hos encoding bin hos encoding bin hos encoding bin zoomed region elevation bin zoomed head region elevation bin figure hos encoding hollywood drive large number interest points overlaps local green squares represent red dot centre images zoomed regions indicated yellow boxes original chapter motion features encode general features appear effective classes features work well shoot drive improving classes dance shoot improved actions characterised stationary small amount complex motion eat sequence figure traditional optical flow schemes accurately resolve motion smoothing interest points occur object lack smoothing scene particles algorithm allows motions 3d-int features benefit lack exhibit better performance improving global actions exploitation plane motion conclusions chapter techniques presented allowed motion features chapter extended exploit hollywood scene flow estimation techniques scene flow optical flow lead improved flow fields camera itfs scene-flow estimation robustness leading improved recognition encoded correlation out-of-plane motion orientations allowing efficient independent encoding scheme scheme provide improved performance cases number training samples low reducing sparsity change increased number training lack smoothing scene particles algorithm recognising classes involving small amounts conclusions dance plane incorporating plane motion greatly improved performance actions drive characterised large amounts simple motion future improve estimated scene flow utilising larger number hypotheses improved parallel gpu order remain tractable datasets size hollywood alternative approach employ automatic estimation schemes camera performing including parameters motion estimation framework lead reliable motion length extreme sequences approaches motion feature encoding developed future sparsity issues scheme addressed allowing motions multiple neighbouring orientation extending independent encoding scheme include calculate distribution orientations additional promising exploration inclusion rotational invariance orientations lead action described wide range feature distribution frame remains rotational invariance simplifying task action classification reducing intra-class compensation scheme remove camera greatly reduce noise hos simplifying chapter motion features chapter discussion thesis set explore methods effective task recognising natural human work motivated rise commercially footage suitability highly invariant depth task characterised complex intra-class challenge addressed lack suitable dataset benchmarks action recognition hollywood dataset wide range existing action recognition techniques extended additional including interest point detectors feature well action recognition improvements performance depth aware interest points features encoding structural employing combination focus estimation areas structural features leading rise average precision analysing structural varying saliency threshold lead differing behaviour feature rmd descriptors accurate higher bag features implies rmd sensitive noisy better chapter discussion extract large set noisy finding fact sparser features lead improved accuracy addition increased computation comparisons feature descriptors lead conclusions saliency challenge incorporation motion existing techniques motion estimation conjunction dataset size hollywood formulation scene flow estimation based particle filtering maintaining multiple hypotheses exploiting accumulation constraints approach applicable real computer vision tasks previous state operating orders magnitude faster removing smoothing artefacts motion estimation salient regions technique robust types measurement offer improved performance standard scene flow developed formulation flexible operate combination appearance depth sensors highlight wide alternative example application tracking hands sign hours scene particles algorithm developed exploring inclusion additional occlusion motion smoothness assumptions provide large gains performance cost reduction approach performance scene particle algorithm previous state art techniques directional accuracy improved performance measures suggests formulations smoothness limiting factor inclusion constant velocity assumption initially additional computational analysis demonstrated extreme sensitivity violations making motivated sensitivity analysis original brightness constancy 153 scene particles algorithm motion estimation common formulations brightness constancy violated extent real violations violations erroneous motion implying estimated motions artefacts real transfer employing machine learning techniques create general robust matching functions better represent real translate improved motion estimation accuracy optical flow scene flow negligible computational itfs appear valuable variational convergence finally estimated motion fields exploited generate features action recognition hollywood itfs offer robustness errors camera translating noisy motion fields greatly improved action recognition demonstrated generalisation itfs location contained correlation plane plane efficient independent encoding improve performance cases sparse histograms breakdown performance actions benefit motion features comprises actions characterised large amounts strong single drive actions plane motion detection actions performed directly second actions involved small amounts complex shoot actions easily lack smoothing scene particles traditional optical flow techniques contributions thesis large dataset actions wide range benchmark chapter discussion publicly source saliency functions based spatio-temporal appearance range feature extraction schemes action recognition extensions previous number schemes based motion novel particle based approach scene flow smoothing errors operating orders magnitude faster previous intelligent transfer function greatly improves accuracy robustness motion allowing motion features provide action recognition despite camera future work excellent development work field natural action recognition hollywood dataset benchmarks community explore additional techniques fully exploiting promising development include feature encoding schemes motion boundary histograms volumetric development saliency detectors scale invariant improve performance high resolution data hollywood integral volumes parallel processing exploited increase saliency estimation runtimes wider range classification stage action recognition addressed dedicated multi-modal learning schemes multiple kernel classifiers better deal increased sparsity high dimensional feature effectively exploit prove future work interesting explore voting based classification schemes actions time reasonable assume structure scene flow enable full addition improving accuracy scene particles algorithm promising exploration state space smoothing schemes filter forward smoothing schemes developed field allow particle filters joint distributions repeated simply schemes allow particle filter previous accurately previous providing estimate context scene particles algorithm extremely allows temporal accumulation motion constraints apply forwards currently motion point ambiguous motion hypotheses leading poor hypotheses leading accurate motion currently principled previous noisy estimate estimating joint motion field multiple frames naturally alternative simpler approach handling ambiguous motions interpretation stage particle cloud converted estimated motion basha weighted multi-modal easily scene efficient gpu implementation scene particles algorithm extremely allowing application scene flow wider range vision including real time considerable publicly gpu implementation future deal work remains itfs chapter discussion interesting explore integration local variational motion estimation addition approach areas computer vision image registration finally exploitation scene flow feature descriptor developed inclusion rotational invariance techniques reduce histogram additional elements incorporated estimation automatic camera parameters improving robustness video tasks hollywood appendix pose recognition structure work appendix explores coarse depth data instantaneous pose serves complex temporal action texture features local binary pattern texture features appearance structure features provide invariance translating illumination changes appearance object distance features highly possibility rotational accuracy multiple lbps describe image terms histogram components dark points light points intensity peaks depth lbp pixel image labelled neighbourhood thresholding point centre result long binary number 157 appendix pose recognition structure lbp lbp features extended capture texture components allow operator indicates points sampled uniformly controls feature scale controls length output size feature limit detail greater number distinct pixels bins feature histogram lbp labels tend belong small subset patterns termed lbps characterised binary removal histogram bins improves classification experiments dataset large removal variant lbp operator rotational order achieve lbp pixel minimum minimum equation defines represents binary shift lbp lbp greater reduction feature vector size uniform apply rotationally uniform histograms lbp single lbp variant labelled variant histograms provide additional histograms computed providing description local texture region labelled concatenating region histograms forms feature vector image finally pose recognition framework 159 concatenating image histograms depth appearance images objects feature representation exact effects specific feature variants performance tasks normalising histogram features invariant scale detected pose recognition framework random decision forest classifier large number decision trees based random subset allows trees capture class weak classifiers combined strong experiments appendix forest 100 trees random sample ratio feature random forest classifiers likelihood distribution classes input observations allows likelihoods estimated mitigating classification based likelihood distribution proves advantage particle filtering particle filtering particle filter takes output classification stage observation prior probability class based appendix pose recognition structure previous system state system bayes probability class particle filter number weighted modified previous state based system stochastic resampling step higher probability distribution accurately estimated larger number hypothesis previous iteration number based normalised equation illustrates resampling represents ith function normalised hypothesis total number set hypotheses time figure example output pose classification system applied particle initially particles uniformly classification output particles converge peaks distribution particles centred higher pose tracking allows pose estimate continuously despite initially discrete classification pose data collection depth extracted stereo point stereo camera mask size sparsity smaller provide order provide dense depth stereo reconstruction performed mask images pose data collection 161 figure likelihood tracking hypotheses pose likelihood distribution pose applied pose positions hypotheses application likelihoods figure combining multiple depth image previous combined higher mask mask size figure demonstrates showing sequence images pixels previous filled depth map captured larger mask set depth maps generated set stereo performing stereo point matching left images represents stereo matching ith output depth map created pixel pixel values depth map ith stereo 162 appendix pose recognition structure discrete head pose object pose larger initially object step task example head location extracted well boosted classifiers technique semantic hand classes hand detection detector human hands requires large amounts data performs worse faces hand detectors segmentation segmentation performed background motion skin segmentation case general work depth images utilising heuristic hand extended front weak camera model scale object image plane resultant scale object image determined distance object image scale ratio equation focal length case scale ratio face hand based measurements depth background object depth object image depth distance expected object suppressed intensity depth simple heuristic allows operation noisy pose recognition evaluation 163 figure head hand detection depth image showing face detection point detection estimated hand scale hand face appearance background detection background clutter depth object suppressed objects location scale generally clutter small region specific performance increase background figure illustrates hand detection face closest region depth represented red yellow dot scale hand estimated depth represented green second image intensity background performed pose recognition evaluation datasets captured pose datasets appearance depth datasets subjects performance measured cross random split test training set case small amounts scale translation variation image directions 164 appendix pose recognition structure figure hand pose randomly selected appearance depth image pairs dataset scissors scale variation images dataset creating additional image producing additional hand pose classification test hand pose small number static scissors determined suitable dataset depth appearance images created including subjects wide range create specific gesture orientations total appearance depth image pairs captured random selection image pairs dataset figure performance measured number feature table table testing entirely depth required object structural features removed classification classification based depth appearance features achieve performance combination improves hand pose classification 165 feature type channel depth channel channels lbpu lbpr lbpu lbpu lbpu lbpr lbpr average correct classification standard deviation table hand pose classification operating variants lbp lbpu lbpr rotationally invariant rock rock paper scissors paper scissors table hand classification confusion multi-scale lbps predicted classes true 166 appendix pose recognition structure standard lbp features provide excellent utilising features scale provide improved class based represented higher uniform lbps implying components determining positions uniform removing patterns smaller feature improving training test times rotationally invariant lbps perform worse compared rotationally variant rotational variations well represented invariance features confusion matrix table performance rock paper class higher scissors scissors examples suffer higher class rock paper images classified features scissors class extended number image points scissors shape head orientation head pose parameters pose direction pan angle tilt angle dimensions classes degree including subjects range required position relevant angle feature faces background suppressed dataset sparser hand 153 images class pairs appearance depth images sparse dataset task classification based capable operating discussed sparse datasets head orientation figure head pose three randomly selected appearance depth image pairs head orientation degrees degrees degrees degrees degrees degrees note scale variations included feature incorporate directly feature difficulty dataset inconsistency degrees rotation difficult capture accurately human subjects naturally tend move dataset classes remainder randomly selected example images dataset figure tests initially performed isolated range feature variants classification performance listed degrees listed reflecting probable range mentioned depth background detected objects improves performance removing clutter depth feature accurate standard appearance channel effective system combination feature channels provide improved standard lbp features achieve classification sparse dataset cover variations features improved uniform lbps providing best feature bins appendix pose recognition structure test mode average exact classification classification degrees standard deviation colour features depth colour features colour features depth features lbpu lbpr lbpu lbpu lbpu lbpr lbpr depth colour table head pose estimation isolated types lbp features lbpu lbpr rotationally invariant head orientation 169 mode exact classification average pan error average tilt error classification degrees frame classification pose tracking table continuous head pose head pose estimation continuous sequence pose chosen hand pose improvement features multiple rotationally invariant lbps considerable expected test dataset rotational invariance pose tracking framework head pose estimation performed continuous set isolated test particle filtering framework sequence complete occlusions subjects changes table applying temporal constraints determining current examples classified correctly isolated dimensions average error angle roughly coupled fact frames classified examples lie figure confusion matrices pose tracking framework dimensional pan tilt classes tilt angle changes pan angle changing points classes 170 appendix pose recognition structure figure head pose confusion pose 153 class head pose darker pixels indicate greater classification average correct classification rates confusion reality degrees observed confusion matrix multiple diagonal class image fewer diagonal features relate lower average confusion tilt pan cases extreme meaning classification system accurately find correct region pose feature confusion matrices increased number extreme compared central tilt angle easily determined faces pan greater confusion tilt interactive system order demonstrate systems real-time interactive demonstration system hand pose demonstration figure image demonstration system video system head orientation figure interactive demonstration hand pose estimation appendix pose recognition structure appendix additional motion function additional brightness constancy itf chapter presented alternative sequences middlebury stereo 173 appendix additional motion function optical flow scene flow abs abs sqg figure response distribution responses motion estimation applied ground truth error motions venus responses normalized range optical flow scene flow abs abs sqg figure response distribution responses motion estimation applied ground truth error motions laundry responses normalized range appendix additional motion function optical flow scene flow abs abs sqg figure response distribution responses motion estimation applied ground truth error motions wood1 responses normalized range optical flow scene flow abs abs sqg figure response distribution responses motion estimation applied ground truth error motions rocks1 responses normalized range appendix additional motion function optical flow scene flow abs abs sqg figure itf convergence performance venus average function response varying levels motion best itfs contextual previous function rescaled comparison optical flow scene flow abs abs sqg figure itf convergence performance laundry average function response varying levels motion best itfs contextual previous function rescaled comparison 180 appendix additional motion function optical flow scene flow abs abs sqg figure itf convergence performance wood1 average function response varying levels motion best itfs contextual previous function rescaled comparison 181 optical flow scene flow abs abs sqg figure itf convergence performance rocks1 average function response varying levels motion best itfs contextual previous function rescaled comparison appendix additional motion function bibliography ieee computer society conference computer vision pattern june ieee computer society conference computer vision pattern ieee international conference computer october ieee computer society conference computer vision pattern june european conference computer ieee international conference computer october european conference computer september ieee computer society conference computer vision pattern june gesture dataset ieee computer society conference computer vision pattern june bibliography international conference image analysis dynamic reconstruction structure visual proceedings ieee workshop structure motion scene computer vision pattern recognition ieee conference june multi-view scene flow view variational proc ieee computer society conference computer vision pattern recognition van speeded robust leonardis active visual computer international conference jun rotationally invariant image international joint conference pattern filtering sequential methods black framework robust estimation optical computer fourth international conference actions spacetime ieee computer vision iccv volume coupled hidden markov models complex action computer vision pattern bibliography ieee computer society conference jun random machine high accuracy optical flow estimation based proc european conference computer mesh proc ieee computer society conference computer vision pattern recognition multi-view scene capture video streams non-rigid shape proc ieee international conference computer vision multiple maps augmented 12th ieee scene flow estimation growing correspondence proc ieee computer society conference computer vision pattern recognition histograms oriented optical flow nonlinear systems recognition human proc ieee computer society conference computer vision pattern june human action analysis multi-view computer workshops james local scale selection gaussian based description proc european bibliography conference computer volume computer june robust structure motion motion computer fourth international conference dense accurate spatiotemporal multi-view proc conference computer histograms oriented gradients human proc ieee computer society conference computer vision pattern recognition cordelia human detection oriented histograms flow leonardis real-time simultaneous mapping single proc ieee international conference computer vision real-time single camera ieee transactions pattern analysis machine non-rigid structure motion tracking non-linear computer vision pattern recognition conference june particle systems multi-camera scene flow tracking points proc ieee computer society conference computer bibliography vision pattern volume june recognition sparse spatio-temporal joint ieee int visual surveillance performance evaluation tracking surveillance comparison resampling schemes particle image signal processing proceedings international symposium van visual object classes international journal computer june high accuracy velocity estimation orientation simultaneous segmentation motion proc ieee international conference computer vision dense sparse accurate motion segmentation monocular video proc international conference image analysis recognition multi-camera tracking probabilistic ieee transactions pattern analysis machine real-time face object intelligent international conference dense motion capture video proc ieee computer society conference computer vision pattern june bibliography fast realistic recognition dense spatio-temporal ieee 12th int computer vision action recognition pattern analysis machine ieee transactions disparity flow estimation dynamic proc international conference pattern volume novel approach state signal actions spacetime ieee transactions pattern analysis machine mikolajczyk feature tracking motion compensation action simon hadfield richard particle based scene flow depth international conference computer simon hadfield richard hollywood recognizing actions natural conference computer vision pattern recognition june selection context action ieee 12th int computer vision bibliography harris combined corner edge vision horn determining optical artificial huguet variational scene flow estimation stereo proc ieee international conference computer vision ieee international conference computer european conference computer october dense motion disparity estimation belief proc conference computer efficient visual detection volumetric proc ieee international conference computer volume 166 173 cordelia spatio-temporal descriptor based british machine vision september parallel tracking mapping small augmented ieee international symposium laptev space-time interest ieee int computer vision learning realistic human actions ieee computer vision pattern recognition cvpr bibliography laptev actions ieee computer vision iccv multi-scale scene flow stereo computer vision image stereo multi-scale scene flow ieee workshop motion video volume volume action recognition based bag computer vision pattern recognition workshops ieee computer society conference automatic selection temporal scales proc frames feature detection automatic scale international journal computer recognizing realistic actions videos ieee computer vision pattern recognition cvpr david distinctive image features scale-invariant international journal computer lucas iterative image registration technique application stereo proceedings international joint conference artificial colour proc british machine vision september actions ieee computer vision pattern recognition cvpr bibliography tracking colour objects image vision mikolajczyk based scale invariant interest computer iccv ieee international conference volume comparison hand shape recognition proc conference ieee spatio-temporal stereo international journal computer dense reconstruction single moving proc ieee computer society conference computer vision pattern recognition dense tracking mapping computer vision 2011 ieee international conference richard andrew david david andrew real-time dense surface mapping augmented reality 2011 ieee international symposium 127 rotation invariant texture classification local binary ieee transactions pattern analysis machine boosted classifier hand shape proc ieee international conference automatic face gesture bibliography relative distribution features action ieee int automatic face gesture recognition workshops andrew richard automatic mode finding action recognition pattern recognition image volume computer highly accurate flow computation justified international journal computer alignment sign language workshop gesture modelling dynamic scenes multi-view image proc ieee computer society conference computer vision pattern recognition variational scene flow estimation similarity proc ieee international conference computer vision multi-view stereo reconstruction scene flow estimation global matching international journal computer accurate motion field estimation stereo image sequences proc european conference computer vision real-time bibliography stereo matching proc european conference computer vision machine learning corner leonardis estimating scene flow multiple optical proc international machine vision image processing gaussian propagation model based dense optical flow objects proc international conference image analysis recognition learning discriminative spacetime actions labelled proc british machine vision september stereo depth maps ieee computer society computer vision pattern volume range flow varying proc european conference computer vision range flow varying algorithms ieee transactions pattern analysis machine recognizing human local svm pattern recognition volume sift descriptor application action proceedings international conference bibliography scene reconstruction voxel international journal computer good features proc ieee computer society conference computer vision pattern jun real-time human pose recognition parts single depth proc ieee computer society conference computer vision pattern recognition probabilistic tracking reconstruction human motion monocular video range flow computer vision image correspondence surface proc ieee international conference computer vision real-time monocular proc ieee international conference learning optical proc european conference computer vision shape motion image streams international journal computer prior motion pattern bibliography tuytelaars local invariant feature detectors computer joint estimation structure stereo proc european conference computer james differences stereo motion behaviour synthetic stereo international conference image vision scene proc ieee international conference computer volume september shape motion carving proc ieee computer society conference computer vision pattern recognition scene ieee transactions pattern analysis machine object detection boosted simple proc ieee computer society conference computer vision pattern recognition action recognition dense computer vision pattern recognition 2011 ieee conference june cordelia evaluation local spatio-temporal features action british machine vision september bibliography non-rigid structure motion based intelligent computation 2011 international conference volume 181 scene flow computation motion international journal computer wedel scene flow motion efficient dense scene flow sparse dense stereo van efficient dense scale-invariant spatio-temporal interest point zhang integrated scene flow structure multiview image proc ieee computer society conference computer vision pattern recognition zhang scene flow structure proc ieee computer society conference computer vision pattern recognition zhang scene flow structure multiview image ieee transactions
learning real-time temporal scene flow estimation lidar data autonomous systems motion dynamic algorithm estimates motion raw lidar data real-time sensor data construct occupancy foreground extracted learned background filtered occupancy raw scene flow successive scans incorporate measurements filtering framework estimate temporal scene evaluate method kitti autonomous driving vehicles mobile dynamic environments motion systems detection sensors provide point cloud consider dynamic scene work point cloud obstacle tracking techniques vehicles rely detection objects directly lidar point including previous work point free space point lidar occupancy grids free unknown occupancy lidar point sensor grid observations occupancy grids occupancy grids sensors increase number voxels occupancy grid easily better work input raw lidar construct occupancy grids filtered compute raw scene flow measurements occupancy constancy incorporate measurement filtering framework estimate reject estimate temporal scene formulate real-time temporal scene flow framework lidar pipeline raw lidar data input scene flow previous problem obstacle tracking previous key goal proposed method temporal data object formulate problem optical flow occupancy work learned framework tracking lidar observations occupancy background temporal occupancy algorithm estimating scene real-time scene flow method kitti dataset estimating dynamic motion sensor data including computer driving mobile computer optical flow scene flow motion image moving dynamic objects environment motion estimation camera data lidar note sensor lidar sensors provide set provide provide accurate estimate appearance lidar sensors report laser laser optical flow dimensional motion image plane constancy flow scene motion estimated camera sensor scene flow generally real-time kitti scene flow including process scene presented method estimate scene flow real-time sensing methods directly computer lidar key obstacle tracking sensing motion dynamic obstacle objects extracted sensor data observations objects associated temporally compute build appearance obstacle tracking methods rely measurements time methods appearance previous obstacle tracking problem point cloud computed provide framework efficiently successive scans estimate relative motion scans obstacle problem approaches obstacle tracking assume sensor data objects associated consider problem estimating rigid scene flow lidar formulate energy problem based matching feature rely point rely data temporally filter result successive filtering framework dimensional grid rely produce observations step sensing directly motion sensor goal compute scene flow lidar scans work application autonomous assumption locally autonomous vehicle assume object motion plane objects vertical column plane note assumption application proposed method compute motion goal compute temporal scene flow location sensor observations lidar scan static background structure scene flow estimation ground autonomous vehicles estimate scene flow static estimated relative motion work estimating scene flow dynamic objects dynamic objects describe point lidar sensor time construct occupancy grid consider frame vehicle time learned background filter extract foreground estimate scene flow successive occupancy grids gt-1 learned compute flow frame easily frame estimate vehicle raw scene flow measurements filtering framework provide estimate temporal scene lidar describe lidar sensor create occupancy grid occupancy grid process lidar build occupancy grid voxels object occupied computed laser lidar prior state assume prior log-odds set approaches rely prior maps environment maps maps ground plane static structure obstacle trackers rely maps sensor measurements objects approaches accurate approaches appearance structure data foreground vectors extracted create feature vector report runtime report runtime filter full 100 method previous techniques occupancy logistic location occupancy find method prior maps allows training data structure columns occupancy column build binary feature vector extract neighborhood occupancy grid location build binary feature vectors soccu state full neighborhood binary soccu free soccu ray ray log-odds update observation log-odds observation free occupied compute log-odds lidar ray algorithm efficiently implemented background filter flow background static step columns occupancy grid dynamic scene flow associated filter find helps runtime performance errors techniques lidar sensors rely methods static background ground plane free occupied vectors binary feature vector logistic free soccu note feature vector unknown state decision evaluate train extract training data kitti kitti log sequences build feature vectors columns occupancy grid extract columns labeled kitti tracklet build training set train logistic decision threshold classifier accuracy allows reject static background dynamic background filter classifier column occupancy step implemented efficiently columns raw scene flow measurement estimate vehicle describe process compute temporal scene flow occupancy grids gt-1 background filter filter gt-1 filter helps errors background voxels filtered gt-1 corresponding location occupancy constancy compute scene flow occupancy method occupancy state columns successive occupancy rely occupation occupation state matching columns successive assume generally valid method achieve performance objects formulate occupation constancy learning find learning better columns ct-1 gt-1 vertical array voxels occupancy grid construct binary feature free columns feature vectors free training sample neighborhood location nct-1 true corresponding construct training kitti log sequences data build training training train logistic classifier occupancy constancy pmatch columns window scene store result table log pmatch window columns gt-1 centered ct-1 raw scene flow scene formulate energy problem learned occupancy constancy iterative algorithm estimate locally flow successive occupancy step ct-1 gt-1 maintain current estimate scene flow column compute associated scene flow estimate leads ct-1 gt-1 store energy associated computed scene flow estimate leads algorithm column occupancy process columns efficiently implemented estimate scene flow column ct-1 gt-1 location neighborhood nct-1 neighborhood columns centered location compute energy nct-1 binary feature vectors binary feature vector ct-1 logistic pmatch ct-1 columns occupation train rely kitti dataset extract training create training dataset sample column ct-1 gt-1 location scan matching ground truth relative estimate labeled kitti tracklet compute true scene flow find true corresponding column ct-1 construct sct-1 gt-1 neighborhood ct-1 valid scene flow estimate sct-1 scene flow vector ct-1 ct-1 gt-1 find corresponding scene flow computed simply update energy store estimated scene flow sct-1 assumption locally column ct-1 gt-1 column consider ct-1 gt-1 ct-1 energy column previous scan leads set ct-1 temporal scene flow raw scene flow measurement filtering maintain dimensional array flow temporally filtered estimate scene flow step procedure work presented flow tracklet filter velocity state incorporate raw flow measurements resolution occupancy reject flow measurements compute raw scene flow gt-1 process result dimensional array flow velocity process update flow tracklets current raw scene flow measurement location flow tracklet location observation flow tracklet location flow tracklets valid raw scene flow measurement background flow tracklets location dimensional array scene filter better estimate scene helps background filter static background raw scene flow flow maintain count observations flow tracklets increase age incorporate raw estimate scene flow proposed method kitti dataset sequences raw data performance background filter curve background curve decision threshold occupancy construct grid centered vehicle resolution construct binary feature note feature vectors unknown space binary decision allows unknown space relative motion estimation window algorithm background filter evaluate performance background curve decision threshold foreground decision threshold classifier achieve accuracy foreground accuracy background filter achieve methods method raw scene flow evaluate raw scene flow ground truth scene flow kitti labeled tracklet data relative successive lidar computed scan matching compute error ground truth flow vector computed raw class count fraction observations fraction observations car cyclist pedestrian background labeled tracklets median error error table error statistics raw scene flow perform analysis class labeled tracklets kitti note background error background class car cyclist pedestrian labeled tracklets count median error error magnitude scene flow error magnitude scene flow error car cyclist fraction observations fraction observations table error statistics temporal scene flow measurements flow tracklets age perform analysis class labeled tracklets kitti temporal scene flow magnitude scene flow error magnitude scene flow error pedestrian background fraction observations magnitude scene flow error labeled kitti tracklets error raw scene flow labeled kitti vertical resolution occupancy note vertical evaluate temporal scene flow computed flow tracklets evaluate error tracklets allows filtered scene flow estimate age tracklet measured simply number scans tracklet age step sensor compute velocity error vector flow tracklet tracklets full error statistics table accuracy temporal scene flow tracklets time flow tracklet median error resolution occupancy sample temporal scene flow runtime performance key goal real-time proposed provide runtime analysis algorithm lidar data algorithm 100 achieve real-time full set kitti log sequences table find pipeline runtime iterative scene perform procedure columns background scene presented error class computed error statistics table find raw scene flow measurements generally number occupancy error resolution occupancy find compute scene flow objects assumption motion fraction observations fraction observations age age age age age age age age age age magnitude temporal scene flow error magnitude temporal scene flow error car cyclist fraction observations age age age age age fraction observations age age age age age magnitude temporal scene flow error magnitude temporal scene flow error pedestrian labeled kitti tracklets error temporal scene flow labeled kitti note measured velocity measured step occupancy grid background filter occupancy constancy iterative filtered temporal flow total runtime table runtime performance proposed total runtime runtime key sample temporal scene flow temporal scene flow perform procedure columns runtime increase 100 real-time performance number find input total runtime 100 real-time find estimate scene flow kitti produce raw measurements scene flow ground truth resolution occupancy static background background filter extract scene flow raw scene flow evaluate method kitti provide error sequences moving method occupancy grid directly rely point data filtered temporal scene flow allows accuracy estimate time filtering accurate estimates temporal scene algorithm obstacle accuracy temporal scene flow estimate performance full obstacle tracking state obstacle trackers report velocity errors accurate error errors free space obstacle trackers provide estimates point cloud method note presented temporal scene flow flow dynamic object flow tracklets moving assume accurate flow tracklets temporal flow estimates produce accurate key feature proposed method sensor occupancy grids easily measurements sensors camera environments sensor simply background filter occupancy constancy approaches autonomous vehicle prior methods estimate scene real-time method lidar data autonomous presented pipeline lidar data estimates temporal scene performance algorithm kitti better current state algorithm real-time mobile estimation dynamic obstacle real-time tracking based vehicle detection tracking autonomous object detection tracking ieee tracking dynamic objects sensors autonomous driving detection tracking moving objects ieee object tracking ieee kitti optical iterative image application scene flow autonomous ieee scene flow estimation rigid scene rigid scene ieee framework real-time scene ieee tracking ieee scene flow lidar tracking dynamic environments environment ieee tracking driving environment occupancy ieee resolution maps ieee framework based computer systems laser ieee detection tracking dynamic objects learning motion
joint estimation structure geometry stereo sequences image variational method estimation dense scene flow structure stereo contrast existing approaches rely calibrated camera assume camera estimation structure propose joint energy functional spatial temporal image pairs unknown stereo introduce normalisation image stereo constraints deviations model assumptions interpreted geometrical separate regularisation experiments calibrated uncalibrated data demonstrate performance outperform techniques rectified case explicit computer motion capture recover displacement field scene flow real motion optical flow projection motion image plane required scene flow computed estimating scene structure contrast structure scene flow scene estimating scene stereo sequences required views time existing scene flow stereo motion rely computation scene flow structure quality estimation motion estimation spatial temporal dependencies image sequence methods solve scene flow structure variational approaches techniques problem space based optical flow computation optical flow eccv heidelberg joint estimation structure geometry stereo sequences methods assume calibrated order general stereo explicit jointly estimate scene scene structure stereo paper propose variational scene flow method uncalibrated stereo spatial temporal stereo pairs global energy functional estimating unknown stereo geometry consecutive time camera method allows recover dense scene structure dense scene flow scale additional required handle large introduce notation linearised notation allows normalise constraints deviations model interpreted geometrical propose regularisation strategy discontinuities displacement fields motion experiments demonstrate performance method techniques rectified context scene flow work methods jointly compute spatial temporal motion fields method huguet devernay data constraints joint smoothness term displacement separate treatment smoothness term proposed wedel estimation structure motion separate smoothness term accurate work joint regularisation previous approaches based rectified sequences consider constraint methods displacements terms image techniques work techniques include methods based errors space nonlinear fields methods rely previous projection context optical flow work valgaerts jointly estimating dense displacements stereo second normalisation data constraints penalise geometrically approach scene flow data stereo paper derive variational model uncalibrated computation constraint normalisation alternating minimisation proposed paper valgaerts scene flow model uncalibrated stereo sequences consider classical case consecutive image pairs stereo left image g1l image g1r time left image g2l image g2r time denotes image assume sequence fixed stereo fundamental matrix epipolar geometry stereo pairs time occlusion score o1r g1r optical flow g2r g1l time stereo flow wst left optical flow g2l time second stereo flow wst occlusion score o2l wst occlusion score o2r correspondences frames stereo sequence contrast previous variational methods rectified stereo sequence method general stereo geometry unknown fundamental stereo correspondences form disparity displacement field stereo consider correspondences optical flows consecutive frames camera stereo flows left frame time dependencies correspondences unknown respect reference image g1l stereo flow wst vst left optical flow flow interpreted change optical flow change stereo fundamental matrix points corresponding epipolar fact matrix defined scale camera fundamental matrix recover projection matrices left image sequence stereo flow time matrices reference image point scale camera reconstruction time scene flow left optical flow flow change joint estimation structure geometry stereo sequences joint computation structure wst propose global energy functional spatial temporal views geometric functional form data term assumption image constant epipolar term stereo views unknown epipolar smoothness term solution data constraints derive constraints model images reference assume brightness corresponding image points constant frames constraints ed1 g1l ed2 wst g1r wst ed3 g1l ed4 wst g2l terms optical flow constraint time terms stereo consecutive time penalise constraints optical flow stereo occur function proposed final model include gradient constancy assumption rgb colour term ed1 g2l g1l g2l g1l denotes spatial gradient three rgb colour constraints ed2 ed3 ed4 occlusion scores order handle scene occluded motion change camera additionally introduce occlusion score o1r points reference image valgaerts g1l g1r fundamental matrix projection matrices o1r determined reconstruction time image plane points optical scores o2l o2r image pairs g2l g2r determined time data terms occlusion scores constancy assumptions final data term o2l ed1 o1r o2r ed2 o1r ed3 o2l o2r ed4 term occlusion scores images occur data points g2r occluded g1r g2l reference epipolar constraints model geometric left images stereo pairs g1r g2r introduce terms unknown flows fundamental matrix epipolar constraints ee1 ee2 wst denotes terms ee1 ee2 constraints penalise deviations point epipolar estimation respect epipolar term second epipolar constraint linear wst quadratic respect left optical flow minimisation corresponding energy linear expression flows propose introduce epipolar constraint term ee2 wst wst additional term required weights final epipolar term ee1 ee2 solution additionally constraint frob fundamental matrix proposed joint estimation structure geometry stereo sequences smoothness constraints design smoothness problem remaining terms solution discontinuities wst joint smoothness assumption general case flow stereo discontinuities propose separate flow function defined total regularisation smoothness term weights smoothness assumptions three displacement linearisation normalisation epipolar smoothness terms energy functional unknown flows arguments data strategy problem computation fixed point euler-lagrange equations model level second strategy corresponding energy level wst total energy terms increments dvf dvst dwd dvd dwa dva allows introduce tensor notation energy functional flow increments normalisation strategy deviations model assumptions geometric linearisation data term differential form data term data constraint expression order expression respect increments g2r dwst dwd g1r wst g1r g1r g1r g2r g2r dust dud g2r dvst dvd valgaerts terms g2z g2r g1r wst g2rx g2r wst g2xz g2z g2ry g2r g2yz g2z linearised term g2rx g2ry dvf g2xz dust g2yz dvst g2rx dud g2ry dvd g2z defined g2ry g2xz g2yz g2rx g2ry g2z dvf dust dvst dud dvd classical optical flow constraint argument quadratic form ed2 matrix motion tensor notation optical flow estimation scene flow linearisation three remaining data constraints scene flow tensors dependencies tensor gradient constancy assumption rgb colour images weighted corresponding tensors treatment epipolar term epipolar term wst dwst linear case data terms vector dvst argument epipolar term quadratic form ee1 corresponding epipolar tensor size defined epipolar distance point respect flow increments second epipolar term differential variant argument dwst dwd dwa dwa additionally terms fundamental matrix treatment left flow joint estimation structure geometry stereo sequences required epipolar constraint occur left image terms expression linear second epipolar term ee2 dwa defined dust dud dvf dvst dvd dust dud dva dvst dvd dva dvf case epipolar epipolar tensors epipolar constraint normalisation demonstrate linearised brightness constancy assumption optical flow interpreted geometrically weighted distance estimated flow optical flow brightness constancy constraint weighted distance scene flow hyperplane distance hyperplane normalise constraint magnitude hyperplane vector g2ry g2xz g2yz g2rx g2ry explicit penalise distance constraint distance scale magnitude large image corresponding normalised quadratic form constant denotes normalised normalisation strategy remaining data gradient constancy colour images normalisation scene flow tensors epipolar tensors well geometrical error measure computer distance epipolar derive normalisation factor epipolar factor normalised epipolar tensors valgaerts minimisation solution terms differential form energy level dwst dwd o2l o1r o2r o1r o2l o2r dwst frob energy flow increments arguments order constraint frob method lagrangian dwst dwd dwa dwst dwd lagrangian vector alternating minimisation lagrangian respect flow increments corresponding euler-lagrange nonlinear equations function solve based nonlinear point factor images reference image based lagrangian respect fundamental matrix eigenvalue problem nonlinear normalisation weights solve eigenvalue problem total method weights arguments fixed point minimisation fundamental matrix dense correspondences stereo alternating computation flow increments fundamental matrix euler-lagrange equations estimate fundamental computed fundamental matrix eigenvalue camera matrices dense scene reconstruction occlusion euler-lagrange equations occlusion scores compute epipolar joint estimation structure geometry stereo sequences table evaluation methods rectified sphere rmse aae vst method method wedel ground truth wedel huguet devernay wedel method experiments performance method synthetic stereo sequences ground truth real quality compute error rmse scene flow optical flow stereo flow vst well error aae optical quality measure fundamental matrix error determined estimated fundamental matrix large correspondences ground truth fundamental matrix epipolar distance points matrices measure experiment consider synthetic sphere sequence huguet devernay images sphere fact sequence case stereo existing additionally estimate large stereo displacements variational context vst method large variant optical flow constraint normalisation belief huguet rectified table variational methods huguet devernay wedel errors computed rmse aae outperform approaches scene methods rectified rmse method wedel vst fact stereo correspondences occluded accuracy estimated scene flow ground truth stereo performance method accurate estimation stereo precision valgaerts second experiment performance general stereo synthetic sequence frames ground truth previous sphere plane demonstrate design model variant joint regularisation flows include constraint model normalisation separate table errors computed image aae defined flow computed flow fields occlusion reconstruction scene estimated displacements ground truth precision stereo general sphere sequence size left frame time ground truth left optical stereo flow flow colour brightness magnitude colour left frame second time estimated left optical stereo flow flow estimated occlusion scores o1r o2r estimated scene estimated scene joint estimation structure geometry stereo sequences table evaluation method general sphere sequence rmse vst joint regularisation joint regularisation normalisation separate regularisation normalisation method real sequences size left frames consecutive time closing left frames consecutive time reconstruction scene flow magnitude closing experiment uncalibrated stereo sequences performance method real sequences closing structure motion well additional real valgaerts general approach dense estimation scene scene structure geometry uncalibrated stereo classical case stereo epipolar constraints joint energy functional data smoothness introduce tensor notation allows normalise data stereo constraints geometrically separate smoothness terms handle flow discontinuities evaluation proposed approach general existing methods stereo outperform techniques rectified stereo geometry estimated precision real data scene structure motion determined joint computation structure international large displacement optical ieee computer society conference computer vision pattern ieee computer society accuracy flow estimation based eccv heidelberg motion accuracy international conference computer ieee computer society scene capture international journal computer vision computer design space dense accurate computer vision heidelberg geometry cambridge belief international journal computer vision dense motion capture ieee computer society conference computer vision pattern ieee computer society joint estimation structure geometry stereo sequences geometry computer cambridge cambridge optical variational method scene flow estimation stereo international conference computer ieee computer society dense motion disparity estimation belief heidelberg computer scene image international journal computer vision fundamental international journal computer vision dense estimation optical flow ieee image joint international conference pattern joint disparity motion field estimation image international conference pattern stereo reconstruction scene flow estimation global international journal computer vision nonlinear total based variational model joint fundamental matrix optical heidelberg scene ieee pattern dense scene flow dense stereo eccv heidelberg evaluation approach scene flow motion heidelberg scene flow structure ieee computer society conference computer vision pattern ieee computer society heidelberg
ieee transactions pattern analysis machine march 2005 three-dimensional scene flow ieee ieee optical flow two-dimensional motion points scene flow three-dimensional motion points fundamental optical flow normal flow computed directly image form smoothing fundamental scene cameras image computing scene perform regularization images perform regularization surface object compute scene flow regularization describe three computing scene flow optical flows scene structure inconsistencies multiple optical three-dimensional dense nonrigid optical brightness constancy normal three-dimensional normal fundamental properties scene scene imaged video optical flow motion optical flow motion field image projection three-dimensional motion points scene scene motion dense three-dimensional vector field defined point surface optical three-dimensional motion field scene flow computing three-dimensional motion scene computer vision scene three-dimensional scene structure motion computed single monocular video sequence scene algorithms nonrigid motion single camera form model assumption motion motion three-dimensional nonrigid motion monocular view monocular nonrigid motion robotics carnegie mellon robotics number 2005 ieee ieee computer common approach three-dimensional motion multiple cameras stereo motion approach algorithms assume scene consider nonrigid algorithm dense second model generalization monocular optical flow normal flow computed directly image form smoothing fundamental scene cameras image compute scene flow directly image smoothing regularization points result holds lambertian reflectance model generalization common reflectance compute dense scene perform regularization images perform regularization surface object scene flow scene flow scene scene flow algorithms regularization image describe three algorithms compute scene flow optical flows assuming scene shape algorithm single multiple algorithm scene structure inconsistencies multiple optical three algorithms consider moving surface imaged fixed camera projection matrix image sequence iit iit captured camera geometry describe brightness constancy optical camera surface geometry relationship point surface image projection camera equations ray fixed time differential relationship jacobian matrix three jacobian matrix differential change projected image change expression function jacobian relationship change point surface image camera ieee transactions pattern analysis machine march 2005 optical flow algorithms dui riit riit gradient optical instantaneous rate change image iit iit depends shape surface illumination structure three-dimensional constant common assumption holds illumination constant surface normal change assuming optical flow constraint riit dui nonrigid surface moving respect fixed system normal surface surface lambertian albedo illumination camera fixed projection matrix image sequence iit iit scene flow instantaneous motion point surface single point optical flow projection scene flow brightness constancy large number differential algorithms estimate optical flow note constraint optical flow direction image gradient normal optical dui illumination surface suppose illumination radiance light scene radiance depends scene point direction time radiance function version function radiance light point surface time radiance vector surface radiance compute dense optical form regularization smoothing optical flow regularization three-dimensional scene flow light surface surface normal assume surface lambertian albedo point camera image iit radiance point image image scene radiance iit optical flow two-dimensional motion field scene flow three-dimensional flow field motion point scene point instantaneous scene flow image point camera optical flow projection scene flow image dui jacobian equation optical flow computed scene flow scene flow multiple normal constant depends image plane pixel point two-dimensional optical flow suppose image point camera optical flow motion image plane camera point surface assume albedo assume normal optical flow computed normal flows cameras estimate scene flow gradient constraint equation riit note assumption lambertian assumption optical flow constraint ieee transactions pattern analysis machine march 2005 linear constraint three unknown scene flow estimate scene flow directly three respect riit expression camera depends properties scene surface albedo scene structure illumination number equation equations point result compute scene flow point form regularization lambertian holds reflectance function generalization lambertian model lambertian model model generalization lambertian model result estimate directly normal flow perform regularization compute scene surface object scene image computing optical carceroni brightness constancy equation motion directly input algorithm surface consider second assume optical flow computed camera compute scene describe three optical flows single scene multiple scene multiple unknown scene depth surface single camera estimate scene flow rate change depth second depth multiple cameras estimate rate change depth single camera scene flow multiple scene geometry multiple optical volume geometry scene voxel voxel coloring algorithm voxel projected images consistency voxel coloring suppose recovered scene geometry number voxels voxel coloring voxel compute set cameras equation optical flow projection scene flow assuming optical flow linear three scene cameras point scene flow optical flows estimate scene suppose cameras set system equations single scene geometry scene flow computed single optical computing scene flow note depends surface expression respect time dui equation motion point projection scene flow plane surface instantaneous motion image plane optical flow scene jacobian second scene flow three-dimensional motion point scene imaged fixed instantaneous motion ray magnitude rate change defined scene surface gradient constant system equations voxel camera scene flow optical note cameras set properties scene flow suppose recovered scene flow voxel note defined voxel model time voxel model time flowed time voxel model time consider voxel time flow voxel time voxel time voxel time flows ieee transactions pattern analysis machine march 2005 input scene flow algorithm image set video captured images dancer left properties voxel model time flowed time voxel model time input images horizontal vertical optical flow computed version algorithm pixels indicate flow horizontal vertical flow scene shape voxel model flowed properties scene flow voxels flow properties geometry consider dynamic dancer captured sequence cameras images time image captured images dancer left optical flow computed pair version algorithm optical flow algorithms pair horizontal vertical optical flow computed pair pixels indicate motion pixels indicate motion left scene flow computed voxel voxel scene scene flow dense flow motion vector computed voxel motion voxels dancer flow vectors notice motion dancer notice flow field left optical flow noisy scene flow multiple cameras scene flow noisy input optical computing shape inconsistencies multiple optical flows voxel coloring scene shape consistency pixel image voxel coloring voxel voxels pixels voxel ieee transactions pattern analysis machine march 2005 scene flow computed model notice motion dancer shape surface image optical flow point camera cameras reconstruction approach single large magnitude flow equations image point vectors plane scene flow form measure vectors computing direction ray pixel unknown constant depends instantaneous properties surface holds constraint scene dui dui algorithm large magnitude measure measure number scene three-dimensional voxel coloring algorithm compute coplanarity measure algorithm flow algorithm vector plane defined camera optical flow algorithm sequence computed scene flow voxel three volume coplanarity measure scene structure recovered algorithm structure scene recovered computer digital library volume ieee transactions pattern analysis machine march 2005 optical flow measure coplanarity computed voxel three volume computer digital library structure scene reconstruction algorithm structure scene best reconstruction scene dense nonrigid fundamental dynamic scene flow form smoothing algorithms compute scene flow single multiple regularization image optical flow carceroni perform regularization scene consistency scene flow projected multiple shape work scene flow algorithms compute scene algorithms compute scene flow structure work scene flow view nonrigid best cameras best scene nonrigid motion estimation ieee pattern analysis machine ieee pattern analysis machine nonrigid computer image nonrigid motion ieee pattern analysis machine scene reconstruction voxel computer optical shape nonrigid view dynamic robotics carnegie mellon view scene ieee computer image ieee pattern analysis machine motion estimation sequence noisy stereo ieee pattern analysis machine computing digital library work function linear ieee computer vision pattern optical flow computer estimation optical computer carceroni scene video nonrigid shape ieee computer moving computer multiple video robotics carnegie mellon reconstruction dynamic structure stereo machine vision image stereo motion structure estimation stereo computer vision image
three-dimensional scene flow algorithm non-rigid multiple normal flow estimate scene flow form scene flow efficient robust model scene estimate structure time efficient computation structure time estimate robust computation structure structure stereo scene flow dynamic rendering interpretation modeling scene flow three-dimensional motion field points optical flow motion field points optical flow projection scene flow image plane framework computation non-rigid scene flow optical approach leads linear algorithms task three major complete instantaneous knowledge scene knowledge knowledge scene multiple normal flow estimate dense scene flow directly form optical flow motion field image projection three-dimensional motion completely points scene scene motion dense three-dimensional vector field point surface optical three-dimensional motion field scene framework computation non-rigid scene flow directly optical approach leads efficient linear algorithms task three major complete instantaneous knowledge structure including surface normals rates change depth optical flow compute scene knowledge stereo optical flows compute scene knowledge optical flows reconstruction algorithm estimate scene structure scene computing three-dimensional motion scene task scene cameras three-dimensional scene structure relative motion computed single monocular sequence structure-from-motion scene structure-from-motion algorithms structure-from-motion non-rigid motion estimated single camera assumptions assumptions form model motion motion recovery three-dimensional nonrigid motion monocular monocular non-rigid motion assumptions compute approach motion multiple cameras stereo motion approach motion-stereo algorithms assume scene optical flow analysis camera motion-stereo consider non-rigid including algorithm temporal dense model monocular based motion-stereo surface illumination flux image camera center projection consider moving surface imaged fixed camera projection matrix figure image sequence camera relative camera surface illumination surface image figure non-rigid surface moving respect fixed system normal surface surface lambertian albedo illumination flux ith camera fixed represented camera matrix image sequence relationship point surface image camera relative camera surface knowledge equations requires ray surface represented jacobian matrix jacobian matrix change projected image change expression function differentiating equations jacobian describes relationship small change point surface image equations point surface image camera camera inverse describes relationship small change point image camera point scene image map scene inverse jacobian computed knowledge surface inverse jacobian estimated solution linear equation constraint small change small change projected image original change equation constraint small change change corresponding point linear equations equations surface normal equations ray camera center projection tangent illumination surface point irradiance illumination flux direction time represented irradiance function described function irradiance light point surface time irradiance vector surface irradiance visible structure three-dimensional constant illumination surface normal change lambertian normal flow gradient constraint optical flow algorithms rii light surface surface normal assume surface lambertian albedo point visible ith intensity image proportional point image image irradiance proportional scene large number algorithms estimating optical flow three-dimensional scene flow constant depends image image pixel surface point equations optical flow describes instantaneous motion field scene flow motion three-dimensional flow field point analysis fixed time suppose point moving image point camera camera rate change image motion motion point dui projection flow field optical point assume albedo assume optical flow suppose point surface image point camera relationship knowledge surface depends surface differentiating expression respect time dui dui moving surface properties albedo points optical flow algorithms gradient optical instantaneous rate change image intensity term depends shape surface illumination rii dui equation motion point projection scene flow plane tangent surface passing instantaneous image plane optical flow scene inverse jacobian term scene flow three-dimensional motion point scene imaged fixed instantaneous motion ray corresponding magnitude rate change depth surface presented appendix three major computing scene scene figure sequence scene scene flow sequence presented three-dimensional completely instantaneous structure including surface depth temporal rate change depth knowledge stereo depth surface normals temporal rates change depth completely scene leads estimating scene knowledge scene structure requires optical result linear equations compute scene algorithms three flow computed multiple image image sequence figure surface completely surface gradient computed estimated inverse jacobian set linear equations equations inverse scene flow equation optical flow figure optical flows camera figure darker motion left assume surface computing inverse surface point image ray optical flow scene flow computed points scene visible multiple optical flows multiple cameras flow recovered change depth map pixel parts surface time motion surface large relative pixel flow consider scene sequence figure scene flow time scene flow computed depth maps model volumetric multiple computed described optical flow figure computed optical flow temporal rate change depth estimated independently computed volumetric figure result computing scene flow visible set points original points light points original points scene flow darker points represent prediction complete knowledge surface dui computing requires temporal surface depth described appendix complete knowledge scene structure compute scene flow optical rate change depth map corresponding scene optical flow projected tangent plane passing rate change depth map component ray passing scene point center projection original scene points flowed location points figure magnitude scene flow displayed points model locations depth maps cameras magnitude scene flow displayed motion arm motion figure computed scene original points model locations points flowed points form prediction model time model time shape figure motion player player motion major motion player left player motion left flow recovered points arm arm relative completely rate change depth points flow estimate image correspondences major structure scene completely correspondences correspondences compute depth depth maps estimate surface normals temporal rates image based modeling rendering consider scene flow image based modeling temporal dynamic surface completely directly consider linear scene optical described jacobian equation set equations linear system equations equations system estimate scene system equations point camera solution scene flow optical algorithm sequence surface normal rate change depth optical flows optical flows point compute scene flow figure magnitude computed scene computed flows displayed intensity image correspondences camera depth scene flow computed set locations depth maps cameras motion original scene points flowed location points flowed location points independently computed model figure point light set points points flowed three-dimensional motion player figure flowed points independently computed model time flowed points model time motion left arm player displayed figure light points represent model estimated scene flow darker flowed points figure light computed points represent model stereo volumetric points scene flow moving scene flow structure scene time riui constraint scene dui riui riui vector perpendicular plane camera center optical flow image point plane perpendicular scene flow form measure computing knowledge surface point equation camera equation cameras reconstruction approach single large magnitude flow equations solution equation written constant depends instantaneous properties surface dui riui riui ray pixel appendix algorithm large magnitude measure measure number scene three-dimensional algorithm compute visibility visibility coplanarity measure algorithm visibility algorithm figure cameras estimate scene flow directly three differentiating equation respect rii figure rendering coplanarity measure scene structure recovered algorithm structure scene parts recovered camera presented figure computed optical flow measure coplanarity computed figure rendering structure scene reconstruction algorithm structure scene parts expression camera depends properties scene surface albedo scene structure illumination equation number equation equations point result compute scene flow independently point form result estimate directly normal three-dimensional normal flow constraint optical flow dui vector normal flow tangent normal flow component direction image gradient tangent flow component perpendicular normal magnitude normal flow estimated directly equation three-dimensional scene flow dynamic prediction robust stereo scene interpretation rendering presented framework computing scene flow optical instantaneous properties scene framework knowledge structure computed independently time algorithms computing scene flow optical rii dui estimating tangent flow form estimate complete optical flow estimation tangent flow major normal flows cameras estimate scene flow form normal flow constraint equation term motion point scene imaged pixel suppose depth surface ith camera point written function camera matrix written computing rii center projection camera direction ray pixel direction camera matrix linear constraint scene flow riui surface camera center projection figure camera matrix direction ray pixel direction camera expression point expression function vector points camera riui magnitude proportional rate change depth map direction ray center projection function non-rigid linear cvpr optical flow approach cvpr independently moving reconstruction dynamic structure stereo motion structure estimation stereo shape nonrigid motion estimation ieee dense nonrigid recovery nonrigid motion ieee recovery dynamic scene structure multiple image scene cvpr optical flow field approach motion analysis sequence stereo interpretation recovery 3-d shape nonrigid image ieee 3-d motion estimation sequence stereo ieee dynamic scene estimation 3-d ieee
scene flow estimation rigid motion prior approach scene flow realistic scenarios image motion observer motion rigid object dense estimation scene structure motion sequences views energy minimization smoothness total motion discontinuities viewing propose regularize local rigidity local rigidity constraint scene flow define smoothness term penalizes deviations rigidly moving experiments rigid motion prior flow error compared standard regularization data optical flow disparity stereo optical scene flow estimation depth prior scene flow priors smooth motion fields scene flow problem smoothness terms flow case demonstrate tendency motion discontinuities viewing flow image plane scene flow goal issue propose realistic prior model improve scene flow assume rigidly moving scenes regularize problem locally rigid motion regularization framework rigid motion local regions depth penalizes deviations locally weighting scheme motion discontinuities independently moving local rigidity prior global rigidity flow estimation image motion observer synthetic scenes demonstrate proposed rigidity prior avoids leads motion flow error average compared standard total variation real scenes rigidity prior scenarios disparity estimation stereo images optical flow estimation estimation scene flow dense scene flow estimation images points cameras intrinsics well relative goal scene flow estimate geometry motion shape reconstruction dense motion shape motion scene motion scene flow estimation solving stereo problem time optical flow problem estimate scene flow directly images ieee ieee work problem scene flow estimation dense estimation geometry motion work optical flow estimated independently camera flow field flow optical flow computation constraint flow fields images motion optical flow proposed approach estimated scene flow directly image fitting flow method set method multi-view motion proposed scene flow image stereo disparity depth motion estimation scene flow estimate optical flow disparity difference time smoothness term image smooth smooth motion gradients viewing direction projected work smoothness assume camera intrinsics scene object motion estimate scene flow relative orientation cameras image method orientation parameters scene approach work scene flow parametrized terms depth reference view well flow unknowns estimated optimization scene flow total model local rigidity large case note case key cameras total rigidity work recovered scene structure image approach depth dense scene flow directly image rigidity set penalize deviations local approach global rigidity priors flow estimation rigid scene optical flow penalize deviations flow well scenarios observer expected work well scenes independently moving local rigidity motion capture dense scene flow estimation surface scenes depth object rigidity prior work penalize deviations local rigidity flow field terms rigid motion parameters penalizing parameter penalizing changes coordinate system approach avoids issue directly penalizing deviations local rigidity reference coordinate local rigidity priors estimation scene scene flow framework goal depth motion field assume parametrized image reference assume setup cameras intrinsics relative orientation camera scene images stereo optical flow assume brightness constancy time penalize brightness differences data term data term spatial term regularization energy energy minimization optical flow setup approach scene parametrized image reference camera views projection assume cameras capture scene time cameras scene points time views time reference camera left figure scene flow image points observed estimate point motion data terms optical image points note denote surface points parametrized pixel depth reference camera time camera center reference system time point unknowns image plane camera point projected projection scene flow observed optical flow camera point setup image three terms increase robustness influence terms leads spatial standard increase robustness current estimate pixel camera three energy terms spatial term problem scene flow computation case brightness constancy system unknowns pixel image set flow vectors image gradients valid small pixel image regularization spatial term surface scenes assume scene piecewise data term data term standard brightness constancy penalizes brightness differences image scene point reconstructed depth difference standard brightness constancy images real-world scenes brightness constancy reduce influence function function view leads brightness constancy terms optical standard total variation prior standard define spatial term penalize gradients motion depth optical scene flow total variation reference camera views time leads terms stereo camera terms brightness constancy total variation work well optical flow regularizer data term scene flow large changes figure realistic motion discontinuities cameras scene motions fitting incorrect motion field avoids smoothness motion incorrect motion moving data weighting function pixels denote large pixels small small changes projected flow vectors regularizer tendency motion discontinuities reduce spatial term data weight local scene weighting function spatially varying weights key difference global rigidity priors rigidity rigid motion valid small estimated flow time motion non-local total variation note motion residual constraint regions weights well robust function reduce motion discontinuities piecewise rigid flow regularizer time estimate rigid motion measure deviation spatially rigidity prior vdsc rigidity prior scenes rigid piecewise rigidity expected better prior define spatial term denote regions reference non-rigid motion residual vdsc total variation term flow rigidity prior denotes non-rigid motion residual flow point reference robust function reduce influence contrast rigid motion priors optical assume rigid scene propose local rigidity motion small scene locally rigid denote local region point rigid motion component flow define motion residual deviation rigid motion flow vectors rigid motion component flow patch term denotes matrix local weighting small motions rigid motion component projection rigid motion leads non-rigid motion residual vdsc key proposed rigidity prior deviations locally rigid motion rigid motion contrast projection rigid motion well small rigid motion well translation non-local regularizer rotation rotation vdsc surface point pixel computed current depth estimate local non-rigid motion residual vdsc weights increase robustness rigid motion fitting errors robust function error rigid motion motion experiments synthetic well real data rigidity demonstrate proposed local rigidity total variation data term optimization influence varying data images optical flow estimation image gradients valid small energy minimization scheme better local optimization framework energy parameters set regularization parameter current image set image gradients images computed non-rigid motion residual pixels region synthetic scenes depth real images depth dense stereo algorithm computed current error observed evaluation optical flow stereo directly evaluation scene flow deviation errors issue flow fields measure error recovered surface motion errors projected motion field scene depth normalized error difference point reference solving projection rigid motion matrix weighting matrix robustness rigid motion fitting points rigid spatially varying weight similarity measure surface point surface center region weights patch normalized define similarity measure flow depth field differences optical flow field disparity pixel patch center similarity penalizes disparity similarity optical flow set weight weighting similarity note minimization scene flow rigid motion table proposed local rigidity prior motion field standard total variation error averaged three scene errors averaged rot rig txyz rig rig aaew nrmsw aae figure three scenes rot rotation txyz coordinate translation flow vectors normalized point error nrmsw flow errors motion vectors proposed based differences flow average angular error flow aaew errors projection motion field error optical flow average angular error aae average error total variation rigid prior rigid motion prior synthetic scenes ground scene images rigid scenes observed note motion prior method scene experiments proposed rigidity standard total variation terms error table terms projected motion errors contrast aae improve differences projected areas small regularize depth total depth errors approach error estimated motion rigid motion prior nrmsw angular error aaew rotation translation averaged rigid motion prior motion estimation error table evaluation errors occlusion areas discontinuities regularizer errors smooth occlusion areas small rigid motion prior improve errors occlusion areas scene flow scenes rigidity prior motion errors regularization exhibits flow difference viewing direction incorrect observed flow txyz exhibits table expected error areas discontinuities rigid motion prior exhibits aaew rot rig txyz rig rig nrmsw txyz rigidity figure scenes scenarios txyz ground rigidity flow field normalized coordinate flow tendency regularizer motion difference viewing direction data error txyz scene error image plane small motion rigid motion prior scene scene flow algorithm scene flow synthetic sequence images independently table errors flow vectors disparity flow errors method depth flow rigidity prior leads depth better moving large regions well moving flow method capture non-rigid motion motion left moving direction motion vectors well sequence scene non-rigid motion well large occlusion real data algorithm real real-world ground three scene stereo three three independently moving left note stereo shape motions recovered reconstruction scene images rigidly moving table errors sequence flow disparity rig standard smoothness priors motion estimation scene flow method scene flow computation penalizing deviations local rigidity motion energy minimization experiments motion estimation error compared standard total variation real-world scenes work improve weighting scheme rigidity method sequences rigidity constraint three-dimensional motion structure optical flow moving rigidity figure data image reconstructed flow vectors figure data images three cameras time reference camera images denotes estimated flow estimated scene depth reconstructed surface motion image parameter scene flow view variational optical flow estimation based multi-view scene capture non-rigid shape solving three-dimensional motion dense motion capture image stereo ieee variational method scene flow estimation stereo variational optical multi-view stereo reconstruction scene flow estimation global motion field estimation stereo image sequences optical flow estimation prior motion variational model matrix optical estimation structure geometry stereo differences stereo motion synthetic stereo three-dimensional scene regularization algorithm optical motion dense scene flow dense stereo motion estimation non-local total variation based approach optical
ieee computer piecewise rigid scene flow computer estimating dense scene flow stereo sequences challenging classical disparity optical flow existing novel model scene collection rigidly local scene flow estimation amounts jointly estimating normal rigid motion parameters plane proposed energy data term segmentation optimization initial estimate shape motion parameters segments proposal set moving assignment shape motion parameters moving planes model image including challenging kitti performance exceeding scene flow better motion estimates dedicated optical flow estimation prior geometry regularization locally rigid motion common general stereo optical flow existing scene flow techniques limited model scene collection planar rigid prior work stereo scenes regions motion scene flow plane rigid motion initial estimation superpixel segmentation reference segments generate large candidate planes object rigid scene flow estimation assigns pixel segment segment rigidly moving superpixels lead flow initial segmentation depth motion address pixel assignment pixels superpixel include occlusion reasoning segment pixel novel scene flow approach based piecewise rigidly moving including regularization regions well explicit occlusion formulate energy inference report scene flow estimates experiments realistic data proposed approach outperforms three scene flow best method additional stereo scene flow estimation estimating dense surface shape well dense motion field views scene time steps include motion motion scene flow classical computer dense stereo matching dense optical flow stereo optical flow estimation existing scene flow techniques limited additional stereo motion sequences scene flow outperforms dedicated stereo optical flow techniques respective stereo motion scene flow ieee figure scene jointly estimated motion superpixel processing steps piecewise rigid scene flow estimated motion flow occlusion areas outperforms dedicated stereo optical flow algorithms challenging respective work term estimate dense geometry dense motion field image estimation independent optical flow fields estimated views flow field stereo disparity time optical flow reference view disparity differences view approach smooth flow fields multiple approaches regularizer encourages smooth smooth scene estimate geometry flow variational scene flow depth motion vector reference estimate parameters jointly optical flow method approach total prior motion field realistic encourage locally rigid local motion estimation motion explicit surface optical flow approach flow field rigid motion assuming camera pose dominant rigid motion estimate camera pose scene common previous approaches scene flow penalize deviations robust stereo disparity optical explicit modeling segmentation estimate disparity assuming scene planar superpixels parameterizing segment-based stereo additionally penalize deviations method epipolar optical flow epipolar motion estimate general motion occlusion optical flow parameterizing motion segment perform occlusion estimating consider motion fields assigned segments independent optimization based fusion proposals optical flow estimation optimization scene piecewise rigid model scene flow approaches scene novel model scene collection piecewise planar moves rigidly estimated energy define energy function assigns pixel segment segment geometry motion allows estimate scene flow depth pixel reference segmentation scene representation formulate model classical case image pairs stereo formulate scene flow estimation minimizing figure data terms regularization common correspondence data term ensures corresponding appearance regularization term encourages piecewise smooth geometry boundary term additionally define three terms views time steps scene reference il0 camera time assume reference camera matrix matrix image time moving plane scene normal vector matrix vector note plane visible reference image denote plane normal distance normal note assume pixels moving plane parameterizing scene moving planes allows pixel assigned plane images corresponding energy define set segment large set rigidly moving proposal assignment pixels planar geometry motion segment note model based segmentation segments accurate scene flow ensure depth flow estimation boundary cost segment boundaries depth motion data term data term corresponding points images amounts total constraints pixel stereo constraints time steps optical flow constraints representation rigidly moving planes pixels reference view il0 define reference view denote moving plane pixel define optical appearance constraints time pil stereo constraints views pil function encourage brightness brightness truncated include robust census data term sum terms shape motion regularization regularization terms encourage piecewise smooth well piecewise smooth motion segment assigned rigidly moving segment consider segment assume adjacent pixels assigned moving planes boundary consider model estimate depth scene flow vector pixel reference il0 assume set rigidly moving proposal planes mapping il0 assigns pixel il0 segment mapping segment rigidly moving plane assume boundary pixels image endpoints points pixel boundaries general coincide penalize boundary denote endpoints planes distance boundary distances surface distance endpoints distance endpoints respective normals difference normals define distance function boundary distance shape regularizer defined boundary normal segmentation regularization data term previous regularization term consider motion geometry moving plane assigned segment pixel assigned segmentation encourage segment regularization term approach encourages smooth segments boundaries coincide image energy defined pil neighboring pixels reference image pixels denotes penalty note sum neighboring adjacent pixels segment motion field distances adjacent motion well differences normals term segment coincide large image estimated set denotes term ensures pixel assigned segments seed pixels set segments scene piecewise piecewise rigid segment considered pixel set seed points segment pixel segment initial well pixels respective initial inference perform inference model minimizing energy segmentation rigid motion mapping initial segmentation reference minimize segmentation energy amounts approach seed points ensure segments update depth motion minimizing energy segment-to-plane assuming segmentation fixed segmentation regularization consider data regularizer sum shape motion truncated thresholds note proposed regularizer regularization image plane regularizer distances disparity differences optical flow disparity difference term shape motion regularization segment neighboring pixels segment assigned moving regularization energy computing penalty endpoints segment boundary efficient computing penalty boundary superpixel segments optimization set proposal planes motion fusion moves update segmentation assuming fixed note segmentation minimize ensures pixels assigned segment pixels seed optimization local energy view occlusion apply occlusion handling view pairs formulate data term assume consider view pairs data apply fixed occlusion penalty pixel reference occluded view case corresponding pixel apply data penalty note consider pixels visible plane occlusion reasoning inference binary optimization assume segment-to-plane mapping update per-pixel segmentation address segment-to-plane mapping denote pixel current candidate data term occlusion reasoning function pil proposal perform inference depth motion proposal set planes rigid generate scene flow algorithms stereo optical flow algorithms plane superpixel estimate rigid motion flow fitting robust large depth motion estimates superpixels surface motion address minimizing transfer generate initial minimizing transfer error efficient rigidly moving proposal planes robust transfer error locally proposal considered segments fusion moves efficient binary vector pixel data penalty current data penalty candidate consider pixel occluded binary segment assignment pixel multiple occlusion segment assignment set pairs pixel pixel data term pil occlusion handling data term defined occlusion pixel scene representation defined allows explicit occlusion case scene views scene time pixel occluded difference data penalty occlusion cost denotes consider pixel case pixel pixels assigned segment lead data cost pixel case data penalty per-pixel penalty function terms update segment-to-plane mapping fixed segmentation perform inference segment computing case segments segment considered occluded rmse flow rmse disparity rmse scene flow table experiments proposed model realistic report scene approaches multiple moving independent object complex occlusion pose estimated scene flow processing segment-based scene occlusion handling assigns depth motion regions visible reference image segment-based occlusion model occlusion regions motion realistic per-pixel refinement improves object occlusion ensure robust assuming depth motion exceeding brightness truncated set scene flow scene flow methods consider synthetic scene independently method better dataset well well planar coincide initial stereo pairs video dataset scene flow estimation training dataset quantitative ground compare kitti dataset challenging optical flow scene flow pixel data set large exceeding pixels stereo pixels optical images lighting conditions high motion large regions image boundaries field view correspondence areas appearance address challenging lighting conditions census data cope well complex lighting distances census data set parameters fixed image cope areas visible stereo flow algorithms proposal pixels encourage scene flow estimate binary reference image pixel binary function penalize deviations boundaries image energy term pil kitti dataset quantitative evaluation method stereo pairs kitti dataset dataset images stereo optical flow stereo algorithms ground truth data table compare average kitti training ground truth error threshold deviations ground truth note benchmark methods dominant figure kitti disparity reference flow reference independently moving ground truth dataset better scene flow evaluation synthetic kitti evaluation pixels pixels report error thresholds three scene flow locally rigid scene flow independently stereo matching optical flow data total time baseline stereo optical flow published methods kitti baseline published benchmark steps well report based superpixels per-pixel refinement compare regularization image regularization thresholds lighting conditions challenging include occlusion modeling proposal experiments proposals stereo baseline technique denotes proposal set locally rigid scene flow additionally proposals estimating dominant motion proposal fitting technique baseline epipolar motion epipolar motion proposal minimize energy cope table refinement improves average regularization lead evaluation ground truth scene disparity optical note better quantitative evaluation scene flow additional occlusion reasoning improves motion estimates occluded performance stereo case occlusion reasoning limited limited independent motion optimization technique proposals proposal set large including scene flow techniques average additional average model kitti kitti current fitting proposal optimization per-pixel data term time method published approaches optical flow published methods performance epipolar cope independent object motion outperforms scene flow technique best general optical flow method best scene flow method optical flow algorithms error benchmark additional stereo modeling scene local regions corresponding rigidly moving planes lead joint geometry error threshold table average error kitti training error pixels ground truth threshold areas image motion proposed model accurate geometry motion boundaries initial allows occlusion method outperforms previous dense scene flow approaches challenging data dedicated stereo optical flow techniques respective scenes motion piecewise planar rigid scenes work method sequences image segmentation motion optical flow moving binary eccv scene flow view variational object stereo joint stereo matching object cvpr high optical flow estimation based eccv scene video shape scene flow estimation correspondence cvpr cvpr stereo processing matching variational method scene flow estimation stereo optimization optical flow cvpr variational optical accurate motion field estimation stereo image sequences eccv stereo variational stereo binary image motion explicit depth joint motion estimation segmentation complex scenes occlusion cvpr joint estimation geometry stereo eccv differences stereo motion synthetic stereo scene cvpr superpixels energy optimization eccv scene flow estimation rigid motion moving images ieee efficient dense scene flow dense stereo eccv approaches high performance video fields robust stereo eccv robust epipolar flow cvpr local computing eccv
view-consistent scene flow estimation multiple frames propose method dense scene flow stereo method depth motion field dynamic scene multiple consecutive frames temporal estimate consistent viewpoints frames observed scene collection planar patches consistent rigid motion constant patches energy function plane motion parameters view-consistent multi-frame improves scene flow presence robustness adverse imaging method achieves leading kitti flow scene flow dense surface geometry motion dynamic scene flow estimation images cameras time delivers depth motion optical flow stereo dynamic classical scene flow estimation form dense shape motion including motion video generation scene flow methods reconstruction consecutive stereo images reference shape motion vectors starting point work frames longer stereo video exploit longer time reference imaging images paper propose estimate depth eccv schindler consistency multiple frames scene flow estimation robust input left view time scene flow estimate left disparity reprojected flow motion longer time consistent views frame physically scenes temporal consistency constraints scene flow optical methods scene flow reconstruction constraints object valid key estimating scene flow views consistency estimating single reference image data consistent difficult imaging views improves robustness additionally allows accurate occlusion propose consistency time views single energy jointly solve reconstruction account independently frames reference views estimation temporal frames cost longer longer time intervals frames approach scene flow representation scene collection planar rigidly moving valid road scenes well view-consistent multi-frame reduces number patches simple geometry order time steps additionally assume motion view-consistent scene flow estimation multiple frames segment constant time assumption valid segment time intervals paper propose scene flow model arbitrary reference shape motion image time consistency reconstruction extend dense scene flow estimation time consistent segmentation scene prior constant velocity consistent energy corresponding inference handle number viewpoints number time method challenging kitti dataset real stereo flow two-frame scene flow fixed reference view proposed view-consistent estimation frames reduces average endpoint error improves kitti error metric evaluation full including occlusion method achieves best optical flow hard examples model robust work scene flow estimation motion optical flow estimated independently camera motion based variational estimation estimating stereo correspondence flow fields consistent depth motion estimating additionally allows changes relative stereo scene flow relative regularization scene flow depth motion improves reconstructed motion variation regularization motion field prior deviations local model scene collection planar moving rigidly representation multiple cameras work well allows consistency moving single reference temporal temporal smoothness multi-frame optical flow limited small schindler motion fields time steps encourages estimated allows inference current motion field current jointly three consecutive assuming constant motion assuming constant scene flow time allows constant velocity assumption soft constraints second order smoothness motion soft constraint simple temporal jointly estimate flow segmentation small number constant motion changes scene structure segments video motion temporal estimate motion motion shape pixels rigidly segments consistency segmentation contrast motion model small segments represent temporal consistency longer time intervals explicit model shape smoothness scene pixel geometry motion estimated changes flow longer motion stereo final optimization soft including two-frame optical flow scene flow estimating scene flow time tensor represent scene explicit video three motion work methods starting flow stereo work initial methods segmentation scene depth flow optical flow representation scene moving arbitrary reference views stereo form consistency form consistency disparities extend consistency scene flow images temporal view-consistent scene flow estimation multiple frames method formulation represent scene geometry motion collection piecewise planar move rigidly define scene flow estimation mapping pixels segments mapping planar geometry rigid motion mappings define geometry motion note spatial segmentation pixels moving plane form account well motion depth key formulation estimate scene flow reference views check consistency representation estimate robust allows occlusion segmentation consistent segmentation scene estimate scene flow extend rigid motion time assuming constant velocity moving model classical camera number cameras left camera time step image computing scene flow cameras reference view left camera time step canonical view coordinate canonical view evaluation assume camera left assume camera move camera moving plane defined rotation normal note assume motion parameters describe rigid motion time moving plane defined coordinate canonical assuming planes canonical define plane points transform moving plane views canonical respective normal normal rigid left camera time step points normal schindler data consistency spatial temporal pixel data occlusion normal case scene allows simple pixel corresponding views induced assigned moving homographies canonical view views moving plane lhr lhr homographies arbitrary view canonical view lhr lhr define scene flow estimation energy mappings pixel camera time segment set define super-pixel segmentation contrast segmentation reference rigidly moving plane mappings segment set moving define energy function term data term photo-consistency reference photo-consistency neighboring corresponding pixels consistent geometric regularization term smoothness motion geometry segment final term spatial segmentation describe model time extend multiple frames view-consistent data term compute scene flow views define data term check consistency scene flow view-consistent scene flow estimation multiple frames view time step consistency classical photo-consistency images corresponding pixel correspondence determined assigned moving plane pixel moving additionally corresponding pixel depth values induced respective moving planes note form occlusion longer reference estimate scene flow check consistency pixel location view view time time corresponding pixel location moving plane pixel corresponding pixel location view determined check geometric depth pixel camera image relative distance depth occlusions implausible geometric disparity values stereo case pixel view occluded second depth moving plane corresponding plane determined second check photo-consistency case fixed penalty occ depth moving plane implausible geometric corresponding plane point corresponding plane pixel occluded observed pixel penalize fixed penalty imp relative depth order aliasing induced pixel grid resolution proposal set moving penalize pixel moving fixed penalty oob case case pixels geometric robust transform neighborhood allows define data term consistency pixel view moving plane adjacent view occ imp oob mvp schindler data cost penalty mvp pixels geometric moving planes encourages corresponding segments views moving leading view-consistent penalize pixels moving classical occlusions set oob occ ensures small number corresponding graph penalize physically implausible set imp limited penalty note implausible final penalty plane pixels geometric correspondence set mvp oob allows deviations prior compute scene flow views pixels frames considered neighboring views note view considered data views scene flow shape motion regularization spatial regularization term piecewise geometry motion views segment pixels segment moving adjacent pixels assigned moving penalty defined distance points shared piecewise form shared robustness object motion distance robust cost assuming adjacent pixels moving planes define induced penalty vectors describe distance distance motion shared vectors view-consistent scene flow estimation multiple frames define distance moving plane induced motion current scene flow regularization delivers disparity geometry flow disparity time robustness achieved penalty function set geometry allows account image structure edge based tensor assuming image determined define compute scene flow regularization views define full spatial regularizer neighboring pixels respective image spatial segmentation regularization segmentation regularizer spatial define energy views term form encourages segment image tensor account edge second term segment canonical view respective seed point seed points key reduces time mapping limited set segments considered note second term canonical data term encourages views segmentation regularizer based energy compute reference schindler variation camera constant velocity scene observed moving camera camera images induced flow camera ego-motion multiple frame formulation frames spatial segmentation number data term assume motion constant velocity case time note assumption time extend data term homographies assuming constant velocity rotation achieved view coordinate achieved assuming constant constant velocity assumption motion stereo road surface motion time steps estimated relative respective camera coordinate small changes relative camera changes relative geometry motion extend formulation ego-motion time estimate relative ego-motion consecutive time steps computing homographies time motion induced moving plane representation relative rotation moving plane proposal canonical view relative ego-motion canonical view case computing frame left homographies ego-motion view-consistent scene flow estimation multiple frames optimization proposal generation piecewise rigid model energy motion inference steps fusion starting fixed segmentation moving plane segment set segmentation geometry motion initial segmentation super-pixel initial segmentation grid pixel edge points grid seed points optimization segmentation depth motion grid structure reduces aliasing segments segments large pixels segmentation term aliasing induced initial grid segmentation consistency constraint set mvp oob ensures proposals initial edge segment fusion move set moving plane stereo optical flow two-frame version leading initial proposal canonical view grid step multi-frame case proposals consecutive frame time time proposal proposals time steps proposals considered valid pixel neighborhood seed point canonical fusion move neighborhood views graph segments mapping moving plane proposals time pixels grid constraint ensures fusion consistency pixel penalize set local allows multiple image current time time solve solve images segments proposals time schindler example kitti training set consistent super-pixel segmentation data term normal photo-consistency occluded implausible evaluation evaluation parameters transform values representation model left consecutive frames assigned data term evaluation hard example kitti benchmark optical scene flow methods images presence location consistent views consistent consecutive method exploit consistent depth motion scene flow disparities flow vectors occluded standard error note achieved multi-frame consistency imaging consistent motion images views physically difficult outdoor scenes left input images disparities flow estimated method consecutive stereo ground truth examples adverse imaging model motion scenes occlusions difficult examples dataset example large scene hard single reference view-consistent scene flow estimation multiple frames hard example training input reconstructed scene reprojected disparity flow field left challenging examples input reconstructed scene reprojected disparity flow best kitti benchmark kitti dataset images stereo resolution cameras delivers ground kitti standard stereo optical flow challenging large stereo flow real outdoor scenes benchmark set images ground truth test set ground truth data stereo video multi-frame method method training test set evaluation training error average point error kitti pixels pixels ground images pixels estimation frames three frames frames proposals multiple frames multi-frame test proposals model optimization frames method schindler kitti metric flow vectors disparities pixels endpoint average endpoint error kitti training set flow kitti metric 5px 2px 3px 4px 5px stereo kitti metric 5px 2px 3px 4px 5px occ 2px 3px 4px aep 2px 3px 4px aep frames single reference view version moving single reference view view-consistent estimation standard kitti metric error stereo aep presence including proposals frame improves moving three frames note occluded full images small number frame best reduces kitti error full images flow corresponding version kitti evaluation full images including occluded proposed scene flow method current achieves best flow note contrast method handle scenes independently moving paper exploit consistency time viewpoints dense scene flow piecewise planar rigid multiple consecutive frames stereo shape motion proposed model robust occlusions estimate depth motion road scenes adverse imaging methods work handle deviations constant velocity assumption view-consistent scene flow estimation multiple frames multi-view scene flow view variational cvpr robust dynamic motion estimation cvpr surface stereo soft cvpr object stereo joint stereo matching object cvpr large optical matching variational motion tpami dense accurate multi-view heidelberg scene flow points cvpr variational approach video cvpr stereo matching tpami variational method scene flow estimation stereo iccv consistent depth scene flow temporal surface visual fusion field tpami outdoor stereo camera generation benchmark data optical variation optical flow motion field estimation real cvpr scene segmentation visual motion tpami tensor approach multi-view scene flow estimation eccv heidelberg accurate motion field estimation stereo image eccv heidelberg resolution motion graph cvpr image motion explicit temporal depth schindler model cvpr matching segmentation based joint motion estimation segmentation scenes occlusion cvpr joint estimation structure geometry stereo eccv heidelberg scene cvpr energy optimization eccv heidelberg piecewise rigid scene iccv evaluation data optical heidelberg scene flow estimation rigid motion iccv temporal optical iccv moving images image dense scene flow dense stereo eccv heidelberg optical fields robust stereo eccv heidelberg robust flow cvpr local computing visual eccv heidelberg
int comput vis scene flow estimation piecewise rigid scene model vogel schindler 2015 2015 2015 scene flow estimation jointly recover dense geometry motion image classical disparity optical flow overcome existing represent dynamic scene collection rigidly moving input images geometry motion jointly recovered over-segmentation piecewise rigid scene model conventional represent real-world scenes independent object enables define suitable scene perform occlusion leverage discrete optimization accurate assuming rigid motion time additionally enables incorporate multiple frames view holds encouraged consistent frames temporal viewconsistent multi-frame scheme improves robustness imaging method vogel schindler schindler computer achieves leading performance kitti flow scene flow stereo motion estimation piecewise piecewise rigidity segmentation scene flow dynamic scene defined dense representation shape motion scene flow estimation images cameras time benefit scene flow include video generation motion capture rabe wedel scene flow combination classical computer vision optical flow dense stereo dynamic dense stereo yamaguchi optical flow sun performance scene flow algorithms basha huguet devernay wedel methods yamaguchi leverage additional stereo video respective scene flow stereo optical example 123 int comput vis example scene vaudrey jointly estimated motion superpixel viewpoint matching evidence local problem version scene flow estimation prior geometry stereo optical flow move simple pixelwise smoothness priors layered piecewise planar scene models work priors scene flow regularizer local rigidity common realistic penalizing deviations piecewise rigid scene model contribution step represent dynamic scenes collection planar rigid previous work stereo argue scenes regions consistent motion aim jointly recover scene rigidly moving well shape motion parameters regions model scenes approximation holds well capture shape motion real-world including scenes independent object regularization reasoning terms rigid planar regions pixels number additionally address challenge optimization scene flow stereo optical represent scene flow pixel rigidly moving continuous degrees plane motion start individual initial superpixel segmentation based superpixels compute finite set candidate scene flow estimation labeling inference assigns pixel segments segment candidate moving optimization find best moving plane reasoning level pixel level pixels step inaccuracies initial boundaries generated surface motion discontinuities view-consistent multi-frame scene flow second contribution exploit piecewise rigid scene model overcome existing scene flow reason reference view basha rabe valgaerts wedel imaging affect model single viewpoint fact evidence views data form stereo video appears consistency multiple frames scene flow estimation robust input left left view time scene flow estimate viewpoint left disparity reprojected flow 123 int comput vis exploit longer time piecewise planar rigid scene model extended simultaneously estimate geometry motion longer time estimate consistent views considered time simultaneously parameterize scene flow considering longer help motion classical optical flow estimation multi-frame extensions desired two-frame methods state art argue constraints scene representation constraints caused remain valid long exploited estimate consistent views longer segmentation remain representation integrate dynamic model favors constant velocity individual empirically assumption valid long segments temporal proposed approach scene flow model scene piecewise rigidly moving regions regularization regions explicit occlusion view-consistent model extension leads challenging simultaneously representing shape motion image time consistency multi-frame extension yields consistent segmentation scene favors constant velocity formulation well suitable discrete inference formulation number time model qualitative quantitative hard qualitative model missing quantitative evaluate method challenging kitti dataset street stereo flow achieve leading methods specific situation time full model top method optical flow evaluated full images including occlusion paper based conference describe approach including model inference proposal occlusion analysis detailed comparison conventional parameterization view-consistent experimental optimization parameters quantitative work defined scene flow estimation dense geometry motion image approach computing independent optical flow fields views final flow field fit image data wedel rabe data calibrated stereo camera starting disparity optical flow reference frame disparity difference view geometry flow jointly huguet devernay problem variational problem generalized valgaerts work relative assuming camera scene flow estimating relative camera approaches proposed regularizer encourages smooth smooth scene basha parameterization depth motion vector reference view estimate parameters popular optical flow method scene total variation prior motion field realistic vogel regularizer encourages locally rigid model local rigidity explicitly identify regions consistent motion image local rigidity priors assumption sparse motion idea extended sparse scene flow devernay extend technique multi-camera scene flow rigidly moving regions 123 int comput vis scene representation devernay regions move independently global step locally rigid motion input global optimization defined explicit surface scenes fixed rigid motions exploited context scene flow estimation data assume local surface exploit additional depth local rigidity prior overcome large computing optical flow field explicitly rigid motion previous dense scene flow methods common penalize deviations spatial smoothness robust explicit discontinuities segmentation long context stereo optical flow methods segmentation refine scene segment scene planar superpixels estimate disparity additionally penalizing deviations initial stereo yamaguchi method extended epipolar flow epipolar scene flow assuming flow epipolar geometry result camera general optical flow computed parameterize motion segment allow occlusion estimating method sense regularization motion fields adjacent segments estimated independently perform motion estimation multiple optical flow fields limited small encourage current flow motion fields previous larger assuming constant motion jointly reason three consecutive considering constant scene flow address general constant velocity constraint relaxed encourage second order smoothness motion field soft motion single reference reasoning occlusion regions outliers appears hard longer time intervals estimated motion lie require motions lie prior soft sun argue scene structure time motion avoid temporal jointly estimate flow segmentation small number motion operate video motion temporal motion estimated view-consistent formulation pixels planar rigidly moving consistency segmentation multiple contrast motion representation small segments enables address explicit representation motion shape allows scene flow methods exploit temporal consistency longer time intervals straightforward smoothness constraints better supported scene rabe advantage fact propagate geometry motion frames help pixel vector scene flow vectors current previous estimated method wedel compared output fewer stereo flow longer motion final optimization soft including two-frame optical flow method propagate output scene flow temporal needed application multi-camera operate scene flow estimated time represent scene explicit video data multiple cameras method approaches motion capture 123 int comput vis scene scene collection rigidly moving planar three segments cover car techniques avoid arbitrary reference frame treat views form consistency check consistency directly objective view-consistent extend strategy scene considering consistency images temporal introduced case optical fusion proposal standard optimization employ scheme estimation scene piecewise rigid model scene flow estimate scene describe dynamic scene collection piecewise planar regions moving rigidly time motion geometry region degrees determine minimizing single objective pixels suitable plane rigid motion selected note spatial segmentation aim scene over-segmentation desired capture geometry motion allow accurate detailed description basic parameterization scene single reference view consider time steps achieve view-consistent scene flow multiple frames model classical case images calibrated stereo time note extension larger number views distinguish identify left left camera time define common coordinate system canonical canonical evaluation reference case view consistency lead projection left assume model moving plane rotation vector scaled normal three degrees note explicitly distinguish camera ego-motion independent object describe full motion time extend model reason multiple cope high ego-motion camera case single reference assume planes visible canonical canonical camera center coordinate visible plane define scaled normal nl0 plane equation holds points paper transfer moving plane views respective camera coordinate plane equation valid rigid scaled normal correspondence points plane nl0 left camera time step normal nl1 respective coordinate system nl0 nl1 nl1 determine depth observed acquired time center pixel image camera needed test occlusions well check geometric consistency utilizing planar scene representation allows map pixel corresponding view moving plane homographies canonical view il0 views geometric configuration 123 int comput vis single reference-view data terms homographies pixels reference view areas hard correspondence view-consistency areas pixels data terms three-frame viewconsistent consistency encouraged spatial direct temporal neighbors pixels views considered energy figure mappings arbitrary view pairs achieved canonical view desired define canonical frame operate fixed mapping pixel il0 segments moving note basic form energy considering view consistency data term data term assumption corresponding points views achieve constraints stereo pairs time steps optical flow camera moving plane pixel utilizing homographies defined define stereo data terms cameras single reference view aim determine depth motion pixel reference view il0 define energy function mapping il0 assigns pixel reference view il0 segment mapping moving plane set proposals segments find aim minimize single energy il0 optical flow data terms time il0 data term measures photo-consistency views basic regularization term encourages smoothness geometry motion segment boundary term quality spatial over-segmentation reference visibility term missing areas move viewing frustum energy starting fixed initial over-segmentation segments moving labeling segment moving planes corresponding pixel location view image robustness general conditions census transform scale distances limited specific examples generated census data explicitly complete data term terms 123 int comput vis distance vectors penalize distances shared compute distances point boundary planes distance shared boundary image plane simply combination endpoint distances consider surface exploit endpoints respective plane normals denote difference normals define distance function illustration regularization distance function considers geometric distance integrating squared distances shared edge well normals leads closed form spatial regularization geometry motion scene geometry motion parameters shared pixels explicit regularization segment focus segment benefit pixelwise regularizers vogel boundary regularizer strong scene flow naturally problem area previous scene flow techniques vogel boundaries occur single object regularization term assumes piecewise smooth geometry model shape motion priors independently define regularizer term geometric term measure motion assume adjacent pixels assigned moving planes treat pixels image plane measure contribution regularization term common consider endpoints edge geometry endpoints endpoints case lie pixel boundaries compute weight boundary distance geometry regularizer adding factor integrate squared distance function boundary normal direction closed considers pixels adjacent length common edge weight incorporate edge image denotes penalty form regularization setting energy favors planar configurations integrating squared distances induced penalty situation soft realistic case limited proposal set moving planes motion regularizer applying rigid moving integrate endpoint distances well differences leading normals geometric regions left situation higher energy 123 int comput vis image direction determined assuming set define weight robustness discontinuities achieved employing penalties proposed regularization scheme limited endpoint distances replaced disparity difference optical flow change disparity popular scene flow devernay valgaerts regularization approximation true reasoning segment approximate regularizers computing penalties directly endpoints length boundary edge shared evaluation regularizer superpixels framework accuracy algorithm spatial regularization segmentation data term spatial regularization operate segment-to-plane mapping assignment pixels segments experience lead incorporate prior segments image add additional regularization quality underlying il0 second term segment seed point order maximum size smaller strategy prevents scene flow number candidate segments time needed optimizing energy good strategy define seed points center set values pixels perform note strategy proposed compute over-segmentation single visibility term considered problem areas visible large regions cover portion configurations valid correspondence considered data term cost arbitrary moving planes model lead portion pixels assigned motion moves penalizing configurations region moves solution region area images data cost challenging address problem assume pixel stay image move binary visibility reference image pixel binary function boundaries image encourage scene flow estimate stay visibility term energy oob il0 term contrast sensitive pairwise evaluated weight allows image structure length edge define apply il0 123 int comput vis oob set maximal data common stereo variational flow methods pixels moving output stereo optical flow algorithms proposal generation step visibility ego-motion stereo camera approximate inference inference piecewise rigid model estimating continuous variables geometry motion rigidly moving discrete assignments pixels optimization finite set proposal moving problem labeling problem discrete benefit leverage robust discrete optimization techniques cope well complex fusion move framework occlusions discrete naturally objective start fixed segmentation energy suitable moving plane segment proposal initial superpixel simply minimize segmentation energy non-convex segmentation grid well seed points selected central pixels initial solving consider regularization terms solution mapping fixed energy pixels segments moving planes segment size restricted maximal length pseudo-boolean function representing local energy optimization segments strategy optimizing locally restrict moving plane proposal cover expansion region proposal cover neighboring segments set region size allows test proposals note optimization observe practical general pseudo-boolean qpbo non-submodular complete labeling supermodular edges per-pixel initial superpixel superpixel segmentation optimization compared standard graph graph number nodes energy energy local submodular approximation proposed advantage conventional graph pairwise supermodular local plane approximation bounds true energy idea simple better approximation simple truncation non-submodular compare approaches proposal generation perform inference geometry motion require set proposal planes rigid output scene flow combination stereo optical flow pixelwise correspondence fit parameters plane rigid motion superpixel initial fitting inaccuracies stereo flow superpixels depth motion robust procedure minimize transfer error robust cost 123 int comput vis min occlusion handling data term defined assumes pixel occlusion reasoning takes scene explicitly reason compared handling occlusions scene flow advantage additional views pixels occluded views visible view leverage occlusion handling applied pairs views data term single view formulation summand data applying constant penalty occ pixel occluded views penalty occ oob occlusions areas impact correspondence pixel correspondence data costs note pixels assigned moving plane scene representation naturally occlusion model general single move approximate inference procedure binary optimization assuming fixed segment-to-plane mapping investigate per-pixel segmentation differences procedure solving binary state denote pixel current segment assignment segment data term form pseudo-boolean homographies parameters normal rigid motion denotes conventional projection pixel segment correspondence determined proposal parameterize rotation map define determined scaled normal derive homography non-convex optimization problem solution efficient algorithm quality fit note estimation planes rigid motions problem fitting rigid motion computation ego-motion stereo camera algorithms problem applied consider motion individual segment complete stereo additional proposals strategy parts solution set proposals allows include additional energy exploit including estimated ego-motion stereo system additional ego-motion fitting procedure segment output per-segment solution optimizing mapping current solution estimated local replacement proposal depth motion result proposal estimate motion depth address additional proposals propagate state segments geometry rigid simply corresponding state procedure leading combination geometry motion neighboring strategy idea shared neighboring il0 vector denotes binary pixel data penalty equals current assigned segment pixel occluded depends binary segment assignment pixel multiple occludes occlusion turn depends identify set segment assignment pairs pixel occludes pixel replace data term 123 int comput vis il0 occ denote difference data penalty occlusion cost occ occ better understanding equation focus single pixel respective summand product equals occlusion occluding pixels assigned segment lead pixel set data cost equals occ case standard data penalty recall segment-to-plane mapping reasoning entire segments directly extend occlusion model segment respective pseudo-boolean variables representing segments consider segment occluded central pixel segments segment occluded region central pixel check occlusions employ conventional utilizing compute depth number occluding penalty higher-order pseudo-boolean optimization techniques based graph function including applied quadratic higher-order terms reduced pairwise reduction techniques proposed rother approaches reduction independently higher order summand details point benefit fewer non-submodular edges occur pseudo-boolean function optimization details experimental view-consistent extend segmentation challenge consistent segmentation scene views view-consistent approach set assignments segments moving planes pixels segments computed aim estimate scene flow time extend idea rigidity assuming constant rotational velocity moving note time intervals assumption valid application start description time extend model multiple frames model determine depth motion vector time views superpixel segmentation set segments image view time step energy definition extended function assigns set mappings svt pixel frame segment second rigidly moving plane selected set recall segment denotes candidate set moving definition energy takes basic form view-consistent model basic representation model estimate scene flow views time benefit compared single reference view entire image evidence views robust common imaging occlusion handling view-consistent setting definition data term reference view consistency underlying geometric configuration segmentation regularization term straightforward extensions segmentation term single view evidence view-consistent formulation require explicit visibility term spatial smoothness assumption extended simply motion geometry terms 123 int comput vis implausible case occlusion normal case illustration per-pixel view-consistent data term svt svt extend regularization segmentation considered il0 defined note second term applied canonical maximal size segment restricted canonical note treat frames encourage segmentation consistent views maximal segment size exploited inference consistency superpixel encouraged data described view-consistent data term view-consistent model explicitly description scene terms moving planes observed exploit check consistency scene flow estimate view direct neighbors well views time term classical sense check photo-consistency images corresponding pixel determined assigned moving planes scene representation check geometric configuration test consistency comparing depth values induced respective moving plane based underlying image segmentation assume check consistency pixel location view time denote corresponding pixel location view time moving plane pixel homography allows determine pixel location depth function enables evaluating geometric configuration data term single pixel view assigned moving plane adjacent view imp occ oob mvp cases depicted relative difference depth distinguish implausible occlusion comparing disparity values stereo case case implausible depth plane observed 2nd camera pixel smaller depth assigned pixel 2nd situation point plane occluded moving plane visible 2nd apply fixed penalty imp second case depth moving plane corresponding plane pixel occluded second fixed penalty occ occlusion reasoning current solution views simultaneously estimating solution views occlusion technique single reference 123 int comput vis additional benefit energy function pairwise multiple corresponding location view turn leads higher-order terms respective pseudo-boolean experience view-consistent formulation leads fewer supermodular edges optimization optimization set proposal planes limited practical assume representation assigns accurate depth comparing relative depth values relaxed test including empirically set additionally aliasing artifacts introduced finite resolution pixel case pixels moving viewing frustum fixed penalty oob employing view solution views case single reference additional visibility term pixels geometric correspondence apply usual census data penalty measure proposed additionally data term maximal cost pixel number non-submodular terms optimization reduced lead avoid truncation pixels geometric moving moving plane additional penalty mvp leads desired view-consistent pixels encouraged moving plane neighboring appears penalize pixels correspondence set penalties occlusions pixels moving bounds oob occ aliasing prevents penalizing implausible configurations set imp prevents lead implausible assignments final error allow deviations consistency assumption segmentation empirically set mvp oob views contribution pixels data term data costs stereo pairs frames direct neighbors time example kitti training set data term denote normal photo-consistency bounds occluded moving plane implausible cases figure contrast reference-view formulation view considered data view holds scene flow figure illustrates view-consistent data assigned data term view individual view-consistent multi-frame extension details viewconsistent model motion segmentation regularizers extended larger number frames straightforward data term define homographies additional views transform normals specific view coordinate recall restrict reason time intervals assume motion moving plane constant velocity rotational suitable homographies homographies defined time steps generated application assuming constant note normals proposal set canonical coordinate 123 int comput vis variation camera constant velocity scene observed moving camera camera images induced flow camera removing ego-motion camera model small deviations constant velocity assumption test camera system rotational changes viewing direction affect image application experiments good example scene flow estimation common motion stereo caused surface model motion relative respective camera coordinate slight changes relative camera changes relative geometry motion address include ego-motion estimates time compute relative ego-motion consecutive time steps computation homographies frames applying motion induced moving plane representation ego-motion relative ego-motion time step recall rotation moving plane coordinate system canonical relative ego-motion canonical view applying homography frames left view method proposed approximate inference view-consistency inference procedure approach single reference view perform inference discrete energy solving mappings segmentation mappings segments moving planes optimizing segmentation mappings practical initial superpixel start grid edge length seed points simply grid trivial efficient aliasing caused size segments per-pixel refinement step deliver consistent depth motion grid segments large pixels solving view-consistent segmentation lead aliasing consistency constraints set mvp oob optimization proposals generate proposal set described running single segment-to-plane step reference-view removing proposal set leading reduced computation optimizing proposals generated consecutive frame frames generate proposals time steps additionally additional proposals existing consider proposals valid expansion seed point canonical expansion region size note view normal find rnlt rnlt rnlt homographies ego-motion estimation camera stereo camera system problem 123 int comput vis example kitti training set consistent moving plane assignments segment level final consistent superpixel segmentation yields good accuracy fusion local determined projection expansion region inference mappings moving plane proposals size graph restricted region constraint expansion region pixels determined projection penalize per-pixel parameters mvp figure illustrates computed mappings optimization consistent moving plane assignments segment level consistent superpixel segmentation depicted hierarchical refinement segment allows simple refinement work well directly pixels segments solving segments assigned moving refine segmentation grid resolution image start inference previous prune initial proposal set moving planes hierarchical approach allows reduce aliasing smaller segment considers global context set expansion region set moving plane proposals reduced step note reduce expansion region pixels experiments experimental evaluation basic model based single reference view viewconsistent quantitative experiments kitti dataset standard benchmark optical flow stereo images acquired calibrated stereo top car ground scenes challenging strong motion car leads large stereo flow pixels direct correspondence images acquired realistic conditions lens kitti benchmark large scale dataset allows evaluating scene flow methods stereo optical ground truth independently moving leads methods focus dataset better evaluation scene flow methods huguet devernay vogel quantitative experiments employ kitti training detailed performance parameter size images ground comparison state images test portion kitti dataset kitti benchmark ground truth inaccuracies moving standard kitti metric compute number pixels 123 int comput vis proposal fitting percent outliers init fit kitti metric approximation percent outliers full kitti metric full endpoint error proposal fitting init fit threshold ground report error pixels entire image areas additionally report endpoint error optical flow stereo occluded including respective optical flow evaluation single reference view model experiments fixed set smoothness weight weight segmentation term relative generate proposal set output optical flow stereo computing optical flow employ algorithm vogel census data term total generalized variation popular combination kitti estimate apply factor image disparity map matching evaluate proposal fitting procedure figure kitti metric threshold well endpoint error proposal algorithms per-segment fitting observe small changes planar rigid segment fitting affect slight deviations error depth motion investigate smoothness term reasoning recall evaluate spatial regularizer directly endpoint distances shared contribution boundary pixels approximation accurate superpixels computation experience note report directly approaches employ per-pixel refinement representation optimization approach sense parameter initial segmentation evaluated starting trivial segmentation length leads slight endpoint error approximation proposal fitting error proposal algorithms planar rigid segment fitting approximation regularization term per-segment error evaluating pixel integrating distances endpoints shared edge figure per-pixel refinement takes closed refinement small difference accuracy compared starting superpixel note understanding inference practical energy error reduced starting number superpixels depicted left initial accuracy final result per-pixel refinement inaccuracies initial starting fewer segments performance weight regularization term maximum superpixel size per-pixel refinement method sensitive changes case higher values lead better longer computation 123 int comput vis grid superpixel percent outliers seg pix kitti metric seg pix endpoint error percent outliers grid superpixel kitti metric endpoint error regularization percent outliers regularization occlusion handling seg pix kitti metric seg pix endpoint error occlusion handling visibility percent outliers kitti metric visibility endpoint error evaluation model kitti training set per-segment final solution per-pixel initial grid blue red compared superpixel segmentation left comparison regularization blue red occlusion basic model blue red occlusion visibility pixels stay bounds compared standard per-pixel figure percent outliers percent outliers percent outliers pix-fn pix-fa pix-sn pix-sa pix-fn pix-fa pix-sn pix-sa pix-fn pix-fa pix-sn pix-sa initial superpixel size smoothness weight maximum superpixel size performance number initial performance smoothness weight performance maximum superpixel size investigate regularization set robustness observe regularization evaluated fact error measures evaluate quality scene disparity optical figure visibility trivial assumes pixels stay visibility initial algorithms strong flow endpoint error occluded impact quality estimated scene flow proposal algorithms extend standard proposal set adding proposals scene flow methods scene flow locally rigid scene flow evaluate local replacement strategy ego-motion proposals proposal evaluate replace rigid motion proposals estimated camera motion motion stereo rigid scene yamaguchi observe adding proposals improves endpoint error optical flow larger gain achieved local replacement adding additional ego-motion approaches combination improves best achieved ego-motion hard kitti evaluation occlusion model evaluation occlusion model qualitative example street vaudrey compared kitti challenging conditions allow usual census data regularization 123 variants proposal generation pix pix pix pix pix pix int comput vis variants proposal generation pix pix pix pix pix pix stereo-noc flow-all flow-noc percent outliers average endpoint error stereo-all stereo-all stereo-noc flow-all flow-noc error kitti metric endpoint error comparison strategies generate compare proposals adding proposals scene flow algorithms local replacement ego-motion combination removing ego-motion proposals evaluated kitti metric endpoint error per-pixel refinement example scene vaudrey occlusion handling processing estimated depth motion occlusions scene independently moving complex occlusion pattern challenging figure processing steps estimate appears occlusion regions visible reference left adding occlusion handling allows occluded regions motion per-pixel refinement object improves occlusion boundaries compare basic model additional occlusion figure consistent advantage explicit occlusion optical flow truncation parameters parameters set occ additional proposals advantage difference models recall order perform optimization based higher-order occur case multiple reduced pairwise optimization problem supermodular nodes remain unlabeled running minimize rother proposed qpbo-i apply table experience applying method kitti training number supermodular edges unlabeled nodes appears employing qpbo-i qpbo impact pixel number nodes qpbo appears high optimization takes order challenge form occlusion reasoning sensitive 123 int comput vis table optimization explicit occlusion supermodular edges unlabeled energy qpbo qpbo-i inference seg pix supermodular edges unlabeled nodes energy qpbo energy qpbo-i numbers kitti training set data window car note occlusion handling unlabeled nodes occur evaluation view-consistent model parameters parameter reference-view model smoothness set edge length based multiple set per-pixel start initial grid quantitative proposed investigate model benefit hierarchical refinement grid described figure performance single refinement result hierarchical gain performance consistent single refinement step model jointly reasoning multiple frames assuming constant velocity rigidly moving vc-2f vc-2f vc-3f investigate performance considering consecutive frames distinguish proposals time steps current derive proposals disparity flow computed adjacent frame pairs time include proposals three frames add single reference-view version local reduce initial proposal set current frame note reference-view method applied segment observe moving single reference view yields notably optical flow error view-consistency considering data views parts occluded canonical view evaluation reference view visible strong endpoint error reduction including proposals previous time step considering image data previous frames combination leads larger performance gain occluded regions suggests larger set proposals multiple frames image evidence longer including frame model yields three frame proposal recall reference-view version vc-3f poor proposals poor proposals proposals vc-2f percent outliers percent outliers percent outliers average endpoint error average endpoint error refine refine refine pwrs-seg average endpoint error pwrs-seg proposals vc-2f kitti metric endpoint error error kitti metric endpoint error error kitti metric endpoint error error segmentation two-frame proposals view-consistent three-frame proposals single reference-view model local replacement additional ego-motion evaluation poor proposal proposals vc-2f kitti metric endpoint error figure 123 refine refine refine variation time window vc-2f vc-3f int comput vis variation time window vc-2f vc-3f percent outliers average endpoint error flow-noc stereo-all stereo-noc flow-all flow-noc stereo-all stereo-noc flow-all error kitti metric endpoint error evaluation number single reference-view method frames view-consistent method frames frames frames proposals previous frame frames proposals previous frame frames proposals frames kitti metric endpoint error figure method order prune proposal set advantage reduced computation time view-consistent figure accuracy evaluated three frame case considering additional proposals previous time fact application pwrs-seg yields proposals analysis variants deliver final energy proposal set parts solution supported fact observed accuracy difference proposals previous time step well previous observe strong accuracy gain local replacement strategy ego-motion proposals cases additional proposals final method proposals computing scene poor proposal set change parameters initial stereo flow optical flow case single image change scale apply two-frame view-consistent method reduce proposal result depicted notably high error algorithms reduced factor scene flow approach cope unfavorable proposal achieved ego-motion dataset algorithms compute flow algorithm deliver areas small motion planar street scenes parts image motion turn suggests scene flow cope large motions recall formulation data directly leading pairwise edge supermodular edges table investigate situation occlusion handling strategy single reference data kitti training apply qpbo-i algorithm optimization problem three-frame version number unlabeled nodes supermodular edges number non-submodular edges reference-view unlabeled nodes occur considering problem graph applying lsa-aux algorithm find submodular approximation problem expansion local approximation bounds true energy energy case supermodular terms final energy order performance qpbo lsa-aux graph 123 int comput vis table optimization view-consistent model average number nodes edges average supermodular edges unlabeled nodes applying energy lsa-aux qpbo-i inference seg pix nodes edges supermodular unlabeled energy qpbo-i energy lsa-aux numbers kitti training set qualitative examples illustration examples kitti benchmark recovered method interesting example top lens flares optical scene flow methods recover motion artifacts consistent consecutive location allows approach recover scene flow notably flow vectors pixels visible areas standard error threshold note robust handling artifacts achieved multi-frame depicted scene low image contrast scene interesting lens flare challenging geometry location consistent image areas car moving ground figure illustrates scenes meister input images scene flow estimates reprojected optical flow usual color examples model handling independent object motion unfavorable motion image plane scenes hard conventional flow stereo scenes well complex occlusion scene challenging sun lens flares suggests scene occluding view recover conventional approaches parameterize scene single camera complex scene cameras image depth qualitative evaluation ground truth scenes incorrect depth estimates typical failure cases figure typical failure cases boundary images occlusion recall replace data term fixed penalty oob pixel correspondence assume proposal image region incorrect data penalty case compared energy true solution model incorrect solution penalties incorrect geometry motion penalties correspondence recovered depends size respective regions second challenge imaging sun flares example sun flare leads low data energy consistency quantitative direct comparison view-consistent single reference-view models table note change kitti ground parameter extensions local replacement algorithms derive proposals usual basic reference-view version local denote additional ego-motion view-consistent version prune proposals distinguish three 123 int comput vis examples kitti training input images left recovered scene flow color disparity motion reprojected image depicted encode length red color endpoint error disparity left flow figure 123 int comput vis challenging examples meister input frames method recovered scene reprojected disparity flow field 123 int comput vis typical failure cases kitti training area moving viewing frustum example meister sun flare consistent motion pattern pixel level time needed additional proposals hierarchical refinement step compare numbers starting model single reference view time optimization additional proposal generation strategies computing initial optical flow disparity view-consistent exploit reduction number proposals running segment low number basic version deliver running proposal algorithms takes three-frame method min including comparison state art table comparison piecewise rigid scene model state art kitti test time benchmark scene flow methods top view-consistent model stereo considering full images occluded note methods assume epipolar motion hard constraint method scenes independently moving considering methods general independent object distance best scene flow piecewise rigid scene advantage single camera methods motion estimation challenging proposals general numbers top basic version achieves reduction error measures compared state-of-the-art strategies generate additional proposals view-consistent model leads visible reduction error measures moving three frames improves occluded considering frames yields numbers comparing best result initial table illustrates time parts distinguish running flow stereo algorithms proposal fitting procedure time inference segment level table kitti training set average kitti metric flow pixels endpoint average endpoint error occluded regions method flow kitti metric noc stereo kitti metric noc noc noc algorithms vc-2f vc-3f 123 int comput vis table kitti images computer proposals numbers initial segments segments proposals init fit seg pix table comparison state-of-the-art kitti test set methods local ego-motion reduce proposal method setting kitti metric noc noc stereo evaluation optical flow evaluation scene flow multi-frame motion stereo methods paper introduced scene flow approach models dynamic scenes collection piecewise local moving rigidly allows recover over-segmentation scene moving leading accurate geometry motion employing reasoning segmentation occlusions observed method achieves leading performance popular state-of-the-art stereo optical flow techniques respective extend basic reference-view technique leverage multiple consecutive frames stereo view-consistent approach consistency time scene flow model robustness missing recover motion geometry unfavorable imaging methods work incorporate temporal consistency constant velocity assumption aim explicitly model small deviations local rigidity include image understanding segmentation motion geometry 123 supported european european framework well int comput vis min higher-order occlusion handling reference view describe data term quadratic pseudo-boolean note interesting case occluding problem quadratic form occluding pixel data term recall defined single expansion pixel assigned previous restrict analysis case consider term term previous variables encode minimum minimum case approach problem replace product variables add min reduction consider case pixel occludes pixel assignments case pixel occluded distinguish case term help non-submodular term weight non-submodular term introduced variables term case variables replaced note restricted variables non-submodular term weight quadratic replace product leading non-submodular motion structure optical flow generated moving ieee transactions pattern analysis machine optimizing binary higher order european conference computer stereo system estimation structure conference machine vision application evaluation optical international journal computer correspondence algorithm image transactions multi-view scene flow view variational ieee conference computer vision pattern robust dynamic motion estimation ieee conference computer vision pattern additional replaced min quadratic variables encode setting minimum case scheme introduced rother variables replace product 123 int comput vis surface stereo soft ieee conference computer vision pattern stereo matching machine vision object joint stereo matching object ieee conference computer vision pattern general dense image matching framework direct ieee international conference computer large optical matching variational motion ieee transactions pattern analysis machine high accuracy optical flow estimation based european conference computer multi-view scene capture video shape international journal computer dense accurate multi-view conference computer transfer joint changes optical european conference computer multi-camera scene flow points ieee conference computer vision pattern stereo relaxed ieee dense motion capture video ieee conference computer vision pattern variational approach video international journal computer ieee conference computer vision pattern local submodular binary pairwise computer vision pattern stereo processing matching ieee transactions pattern analysis machine scene flow ieee conference computer vision pattern variational method scene flow estimation stereo ieee international conference computer consistent depth scene flow temporal international journal computer multi-frame correspondence estimation international journal computer higher-order reduction binary graph ieee conference computer vision pattern computing visual correspondence occlusions graph ieee international conference computer vision optimization optical flow ieee conference computer vision pattern fusion moves field ieee transactions pattern analysis machine image technique application stereo international joint conference stereo camera system generation real-world benchmark data optical total variation optical flow motion field estimation ieee conference computer vision pattern scene segmentation visual motion global ieee transactions pattern analysis machine variational optical international journal computer approach multi-view scene flow estimation european conference computer accurate motion field estimation stereo image european conference computer minimizing variational models non-convex data international conference scale variational methods computer total generalized variation optical flow european conference computer optimizing binary extended ieee conference computer vision pattern minimizing sparse higher order energy discrete ieee conference computer vision pattern high resolution motion graph ieee conference computer vision pattern matching census transform robust international conference computer analysis images layered image motion explicit temporal depth conference processing layered model ieee conference computer vision pattern global matching color segmentation based ieee computer joint motion estimation segmentation complex scenes costs occlusion ieee conference computer vision pattern 123 joint estimation structure geometry stereo european conference computer differences stereo motion real-world stereo international conference image vision computing scene ieee conference computer vision pattern superpixels energy optimization european conference computer scene flow estimation rigid motion ieee international conference computer evaluation data costs optical pattern piecewise rigid scene ieee international conference computer view-consistent scene flow estimation multiple european conference computer int comput vis temporal optical ieee international conference computer representing moving images ieee transactions image efficient dense scene flow sparse dense stereo european conference computer optical machine vision continuous fields robust stereo european conference computer robust epipolar flow ieee conference computer vision pattern efficient joint occlusion stereo flow european conference computer local computing visual european conference computer 123
completed dense scene flow rgb-d space conventional scene flow translational vectors model motion rotation accuracy motion estimation large missing data techniques local approaches global local approaches generate motion global approaches handle large displacement completed dense scene flow framework rotation translation motion combines local method global method considering handle large displacement motion smoothness proposed framework rgb-d image space computation quantitative evaluation based middlebury method performance data kinect dense scene flow estimation challenging computer rgb-d cameras provide depth estimate scene flow rgb-d conventional scene flow methods translation completed scene flow methods rotation translation model motion second vision challenging estimate completed scene flow rgbd challenging problem large displacement large displacement motion searching dimension range scene flow good initial candidate accurate robust second problem noises missing data captured rgb-d rgb-d data authors suppressed excessive length rgb-d scene flow local method global method conventional scene flow completed scene flow work table scene flow conventional completed scene flow local global approach employed reference image target motion estimation local approaches global local approaches feature consistency corresponding points local supporting neighboring frames time local approaches address displacement employ random search generate accurate motion global approaches spatial points occlusion global approaches model spatial local good initial values accurate performance propose scene flow estimation framework address challenging previous framework combines advantages local method global corresponding local method provide good candidate values global method large displacement global method combines candidate values explicitly modeling occlusion enforcing smoothness handle missing data techniques framework combine advantages local global large displacement enforcing scene flow estimation handle missing data propose compute matching cost point local supporting area adaptive robust motion initial values searching dimension accuracy work scene flow motion compared optical scene flow vision completed dense scene flow rgb-d space estimate optical flow scene flow optical flow refine scene flow local method derive candidate motion patterns estimate scene flow global method framework recognition optical flow scene flow scene flow methods employ rgb rgb-d methods rgb-d scene flow methods table proposed framework global optimization compute scene estimated scene flow extended optical flow framework estimation range flow rgb-d captured variational method dense motion estimation motion scene flow problem variational framework local global conventional scene flow methods employ translation motion modeling handle large displacement large displacement motion rotation completed scene flow rotation translation model motion better generate translation proposed completed scene flow method estimated scene flow local introduce error occlusion detection generate accurate motion framework derive good initial candidate values local estimate scene flow explicitly modeling occlusion smoothness global framework figure framework local optical flow nrdc generate completed optical flow rgb image optical flow scene flow good initial values local scene based initial local method combine rgb color depth refine scene derive set candidate motion patterns local scene flow set candidate motion patterns modeling occlusion enforcing smoothness global rgb-d images compute motion reference image target image pixel authors suppressed excessive length nrdc completed scene flow nrdc scene flow rotation change patch principal direction reference image rgb color depth pixel valid depth provided depth valid pixel point color compute depth camera parameters projection denotes 6-dof motion completed scene flow employed 6-dof scene flow point reference rgbd position point motion denotes optical flow optical flow methods large displacement image search dimension scene flow method nrdc generate initial nrdc generate motion field 2-dof translational vectors figure motion parameters method approach motion field completed scene compute rotation point corresponding principal principal direction surface normal principal direction projection principal direction image feature detection principal point point principal completed dense scene flow rgb-d space direction vectors computed rotation point normal principal rotation point rotation translational point computed local method motion initial optical flow rgb color local scene flow estimates rgb-d features consistency point reference image corresponding position target address noise missing data point motion aggregate cost values points adaptive reliability corresponding supporting local matching cost points reference clocal clocal supporting matching cost point motion defined clocal supporting area point weighting function probability points reliability maps reference target rgb-d image matching cost point motion authors suppressed excessive length supporting area noises rgb-d features point assume local aggregate cost values local neighboring points surface robust matching distance better neighboring points surface euclidean compute distance propose patch normal neighboring points surface basic neighboring point point supporting patch point set neighboring points threshold ratio previous normal small threshold ratio weighting function supporting area point neighboring points probability adaptive weight based euclidean distance aggregate cost values neighboring points area weighting function weight reliability map considering depth rgb-d data missing data introduce reliability pixel rgb-d depth noise depth apply depth spatial distance depth reliability measure reliability max valid max constant matching cost point reference image 6-dof motion motion probability point position target completed dense scene flow rgb-d space assume color difference measure difference depth values euclidean matching cost point motion defined ratio searching method scene flow good handle large point initial provided local optical flow refine motion estimates spatial random spatial 6-dof completed scene flow 2-dof translational optical random searching dimension large efficiently good introduce random search random 2-dof translation compute principal direction vectors reference image target image feature detection computation motion field completed scene flow optical 6-dof motion 2-dof random random dimension random searching scene flow estimation global method feature consistency local explicitly model occlusion smoothness global energy function global scene flow cglobal sglobal set points supporting patch set neighboring points image cglobal feature sglobal smoothness motion cglobal clocal modeling robust matching cost occlusion modeling address occlusion occlusion matching cost occluded points constant cost matching robust matching cost global method cglobal clocal authors suppressed excessive length set matching occlusion point points target image occluded points camera previous methods estimate motion considering occlusion consistency refine motion occluded introduce error estimated motion method explicitly model occlusion depth occlusion depth robust depth noises occlusion assume depth noises standard occlusion point defined denotes computation smoothness enforcing smoothness translation apply smoothness considering translation basic point motion smoothness energy function sglobal set points supporting patch set neighboring points image euclidean distance point motion patterns euclidean distance point motion patterns optimization energy function method efficiently combine good generate motion motion pattern result local motion patterns local result motion patterns local result random translation neighboring points rotation axis rotation algorithm energy change completed dense scene flow rgb-d space rmso rmsz aae rmso rmsz aae methods rmso rmsz aae nrdc local global table compared performance proposed apply algorithm middlebury dataset challenging rgb-d images captured kinect cameras distance color local optical flow nrdc threshold ratio normal estimation set threshold ratio supporting area ratio cost computation point set constant cost set standard noise constant weight point middlebury ratio global optimization parameters model data reliability max parameters model depth noise kinect rgb-d max middlebury dataset method middlebury dataset quantitative rgb-d images captured set cameras axis motion axis ground truth motion axis corresponding middlebury color authors suppressed excessive length local scene flow global scene flow ground truth local optical flow nrdc motion projection image middlebury color left images local optical local scene flow global scene flow motion estimation framework ground images middlebury optical flow maps middlebury scene image space optical images ground truth disparity maps frames middlebury reference target rgb-d approach compared three optical flow methods scene flow methods rgb-d scene flow methods point error disparity change error error error computed valid methods estimate scene flow disparity frames middlebury optical flow techniques rmsz computed estimating translational flow depth points flow error values computed provided table method evaluation optical flow scene flow local optical flow local scene flow employed framework methods global scene flow generate motion completed dense scene flow rgb-d space qualitative figure estimated local optical flow nrdc three rgb-d images local scene flow motion result global scene flow motion patterns input result local scene generate accurate kinect rgb-d data apply algorithm frames rgb-d sequence tshirt4 kinect camera rgb-d data captured performance algorithm scene flow method rgb-d flow method surface tracking method large displacement optical flow method based rgb color optical scene flow computed optical flow space camera parameters depth qualitative evaluation motion methods qualitative motion map motion map motion projection image motion depth direction maps based middlebury color coding motion values middlebury color coding map second motion field point reference image point target good motion estimation result point clouds figure estimating motion field frame frame tshirt4 row method optical flow method fail motion marked green surface tracking method method robustly estimates motion difference advantages completed scene flow occlusion modeling employed row registration three three methods fail reference point clouds target point clouds marked green method robustly registration z-motion figure motion rgb-d images waving captured data challenging row methods fail estimating motion field marked green method robustly authors suppressed excessive length input data rgb-d flow surface tracking optical flow method z-motion map xy-motion map rgb-d flow surface tracking optical flow method scene flow frames tshirt4 row input reference target rgb-d depth values row xy-motion maps z-motion maps rgb-d surface optical flow left images extended middlebury color coding maps motion row three basic point clouds reference target data registration motion field view view view completed dense scene flow rgb-d space input data rgb-d flow surface tracking optical flow method motion estimation rgb-d images captured row input reference target rgb-d depth values row xy-motion maps z-motion maps rgb-d surface optical flow left images extended middlebury color coding maps motion quantitative evaluation point rgb-d set frame position displacement points ground truth measure accuracy estimated motion methods sequence time configuration neighboring motion displacement time configuration time configuration considering small displacement large displacement standard deviation error euclidean distance ground small large displacement tshirt4 human hand waving method compared three methods small displacement large displacement motion method better standard deviation proposed method large displacement motion z-motion map xy-motion map authors suppressed excessive length tshirt4 human hand waving rgb-d flow surface tracking optical flow method methods table standard deviation error small large displacement tshirt4 human hand waving framework address challenging scene flow estimation based rgb-d efficiently scene flow motion method address large displacement motion refine local method provide motion considering occlusion local propose matching cost supporting area adaptive robust global explicitly model occlusion estimate occlusion scene flow address occlusion noise missing data rgbd data reliability middlebury challenging kinect rgb-d work kinect rgb-d data sequence scene based scene flow pattern analysis machine ieee transactions range flow kinect springer rgb-d dense motion estimation color ieee international conference ieee scene flow international conference image dense scene flow based depth computer springer correspondence algorithm image acm transactions completed dense scene flow rgb-d space correspondence computer springer computer vision ieee international conference ieee large displacement optical matching variational motion pattern analysis machine ieee transactions scene flow rgb-d computer vision pattern recognition ieee conference ieee recognition pattern analysis machine ieee transactions evaluation optical international journal computer vision scene pattern analysis machine ieee transactions scene computer vision ieee international conference ieee scene flow view variational international journal computer vision variational method scene flow estimation computer ieee international conference ieee dense correspondence image acm transactions acm rgbd computer vision ieee international conference ieee image features international journal computer vision random field pattern analysis machine ieee transactions motion optical flow pattern analysis machine ieee transactions energy approach surface estimation rgbd international conference ieee
cient dense scene flow sparse dense stereo data university university university technique estimating velocity vector field motion visible scene point technique presented consecutive image stereo main position velocity estimation estimate dense variational scene flow yield consistent vectors left decoupling strategy main disparity estimation yield sparse dense achieve approach dense velocity estimates accurate moving scene velocity visible system motion motion scene flow three-dimensional velocity images scene flow computation projection image camera ambiguities camera motion motion scene flow distance moving parked vehicles wedel ambiguities second ambiguities missing structure local parts motion variational framework smoothness velocity allows dense motion missing structure parts motion variational technique order achieve dense estimates motion visible parts scene dense scene flow velocity vector point scene flow camera involves estimating velocity consecutive stereo disparity position velocity estimation disparity estimation consistency involved decoupling motion scene estimation consistency accuracy disparity estimate motion estimation disparity estimation disparity field cient global optimisation point optical flow estimation vector global optimisation motion vectors smaller magnitude optical sub-pixel accuracy variational scene flow computation estimation disparity optical allows technique joint allows real-time computation scene images technique joint scene flow computation achieve accuracy joint estimation work motion vectors optical flow estimation dense well sparse sparse optical flow tracking tracking computational dense optical flow variational based method horn local variational optimisation energy function pixel flow cient dense scene flow framework horn time flow robust image estimated image warping equations variational yield accurate optical flow real-time scene flow involves disparity estimation well estimate change disparity work scene flow joint motion disparity estimation presented energy minimisation constraints dense scene dense scene flow algorithms presented camera flow image point disparity estimation motion method involves optical flow consistent computed best real-time scene flow presented sparse disparity velocity work scene flow algorithm computation method sub-pixel accurate scene flow real-time image dense disparity scene flow field scene flow constraints image motion stereo computation stereo stereo pixel left images rectification images involved points images image point camera system image point left image point image distance camera projection disparity encodes image camera position point equation wedel stereo algorithm disparity order matching left image calculating consistent energy minimisation presented scene flow framework stereo sparse dense stereo constraints image motion consecutive stereo images time optical flow scene flow field projection three-dimensional motion pixel change image stereo image change image change three-dimensional velocity field image change estimation stereo derive constraints estimating intensity left time image position equation left stereo image time flow images constraints optical flow left images disparity image time scene flow dierent optical flow constraints left camera images calculating optical flow left image derive disparity change denote estimated flow left consistency left image variational framework scene flow scene flow estimates constraints computed variational framework energy cient dense scene flow term smoothness term dense scene constraints image data dxdy dxdy dxdy denotes robust function function disparity point camera stereo term equation left second consistency estimated motion left smoothness term local scene flow robust function data term order velocity dxdy smoothness flow disparity change variational dense scene flow estimates disparity minimisation energy div div energy euler-lagrange div wedel denotes denote equations strategy fixed point iteration warping fixed point iteration loop estimated second image warping work images real-time fixed point iterations denotes iteration iterations d0k d0k d0k d0k expressions derive euler-lagrange equations robust fixed point iteration loop expressions div d0k div d0k div d0k iteration fixed point iteration loop expressions iteration computed d0k euler-lagrange equations system iterations computation time dierent figure cient dense scene flow image time 512 512 computational time algorithm time real-time algorithm image ground truth rotating table optical flow disparity change computed sgm stereo colour encodes direction optical flow bottom intensity disparity change parts rms figure high error computed scene disparity images colour areas missing disparity estimates occluded scene flow synthetic ground truth second set real images accuracy algorithms real huguet sphere wedel synthetic ground truth rotating sphere sequence depicted figure sequence sphere sphere 512 512 scene flow method dierent stereo matching sgm smaller correlation stereo accurate stereo algorithm ground truth disparity stereo calculated angular error error denotes ground truth ground truth estimated set calculated calculating calculating set iterations average stereo algorithm flow flow ground truth ground truth sgm sgm correlation correlation census based census based evaluation occluded areas occluded areas huguet table average angular error scene flow rotating sphere stereo algorithms scene flow denotes running sgm best accuracy stereo optical flow evaluation error ground truth flow cient dense scene flow algorithm accuracy computational time table achieve huguet method rms error scene flow smaller disparity estimation achieve real-time table sgm stereo disparity measurements occluded sparse measurements sgm variational dense estimates sgm best computational second ground truth tra scene high areas ground vehicle camera vehicles main left camera system moving image calculated error angular error running average error sequence figure angular error sequence evaluation scene flow real figure figure scene flow real moving scene figure motion camera vehicle depicted figure image sequence depicted scene flow parts vehicle tra estimated moving motion figure scene person parked measurements ground points parked vehicle estimated points person accurate motion well dierent well wedel tra scene colour encodes direction direction intensity magnitude optical flow areas error images denote running rms error bottom scene flow colour encodes direction magnitude scene distance accurate tra scene figure moving bottom accuracy presented variational framework dense scene flow based decoupling disparity estimation velocity consistent motion involved strategy main estimating disparity global optimisation disparity estimation sub-pixel accurate velocity cient dense scene flow dense scene flow tra colour left image encodes distance colour scene flow image vector scene person running parked colour figure dense scene flow approach well synthetic real accurate velocity work consistency velocity high accuracy optical flow estimation based conference computer vision wedel computation variational flow real-time stereo vision tra scene ieee vehicles algorithm rectification stereo vision stereo vision consistent ieee computer horn optical huguet variational method scene flow estimation stereo ieee international conference computer dense motion disparity estimation computer dense estimation optical flow robust ieee image joint international conference pattern ieee computer joint estimation method stereo images image conference cient rectification method ieee international conference computer stereo scene flow estimation global matching moving ieee vehicles cient computation optical flow census tracking point three-dimensional scene ieee pattern based approach optical scene flow structure ieee computer vision pattern ieee computer
int comput vis stereoscopic scene flow computation motion understanding wedel brox vaudrey rabe 2010 2010 2010 optical flow stereo matching propose variational framework estimation stereoscopic scene motion points three-dimensional stereo image proposed algorithm takes account image pairs consecutive times computes depth motion vector point depth estimation motion variational formulation handle sparse dense disparity proposed method depth map computed scene flow computed proposed algorithm runs frame frames second qvga images 240 scene flow intensity consistency input uncertainty measures scene flow scene flow motion reconstruction structure motion motion segmentation image sequences dynamic motion points perform process visual movement objects scene computer vision based methods images single camera well motion estimated sequential optical estimation errors ambiguities optical flow three-dimensional scene motion image motion depth depth scene moving requires motion process complex independently moving objects estimation motion combined independent problem noise local stereo camera system better distance estimate stereo vital additional wedel rabe germany brox germany vaudrey university computer university germany scene flow motion field depicted computed stereo input image images left camera time colour encodes speed stationary moving camera ego-motion note accurate motion estimation running person int comput vis three-dimensional scene ambiguities camera motion camera motion camera points areas missing structure local ambiguities rotating third disparity estimation optical flow common deal missing structure achieve dense estimates variational approaches smoothness dense estimates point estimate figure movement running person scene work motion vectors optical flow estimation sparse tracking perform tracking dense optical flow provided variational based method local variational optimisation energy functional pixel flow basic framework improved time flow robust outliers image larger estimated image equations brox variational accurate optical flow current optical flow methods proposed scene flow computation involves additional disparity estimation well estimating change disparity work scene flow joint motion disparity estimation huguet devernay kambhamettu presented energy minimisation constraints provide dense scene dense scene flow algorithms presented multiple camera allow flow single image approaches real-time table best work scene flow algorithm computation times range yielding discrete gong presented discrete disparity flow scene algorithm range qvga 240 real-time sub-pixel accurate scene flow presented rabe provide sparse disparity displacement real-time scene flow algorithm presented disparity flow algorithm gong gong method method range runs qvga images local int comput vis table scene flow algorithms running range tested algorithm dense approach motion image segmentation kambhamettu three-dimensional scene flow vision motion reconstruction devernay flow decoupled close close close close close close running time stereo method dynamic method note range discrete values range optical flow component disparity method decoupled approach solving disparity scene flow disparity map scene flow error method accurate scene variational flow vector large small vectors close real-time qvga parts work presented three papers disparity estimation motion estimation framework common approach scene flow computation huguet devernay kambhamettu scene flow presented work huguet devernay based brox main contribution propose decoupling motion estimation disparity estimation stereo intensity consistency assumption optical flow scene flow errors real-world conditions input scene flow optical flow solution based residual images provide uncertainty measures object decoupling advantageous decoupling depth motion flow disparity estimation motion estimation disparity estimation disparity epipolar field optimisation methods global dynamic point optical flow requires estimation vector field global optimisation time difference motion vectors smaller magnitude minimising time sub-pixel accuracy provided variational methods motion disparity scene flow computation estimation disparity optical flow disparity allows optimal separate disparity estimation problem motion proposed method involves final scene flow optical flow consistent computed solved joint decoupling strategy allows real-time computation scene flow frame rate qvga images assuming disparity map provided implemented achieve int comput vis approach scene flow times joint scene flow decoupling quality decoupling strategy motion field estimation takes account estimated disparity computation computed motion coupled approaches huguet devernay variables variational optimisation local local coupled coupled energy advantageous formulation disparity estimates based global optimisation variational approach coupled variational estimation accuracy decoupled approach joint estimation paper scene flow energy implementation provide estimating pixel-wise uncertainty calculated follow-on takes image based scene flow real-world metrics likelihoods understanding motion experimental paper final formulation scene flow figure disparity estimation decoupled variational scene flow computation allow efficient optimal stereo disparity compute optical flow change disparity stereo pairs time optical flow disparity change scene disparity estimation disparity calculated pixel position time frame pixel position image current algorithms normal stereo epipolar pixel row left images process matrix stereo camera point depth yielding left image point stereo camera distance camera disparity encodes difference image correspondence left camera parameters position point measurement stereo correspondence algorithm estimate disparity non-occluded pixel left local small matching left global global scene flow algorithm disparity map dense sparse algorithms variational scene flow disparity estimation correlation algorithm yielding sparse sub-pixel accurate disparity runs joos disparity map census based parallel hardware computational consider semi-global matching consistent energy minimisation disparity estimate non-occluded algorithm implemented hardware runs images resolution 640 480 sgm main stereo stereo motion constraints data scene flow algorithm consecutive pairs stereo images time scene flow field optical flow field gong coupled int comput vis pixel position time flow left image image difference component leads flow motion disparity constraints scene flow intensity consistency corresponding points left stereo image successive frames assumption direction additional component disparity three-dimensional scene flow image described temporal change described estimated stereo disparity change optical flow field estimated stereo image equations derived scene normal optical flow intensity consistency intensity images point images scene flow equation derive left intensity left pixel position time leads left flow position component disparity flow disparity change calculating optical flow left image directly derive disparity change denote estimated flow left estimate disparity change consistency left image time values corresponding pixels stereo image time illustrated bottom yields third disparity flow elf erf edf flow image derived intensity scene flow data compute scene flow frame stereo pairs disparity map implicit figure equations real-world stereo int comput vis parameters smoothness optical flow disparity minimisation energy minimising energy compute euler-lagrange elf elf erf erf scene flow constraints illustrated sequential stereo image pairs edf edf div erf erf elf elf energy equations scene flow scene flow estimates constraints computed variational framework minimising energy functional data term derived constraints smoothness term smoothness flow dense estimates image sparse edf edf div edf edf erf erf div constraints data elf edf erf approximation total variation outliers function disparity function disparity data sparse stereo method smoothness term estimate account estimates spatially formulation dense image flow estimates disparity smoothness term local deviations scene flow components robust function data term deal scene flow derivatives algorithm details implicit equations strategy fixed point iteration brox outer fixed point elf erf edf iteration estimated second image estimate derive elf erf edf combined pyramid iterations int comput vis image resolution euler-lagrange equations robust inner fixed point iteration iteration equations implementation details implementation dealing intensity consistency assumption dealing synthetic intensity consistency assumption holds methods proposed deal brox proposed intensity data terms energy intensity common intensity derived image basic idea consider residual difference original image low illumination method well stereo optical flow scene residual image vaudrey residual images real-world images synthetic data original images holds range consistent details residual image implementation example residual image comparing residual image approach constraints brox residual images computational smaller equations energy euler-lagrange quality measure disparity slope disparity cost function quality measure disparity estimate disparity uncertainty scene flow algorithm presented semi-global matching algorithm disparity estimation variational framework scene flow semi-global matching algorithm discrete disparity estimate sgm pixel discrete algorithm energy method pixel left sgm method energy minimisation disparity yields costs sub-pixel estimation sub-pixel accuracy fit function cost standard linear disparity values fit cost minimum cost function costs three disparity basic idea step illustrated example fit yields sub-pixel minimum note minimum underlying energy close evaluating energy pixel position assuming underlying energy function slope function larger cost differences current estimate quality measure slope disparity estimate accurate disparity values slope sub-pixel position disparity expected accurate deviation position larger better expected quality disparity uncertainty scene flow understanding uncertainty estimate field uncertainty derive uncertainty disparity scene flow estimates corresponding underlying energy example residual images input scene flow algorithm original residual image int comput vis plots proposed uncertainty measures corresponding variances left error ground uncertainty distribution uncertainty level true variance uncertainty measure summary scene flow usf bottom based uncertainty measure derived expected variance disparity disparity uncertainty calculated underlying energy scene flow method slope function provide better energy cost stereo uncertainty measures presented provide better additional computational scene flow uncertainty variational optical flow methods idea cost function energy function uncertainty measure complex disparity higher input solution confidence measure proposed idea uncertainty local energy contribution energy compute optical large contribution total energy high uncertainty low uncertainty expected low energy contribution measure yields better approximation optimal confidence optical flow estimates idea scene flow three data terms energy functional disparity flow smoothness scene flow variables energy yields int comput vis expected uncertainty scene flow usf elf erf edf main uncertainty measure provided additional cost intensity data smoothness pixel final final uncertainty comparing variances uncertainty measures uncertainty measures disparity scene flow derived uncertainty measures error disparity optical plots frame evaluation sequence set university auckland plots proposed uncertainty measures true variance variance scene flow component linear function uncertainty fixed parameters image scene flow scene flow methods evaluating scene flow involves scene flow estimating three-dimensional scene flow propose metrics estimating confidence moving points derived image scene flow combined estimation optical flow disparity compute points define point scene equations derived defined -y0 scene flow assume follow-on detection moving segmentation accuracy scene flow define metrics estimate likelihood pixel metrics provided follow-on segmentation example segmentation residual motion likelihood define uncertainty standard deviations standard values assume disparity estimate accurate assume derive covariance matrix scene flow error -y0 -y0 error propagation holds distribution standard deviation problem scene flow estimation estimating errors flow errors disparity change distance likelihood pedestrian running vehicle driving colour encoding point green red low high int comput vis single residual motion likelihood defined left original optical flow residual motion metric red low high likelihood point motion assume parts holds true vehicle matrix matrix error propagation denote standard deviations three-dimensional covariance matrix total vector covariance matrix correlation disparity measurement covariance larger holds true stationary assume motion camera ego-motion estimation method motion camera system total residual motion vector calculated compute likelihood flow vector moving assuming stationary error standard normal distribution covariance matrix deviations assumption evaluating mahalanobis distance residual motion mahalanobis distance outliers distribution three point moving mahalanobis distance holds measurement variances figure demonstrates parts scene parts movement metric point provide speed note metric computes scene examples metric int comput vis speed standard deviation pedestrian running speed vehicle driving forward speed colour encoding green red stationary encoded points distance black speed metrics residual motion likelihood metric estimate speed displacement vector angular error huguet devernay problem points large distances estimated small disparity change yields large residual motion computation derive speed problem variance speed measurement approach estimate variance covariance involves square root ground truth solution root square disparity measure sparse algorithm estimated set ground truth estimated set angular speed metrics leads examples variance speed moving speed pedestrian displacement frame rate vehicle displacement examples high confidence moving objects metrics provided likelihood object moving speed object uncertainty speed evaluation experimental evaluation metrics defined define error metrics time evaluating errors time variance implicit summary synthetic data parts wedel third subsection int comput vis ground truth rotating table left image movement optical flow disparity change computed sgm stereo colour encodes direction optical flow bottom intensity disparity change encoded black parts rms figure high error values computed scene disparity images colour encoded green black areas missing disparity estimates occluded areas table root square average angular error scene flow rotating sphere stereo algorithms input scene flow stereo algorithm ground truth sgm correlation joos census based devernay occluded areas occluded areas evaluation approach stereo real-world algorithm provided segmentation residual motion likelihoods input images pixel rotating sphere sub-pixel accuracy rotating sphere quality scene flow tested synthetic ground truth ground truth rotating sphere sequence huguet devernay depicted sequence sphere sphere resolution huguet devernay sphere tested scene flow method stereo semi-global matching sgm smaller correlation pyramid stereo joos accurate stereo algorithm ground truth disparity ground truth input disparity errors calculated calculating non-occluded calculating huguet devernay pixels included summary table achieve lower errors huguet devernay sparse correlation lower error disparity included computation variational formulation scene data terms dense scene flow int comput vis povray-rendered traffic scene colour encodes direction direction intensity magnitude optical flow areas error images denote larger running huguet devernay generates rms error frames traffic scene top row moving bottom row demonstrates accuracy distances bottom scene flow colour encodes direction magnitude scene distance accurate distances rms error scene flow smaller table sgm good occluded joint approach huguet devernay variational perform well disparity table sgm yields stereo disparity measurements occluded better sparse measurements sgm variational yields dense estimates higher sgm best method hardware computational traffic scene second ground truth example povray-rendered traffic scene auckland scene calculated error angular error defined running devernay average error sequence pixels angular error 640 480 evaluation approach stereo synthetic data subsection traffic scene scene flow complex driving sequential stereo image example scene ground truth optical disparity ego-motion auckland evaluation approach handle large image time povray-rendered traffic scene rms encoded low high flow colour encoded error images encoded points rms error images colour int comput vis subsection error difference estimate ground error variance error provided example scene flow algorithm figure errors object errors image sequence graphs comparing errors scene flow components errors error frames error errors frame occluded areas vehicle image close difficult flow difficult int comput vis rms error error evaluation sequence graphs error variance graphs disparity flow estimates error standard deviation second problem common frame left vision algorithm flow left true motion common optical flow difficult errors standard deviation flow components solved variational energy minimisation sequences provided allow scene flow algorithms real-world scenes real-world scenes complex handle images noise included approach deal real-world scenes images int comput vis figure scene flow stationary left optical flow original image scene flow flow flow vectors vectors green red stationary moving figure scene flow top original image colour encoded scene bottom row points left panning left panning panning vehicle figure example camera scene flow reconstruction good scene flow moving camera ego-motion camera ego-motion estimation depicted figure scene person runs driving forward measurements ground visual points vehicle estimated points person accurate motion well well figure multiple vehicle vectors scene flow vectors figure image sequence driving depicted scene flow parts vehicle traffic estimated moving motion outliers ego-motion accuracy vital dealing moving errors figure driving figure demonstrates scene flow generates direction magnitude segmentation figure example motion likelihood input separate stationary moving points segmentation segmentation graph segmentation algorithm consider uncertainty measures flow vectors image segmentation algorithm requires set graph pixel image node node graph node int comput vis dense scene flow traffic colour scene flow image vector ego-motion red original image real-world example scene left image images scene flow reconstruction colour encoding green red stationary moving left input difference image consecutive residual motion segmentation residual motion likelihood derived scene segmentation moving object camera moving edge costs defined edge edge cost assume assume spatially assume pixel-wise local variance uncertainty usf figure difference energy segmentation top image approach error propagation spatially fixed variances better best pixel-wise local variances estimated uncertainty pixel-wise uncertainty estimation improved fixed likelihood point small motion motion motion detailed algorithm wedel motion likelihood three estimate error propagation left images residual motion likelihoods images segmentation int comput vis figure segmentation independently moving objects moving parallel camera movement additional ground motion generates good segmentation left original image segmentation optical flow energy image red uncertainty measures motion epipolar motion includes objects moving parallel camera camera moving depth includes objects scene flow motion likelihood good segmentation sequences int comput vis figure energy images segmentation objects parallel camera independently moving objects colour resolution 240 640 480 pixels 800 800 800 time table real-time algorithm implemented input images resolution table example motion segmentation running sequence scene flow motion presented efficient motion decoupling disparity estimation estimation process advantageous allows methods achieve higher lower computational presented process illumination differences intensity consistency assumption proposed uncertainty measures well movement motion disparity solving computer vision motion motion stereo motion stereo provide vital account build vision depth motion separate independent paper segmentation independent objects directly scene detailed euler-lagrange equations detailed euler-lagrange equations derived linear elf erf int comput vis div elf erf div erf edf div algorithm scene flow outer iterations compute structure compute vtmp ptmp inner iterations build equation system sor iterations sor step warp warp level algorithm compute structure pixels disparity terms original euler-lagrange equations disparity flow corresponding linear system solved successive implementation scene flow scene flow algorithm implemented speed qvga images 240 implementation allows frame rate computational times outer iterations inner sor iterations pyramid parameters real-time computational time algorithm 640 480 images rate image energy smaller pyramid level close approximation energy higher figure computational time scene flow includes computation image int comput vis algorithm compute pixels algorithm sor step pixels rsum rnorth rsouth rwest reast rnorth rsouth rwest reast rsum auv aup rnorth rsouth rwest reast rsum auv avp rnorth rsouth rwest reast rsum aup avp computed central differences boundary boundary conditions central difference second algorithm outer algorithm computes three-dimensional scene flow forward differences central differences proposed brox values boundary boundary conditions central difference computed warp current scene flow variables temporal variables vtmp ptmp solving original flow variables algorithm directly flow note implementation flow field algorithm inner iterations successive implemented algorithm build equation system pixels elf erf ptmp edf edf erf elf auv aup avp tmp vtmp ptmp vtmp tmp vtmp ptmp vtmp tmp ptmp vtmp ptmp image computer robust approach ego-motion estimation stereo complex motion robust estimation multiple flow computer vision image scene flow algorithm algorithm described algorithms algorithm computes temporal derivatives input images energy derivatives average experimental algorithms energy minimization ieee transactions pattern analysis machine pixels equations image computer high accuracy optical flow estimation based european computer vision confidence measure variational optic flow computation variational optic flow method motion ieee computer vision real-time stereo vision traffic scene ieee ieee computer real-time joint disparity disparity flow estimation computer vision image disparity flow estimation dynamic pattern recognition los ieee computer multiple computer university stereo vision consistent ieee computer vision pattern recognition los ieee computer stereo matching ieee transactions pattern analysis machine optical evaluation stereo ieee computer vision pattern recognition los ieee computer variational method scene flow estimation stereo ieee computer vision los ieee computer dense motion disparity estimation computer computer vision motion transactions energy graph european computer vision distance dense estimation segmentation optical flow robust ieee transactions image int comput vis joint pattern recognition ieee computer joint estimation method stereo images stereoscopic image pattern recognition ieee computer stereo reconstruction scene flow estimation global matching computer detection moving objects complex ieee los ieee computer motion segmentation robust ieee computer vision pattern recognition total variation based noise evaluation dense stereo ieee computer vision los ieee computer sub-pixel estimation ieee computer vision los ieee computer efficient computation optical flow census problem analysis detection tracking point university auckland image sequence analysis differences stereo motion synthetic real-world stereo image vision los ieee computer ieee residual illumination correspondence pattern three-dimensional scene ieee transactions pattern analysis machine improved algorithm optical papers approaches visual motion analysis efficient dense scene flow sparse dense stereo european computer vision evaluation approach scene flow decoupled motion papers approaches visual motion analysis int comput vis detection segmentation independently moving objects dense scene energy minimization methods computer vision pattern recognition motion estimation total variation ieee computer vision pattern recognition los ieee computer framework motion european computer vision solution large linear based approach optical scene flow structure ieee computer vision pattern recognition los ieee computer optic energy minimization methods computer vision pattern recognition
2015 ieee international conference computer vision large displacement scene flow occlusion reasoning accurate rgb-d sensors approaches estimate scene scene flow formulation relies explicit geometric reasoning large displacements model local motion rigidity level point cloud explicitly parameters integrating geometric components energy defined method large challenging mpi sintel flow augmented depth explicitly modeling large displacements handle difficult sequences state art scene flow integrating depth correspondence fields improved spatial support sharper boundaries state large-displacement optical flow scene flow field observed depth color rgb depth stereo multiple depth sensors motion field computer vision scene estimating motion field work estimating optical flow sensors 2015 ieee appearance augmented depth additional reasoning based point cloud scene motion rgb-d frames large displacements motion difficult estimate large-displacement correspondences rely local rigidity point cloud neighborhoods rigid order observed anchors geometric terms based local rigidity augmented constraints appearance point appearance points correspondence depth constraints estimated solution target point motion field geometric constraints large based depth occlusion estimates order structure pass work scene flow handle large explicit occlusion integrates largedisplacement geometric model respect state art approaches variational optical flow scene flow correspondences local rigid explicitly smoothness motion scene consistency rgb-d input work scene flow work motion field computational approaches stereo camera scene flow consistency observed optical flow estimation structure relying local scene rigidity work depth estimating structure intensity depth optical flow formulation depth integrated additional variational depth flow estimated based observed data smoothness camera explicit optical depth consistency scene flow based range constrains motion image method range define function local scene flow constraint intensity depth method handle large regions order dense scene motion field handling rigid scene flow neighborhoods appearance approach handling large relying anchors geometric correspondences energy model model local rigidity assumptions smoothness point cloud modeling motion camera performed large matching initialize anchors flow setting corresponding energy rely initial anchors estimating complete energy formulation geometric appearance relies scene flow robust penalty terms gradient constancy motion neighborhood structure points depth variational applied component motion field order motion depth estimate scene flow point cloud representation large set motion generated multiple solutions additional computational model formulation work rgb-d images rgb depth images corresponding depth maps projection function scene consistent rgb-d input multiple local function point image camera function vector depth component notice point point cloud measured inverse projection function point image plane corresponding map define associated matrix locations points scene flow estimation corresponding locations second frame displacement field solution dense correspondence assumptions local rigidity displacement appearance consistency reasoning local rigidity assumption points object rigid geometric relying rigid transformation applied point rotation matrix rotation vector occlusion reasoning depth reasoning image locations based current model correspondences processing level approaches geometric occlusion presented occlusion maps define occlusion term occ rely rigid order define parameter vector rotation rotation exponential map transformation exponential map defined applied values occ algorithm points based depth frame points correspondence depth depth disparity point second point initial occlusion states image points point algorithm geometric occlusion reasoning algorithm based correspondences estimates frame frame parameter set algorithm integrates data initial measured corresponding target algorithm initial depth occlusion state initial point depth occlusion points notice optical flow quaternion vector component quaternion corresponding identity performed quaternion neighborhood structure neighborhood local rigid motion case well visible second image case integrating occlusion data term points geometric constrains wij wij wij parameters values occlusion map occ function motion field rigid parameter geometric model field rigid rigid parameters neighborhood point neighborhood closest points pyramid neighborhoods large image range image neighbours object large-displacement initial correspondences large-displacement matching define energy term constrains solution wij figure occlusion maps generated well estimates robustness estimate effective case work object support explicit based depth values point depth geometric reasoning effective case training occlusion based effective occlusion states image points point algorithm geometric occlusion reasoning algorithm based correspondences estimates frame frame dij dji dji robust error parameters penalty defined locally rigid geometric define energy component neighborhood rigid wij energy components dense energy model handle large displacement model terms energy relies geometric large-displacement correspondence local rigidity assumptions defined large neighborhoods rgb-d point appearance depth constancy geometric occlusion estimates order neighborhood regions correspondences component complete algorithm detail optimization notice smoothness rigid parameters points neighborhoods large well robustness large additional appearance depth constancy terms order solutions observed appearance appearance points constraint neighborhoods disparity integrated wij model estimated data robust error penalty dji function color rgb energy constraint assumption regions locally assumption local appearance rigid displacement respect consistent projection image terms color gradient projection rigid transformation second parameter fields arg min arg min optimization complete scene flow algorithm complete scene flow rely based processing image processing level define locations parameter fields associated computational estimating largedisplacement order initial set interpolate displacements closest order dense field initial points large-displacement dense matching method largedisplacement correspondences inverse projection order locations image set depth closest inverse rigid parameter field setting exponential parameters identity translations set level optimize energy defined estimates solutions neighbours point cloud points associated target frame function pass parameter fields processing level interpolate translations closest neighbours interpolate corresponding translations compute motion interpolation initialize rigid parameter field neighbours exponential map components method energy displacements large neighborhood matching term model depth constancy point cloud associated scene estimated local translations point cloud estimates appearance penalty constrains energy component integrates disparity depth measured wij dji dji depth component point rigid point frame local rigid image plane frame depth map point scene flow optimization complete energy model consistency solution respect observed data locally energy occlusion estimate energy model methods visible points cloud target algorithm presented dense energy energy model optimize estimated pass quaternion representation exponential interpolation function map component rigid initialize large-displacement correspondences local rigid parameters pyramid level estimate occlusion optimize optimize pass level algorithm computational rgb-d scene experiments image pyramid data optimization rgb-d training rgb-d sequences dataset depth well rgb augmented accurate mpi sintel parameters dataset motion appearance frames camera motion frames large selected challenging sequences mpi sintel dataset set image pairs training image pairs ground truth motion field visible dataset computer generated sequences difficult large motion depth data set training selected corresponding training pairs selected method image computational energy neighborhood image set experiments pairs frames robustness methods motion observed large displacement optical flow method offers second set experiments state art methods scene flow corresponding occlusion estimates accurate challenging methods ground truth spatial support sharper boundaries error sintel displacement difficult based sintel flow occlusion representation terms state art methods challenging sintel dataset method offers improved well accurate estimates presented model compute dense scene rgb-d images matching approach consistent figure scene flow estimation plane image pairs sintel displacements scene 2nd largedisplacement optical scene flow method relying large-displacement anchors displacements notice improved methods handling detail figure pairs images methods sintel input images 2nd state art scene flow method handle large approach sharper boundaries improved spatial support ground truth optical ground truth occlusion states locally rigid formulation optimization energy handle large explicit geometric occlusion integrated energy offers robustness correspondences explicitly modeling large displacements work difficult sequences current state art scene flow depth correspondence fields state art large-displacement optical flow exponential map parameters optimize setting optimization rigid motion parameters wij wij wij wij wij wij matrix function 2nd order current estimate wij wij dij dij wij correspondence field arg min component optimization performed second work identity compute gradient color depth map second segmentation rgb-d proceedings computer vision pattern method international large displacement optical proceedings computer vision pattern optical flow estimation based computer optical flow computer exponential recognition rgb-d proceedings ieee conference computer vision pattern based scene flow depth computer vision ieee international conference rgb-d dense motion estimation color 2013 ieee international conference optical variational method scene flow estimation stereo computer ieee international conference object rgb-d international locally matching motion occlusion computer vision 2013 ieee international conference flow depth color machine vision dense correspondence ieee pattern machine segmentation support proceedings conference computer dense scene flow estimation computer scene flow intensity depth computer vision pattern recognition ieee computer conference scene flow international conference image interpolation correspondences optical proceedings ieee conference computer vision pattern rgb-d scene proceedings ieee conference computer vision pattern dense range flow depth intensity pattern international conference optical flow estimation computer vision pattern recognition ieee conference segmentation optical flow estimation computer vision pattern recognition ieee conference scene computer proceedings ieee international conference scene flow estimation rigid motion computer vision ieee international conference rigid scene computer vision 2013 ieee international conference dense scene flow dense stereo large displacement optical flow computer vision 2013 ieee international conference optical machine vision motion detail optical flow ieee pattern machine 2015 international conference
integrated scene flow structure recovery multiview image sequences image motion eld points image sequences stereo camera system scene order assume non-rigid motion images small assume small region non-linear motion model tting base optical constr stereo constr image region order simultaneously estimate motion correspondences regularization constr recursive algorithm designed local regularization constr synthetic real integrated motion structure analysis optical motion eld image term scene represent dense motion vector eld point scene potential scene structure dynamic automatic image sequences erent system simultaneously scene structure bene assume priori kno wledge dynamic assume scene motion 2000 ieee motion structure recovery reco motion structure monocular view image sequences scene recovered work stereo vision recovery dense scene structure multiview image sequences monocular motion analysis stereo vision considered eac monocular motion analysis point non-linear motion structure stereo vision solve correspondence matching betw een stereo image scene motion stereo assuming scene considered motion stereo described stereo motion structure integrated recover rigid method recover motion analysis bene stereo computed rigid motion parameters assuming depth computed stereo betw een cameras analyze work unknown motion method een binocular velocity non-linear method presented simultaneously compute motion cameras reco points stereo image reco stereo correspondence motion stereo approach motion analysis stereo proposed translational motion parameters binocular image stereo correspondences motion motion translational designed linear algorithm rst estimate rigid motion optimal function initial nonrigid motion motion integration non-rigid motion algorithm dense model monocular model-based approach priori kno wledge stereo non-rigid motion analysis designed analyze computed motion stereo correspondences ned computed non-rigid scene designed linear algorithms erent multiview optical estimate scene scene structure scene work method stereo scene structure abo structure motion motion bene 2000 ieee motion stereo analyses erent integration reco scene dense scene structure multiview image priori kno wledge scene assume scene formulation erent solve problem non-linear model work formulation simultaneously recover motion structure integration motion stereo constraints complete automatic system scene dense scene structure images rst small assume region motion represented non-linear method motion model small regularization constraints ensure recursive algorithm presented incorporate system problem motion stereo multiple camera motion model local model tting presented regularization constraints complete recursive algorithm presented future approach system presented figure assume cameras optical stereo constraints regularization constraints model small scene correspondences dense scene structure simultaneously multiple camera algorithms stereo analysis proposed multiple cameras cameras basic stereo cameras pro integrated system multiple camera camera camera camera image sequence image sequence image sequence optical flow optical flow optical flow stereo constraints model regularization constraints scene flow correspondences dense scene structure motion model image matching model estimate image zhou model analyze non-rigid motion indicating model non-rigid model describe non-rigid motion point frame represented vector assume point frame motion model represented figure system set cameras pro images basic stereo basic view compute scene disparity image point point image plane camera camera camera stereo point image assigned disparity depth point eac base image point disparity set stereo correspondences image cameras represented order describe motion motion model describe non-rigid motion 2000 ieee local motion model model non-rigid motion problem motion structure structure analyses model tting motion constraints stereo constraints considered model motion structure analyses motion model image order estimate point non-rigid motion eld model assume point images local optimal segmentation points priori motion images optical nonrigid motion segmentation image incorporate images local region small approach good tting ensure small constraints model zhou constraints assuming smoothness assuming motion region successive frames smooth betw een motion local region successive frames ned successive frames represented optical image sequence eac camera rst method described motion denote optical point image plane step eof local motion model tting optical stereo frame base image point assigned disparity point scene frame frame motion point image plane camera computed optical flow stereo constraints unknowns successive frames small non-linear zhou local regions monocular image multiview image constraints eac linear method estimate local algorithms algorithm model unknowns small unknowns local region motion smooth successive unknowns unknown vector represented motion model optical point image plane camera represented function optical motion optical constraint represented depth rst basic local model tting stereo constraint betw potential stereo potential stereo correspondence camera camera denote corel range corel well assigned good optimal unknown vector eof tting function good eof local constraints optical stereo regularization constraints complete recursive algorithm constraints 2000 ieee corel optical stereo constraints figure local region tting successive frames ned ensure robust local optical error eof corel scene frame frame basic view frame camera camera camera reference stereo figure constraints local stereo constraints optical constraints optical constraint motion eld solve recursive eof optical stereo constraints considered order estimate motion structure simultaneously algorithm numerical algorithms well algorithm solve initial guess unknown vector assume small motion een adjacent frames motion parameters initial depth guess frame zhou assume depth rst frame initial rst frame depth computed stereo motion eld optical regularization constraint motion compute optical smoothness constraints motion optical experiments image small image regularization performed point regularization term order smoothness term ned initial denote image image point small abo smooth motion local constraint regularization constraints model small region velocity adjacent regions recursive rst well numerical local local range structure small motion depth points frames 2000 ieee incorporate abo recursive algorithm designed algorithm algorithm recursive algorithm indicating range constraint eof local model eof algorithm recursive algorithm scene flow structure recovery depth motion set constraint regions step local model solve region compute adjacent set local model solve region figure synthetic image synthetic image sequences ned structure frame recovered structure frames guess initial structure error figure structure error rst frame indicating motion analysis structure order test performed experiments multiview image image sequences figure synthetic reco initial depth guess frame computed algorithm proposed error structure frame figure structure error figure recovered scene system motion motion analysis bene stereo constraints stereo analysis gaussian noise initial depth 2000 ieee synthetic scene experiments order test approach performed experiments real scene figure image sequences system image sequence reference camera sequences stereo camera real image sequences camera test image sequences initial stereo correspondences optical figure recovered scene structure scene moving scene recovered structure moving described complete automatic system scene structure real scene future work gaussian noise gaussian noise constraints priori kno model scene robust structure error frames pro 3-d motion point point nonrigid motion structure image sequences motion depth rst step motion rigid motion depth velocity binocular image model-based motion optical robust estimation stereo structure stereo correspondence motion algorithm robust algorithms motion estimation stereo image eld segmentation motion estimation robust scene figure structure recovery synthetic image sequences figure recovered scene synthetic scene scene moving camera moving motion stereo constraints simultaneously computed motion motion stereo bene motion structure analyses structure analysis system synthetic system robust scene structure reco scene dynamic stereo matching matching constraints system future work stereo matching 2000 ieee frame frame frame frame figure real image image sequence reference camera recovered scene structure recovered scene frames represent represent optical regularization nonrigid motion stereo motion 3-d motion estimation model-based image 3-d translational motion structure binocular image dynamic structure stereo model-based motion structure estimation stereo 2000 ieee nonrigid reco motion numerical structure stereo formulation stereo correspondence scene binocular image optimal motion structure nonrigid motion structure image sequences
scene flow structure zhang image novel algorithms computing dense scene flow multiview image sequences hierarchical rule-based stereo matching algorithm presented estimate initial disparity constraints multiview camera setup proposed motion estimation formulations scene flow formulation assumes initial disparity map accurate image segmentation maintain motion depth compute scene flow structure point reference novel hard constraints paper algorithms accurate experimental applying algorithms real motion structure estimation algorithms proposed assume scene motion parameters work computing dense motion field multiview image linear algorithms compute scene flow three optical flow view estimate scene scene structure scene computing optical flow algorithms optical flow linear algorithms zhang computed scene flow structure integrated motion model local image adaptive smoothness constraint applied image formulation occluded areas algorithm produce motion structure estimate motion structure multiview image stereo motion constraints combining constraints multiview image sequences points reference image invisible algorithm constraints occluded deal method compute scene flow method occluded scene flow structure reliable motion depth hierarchical rule-based stereo matching algorithm image segmentation enforce depth large motion structure motion estimation compute optical motion frames image optical flow optical flow projected motion clear optical optical flow scene optical scene flow defined point reference difference scene flow field multiview camera setup compute reliable scene number stereo matching algorithms proposed experimental stereo algorithms stereo algorithms produce disparity occlusion map confidence map algorithm work described image segment planar region local local model motion model planar depth image well planar assumption real assumption set rules guide image set rules find occluded areas output algorithm includes disparity occlusion map confidence formulate scene flow estimation energy minimization algorithm thought natural optical flow estimation multiview optical flow constraints views combined scene formulations formulation assumes initial depth map computed stereo matching algorithm accurate novel hard motion constraints hard constraints initial estimation motion high confidence algorithm accurate assume cameras standard set reference camera parameters image sequences captured cameras well easy combine constraints assume motion depth image segment smoothness constraint image note smoothness constraint applied entire method maintain sharp motion depth paper proposed image segmentation discussed stereo matching algorithm described motion constraints hard constraints formulations presented experimental proposed method real suppose cameras reference camera image sequences captured camera denoted disparity point frame reference view denoted scene flow point denoted optical flow defined disparity motion compute scene flow structure multiview image natural thought compute optical flow disparity reference image scene flow optical flow disparity accurate optical flow disparity easy optical flow constraints stereo matching temporal temporal views motion structure scene easily optical flow views scene flow corresponding image corresponding object point occluded projected optical flow accurate scene flow scene flow guide stereo matching algorithms image segmentation experiments described paper image segmentation proposed discussed assume large motion disparity image segment smoothness constraint applied image smoothness regions region enforce smoothness smooth regions regions easily problem motion structure estimation reliable regions smoothness valid matches valid points large standard small initial disparity map segment reference image small image segment figure segmentation valid invalid smoothness applied entire sharp image segmentation algorithm figure initial stereo matching reliable scene initial disparity map smooth produce region smooth disparity values small surface detected initial stereo matching algorithm occluded confidence measurement computed disparity image discussed occlusion confidence scene flow easy stereo algorithm work image hierarchical rule-based stereo matching set rules depth hypothesis defined guide matching output includes disparity occlusion confidence computed correlation volume image disparity matching correlation valid matches depth corresponding points matching compute valid sparse initial disparity views produce valid higher matching score matching score correlation volume initial valid disparity strategy valid disparity points segment invalid high confidence disparity map segment invalid method assumption image segments valid disparity points dense experiments assumption set rules stereo hypothesis rule equals disparity segment filled interpolating segment rule equals search segment equals disparity intensity segment number find segment disparity segment disparity segment filled interpolating segments large set rule equals invalid disparity segment image segment views disparity matching score image image matching score corresponding valid positive disparity segment filled interpolating segments large set rule rule set equals interpolation algorithm energy function defined valid disparity segment computed correlation score valid disparity set rule positive smoothness rule rule defined hierarchical assign maximum confidence values three segments assign valid segments invalid confidence maximum segment assign confidence measurement matching confidence occluded areas detected confidence confidence measurement point point stereo matching algorithm figure motion constraints algorithm combine optical flow constraints optical flow constraint camera represented image intensity small positive linear function defined motion constraints maximum adaptive rule rule enforce depth rules interpolation applied smoothness model applied segment adaptive defined rule rule applied image rules applied segment moving rule measurement segments difference image segmentation intensity segment image disparity segment surface disparity segments rule hypothesis rule rule deal rule rule higher invalid segments matching score applying rules rule segments valid confidence map occlusion map disparity confidence measurement point aii aii aii suppose cameras standard set reference easy combine optical flow suppose camera frame object point camera reference camera projection position point image camera frame camera set combine optical flow constraints suppose cameras standard set frame point projected reference camera projection position frame point projected camera projection position camera optical flow point camera represented combined motion constraint camera energy function defined smoothness term defined confidence measurement disparity stereo matching camera cameras motion constraints combine optical flow constraints multiview occlusion optical flow constraints cameras note object point cameras reference combined motion constraint optical flow hard constraints initial stereo easily hard constraints large number points high confidence disparity map hard constraints temporal correlation consecutive suppose point reference frame search correspondence point frame search maximum frames find correspondence correspondence points motion measurement motion image point valid hard constraint point temporal represented valid motion large matching score motion correspondence valid motion formulation scene flow estimation assume initial disparity map accurate formulate problem image sequences captured cameras accurate initial disparity compute scene flow point reference combining motion constraints hard minimize energy function image segments enforce motion smooth flow field multiview unknowns segment reference image invisible segment reference camera invisible constraints optical flow constraint smoothness point segment three unknowns well defined constraints algorithm deal occluded strategy minimize minimize energy function entire occluded initial values minimize energy function segments occluded second experiments strategy maintain motion method method minimize search clear energy function optical flow energy formulation thought natural optical flow multiview difference optical flow constraints views minimization hard constraints algorithm formulation integrated scene flow structure estimation formulation assumes accurate disparity map initial stereo initial stereo matching stereo matching problem discussed stereo matching algorithm temporal motion disparity formulate problem computing point reference initial disparity initial occlusion number formulation unknowns constraints algorithm three cameras formulation three optical flow smoothness hard constraints correlation volume computed initial stereo constraint disparity image point correlation function disparity energy stereo defined optical flow compute optical flow reference initial energy algorithm experimental positive point occluded corresponding high confidence measurement initial disparity energy term initial disparity confidence energy function defined energy function formulation smoothness term smoothness measurement disparity defined strategy discussed minimize energy function image implemented proposed algorithm digiclops multiview image digiclops three cameras standard image sequences image sequences camera applied stereo matching algorithm left top view initial valid matches applied direct method based three views disparity map second output valid segments applied rule clear algorithm depth direct method algorithm dense disparity map occlusion confidence measurement regions view invisible left clear algorithm occluded areas dense confidence map corresponding disparity brighter disparity note occlusion confidence map based three views based image disparity maximum disparity large disparity occlusion figure reliable occluded three camera top view occluded computing sparse valid disparity map correlation segments valid point set implemented formulations estimate scene described figure consecutive reference left hand moving left hand moving motion small natural hand applied stereo matching algorithm initial disparity map stereo set parameters compute figure algorithm sharp depth applying formulation projected motion scene flow reference brighter object moving dense scene flow left hand moving left hand moving formulation formulation formulation moving areas frames small motion large motion areas camera formulation optical flow image estimation local energy minimization image structure image segmentation local combining stereo work presented novel method estimate scene flow structure multiview image hierarchical rule-based stereo matching algorithm proposed estimate initial disparity constraints multiview setup combine optical flow constraints hard stereo algorithm accurate algorithm constraints minimization formulations estimate scene flow structure algorithms implemented applied real experimental work includes hierarchical function minimization experiments formulations compute dense depth depth optical algorithms motion estimation based stereo image stereo matching algorithm adaptive stereo depth confidence estimation motion surface interpolation hierarchical experimental stereo matching algorithm measurement segmentation based scene image zhang integrated scene flow structure multiview image stereo matching occlusion figure stereo digiclops left top view sparse disparity map generated valid points three views dense disparity map generated direct second output stereo matching segments applied rule dense disparity detected regions reference view occluded left confidence measurement map point brighter higher figure scene flow structure consecutive views estimate scene flow views disparity map generated proposed stereo matching disparity map generated formulation scene flow formulation formulation projected scene flow reference
ieee transactions cybernetics--part august 2003 3-d scene flow structure recovery multiview image sequences zhang novel systems dense three-dimensional scene flow structure multiview image sequences described assume scene nonrigid motion integrated model-based system assumes small local image region 3-d affine motion model fitting based optical flow constraints stereo constraints local region order simultaneously estimate 3-d motion correspondences second system based extended gradient-based system natural extension optical flow hierarchical rule-based stereo matching algorithm estimate initial disparity constraints multiview camera utilized proposed motion image segmentation maintain motion depth framework formulations 3-d scene flow structure formulation assumes initial disparity map experimental synthetic real demonstrate 3-d motion structure recovery empirical comparison ims egs scene scene 3-d affine motion camera compute reliable 3-d scene novel integrated system extended gradient-based system compute dense 3-d scene flow structure multiple video systems presented describe formulations report experimental empirical comparison ims previous work 3-d motion structure monocular view image sequences scene recovered work stereo vision recovery dense scene structure multiview images monocular motion analysis stereo vision considered monocular motion analysis point nonlinear computation 3-d motion difficult structure stereo vision solve correspondence matching stereo image scene natural consider motion stereo assuming scene considered fusing motion stereo better described stereo motion structure integrated recover 3-d rigid method recover 3-d analysis stereo simply computed rigid motion parameters assuming depth computed stereo difference flow left cameras analyze unknown motion method binocular velocity nonlinear method presented simultaneously compute motion cameras recover surface structure computer motion estimation compute optical motion frames image optical flow optical flow projected motion clear dynamic 2-d optical optical flow 3-d 3-d scene introduced optical 3-d scene flow defined point reference difference velocity vector scene flow field multiview received work science computer university object 2003 ieee zhang three-dimensional scene flow structure utilized points stereo image recovered stereo correspondence motion stereo approach motion analysis stereo proposed fusing translational motion parameters binocular image stereo correspondences estimated knowledge motion assumption compared 3-d motion translational designed algorithm estimate rigid motion optimal function previous initial nonrigid motion motion integration nonrigid motion algorithm temporal provide dense deformable model monocular model-based approach priori knowledge kambhamettu coupled stereo nonrigid motion analysis multi-resolution designed hierarchical framework analyze time-varying cloud computed motion stereo correspondences defined computed nonrigid 3-d scene designed algorithms three multiview optical flow estimate scene scene structure estimated scene work method stereo constraints utilized scene structure presented algorithm nonrigid object time-varying multi-resolution surface image stereo object utilized dense scene structure motion motion 3-d motion stereo analyses integration essentially coupled generally estimate 3-d motion structure multiview image stereo motion constraints constraints multiview image sequences points reference image visible algorithm constraints occluded recover dense 3-d scene flow scene structure multiple video priori knowledge scene assume scene designed systems compute 3-d scene flow 3-d scene flow structure recovery order assume local image region motion represented 3-d affine method fit motion model small main ims formulation simultaneously recover 3-d motion integration 2-d motion stereo second natural extension 2-d optical flow design hierarchical rule-based stereo matching algorithm image segmentation enforce depth formulate 3-d scene flow estimation energy minimization problem based optical flow constraints formulations formulation assumes initial depth map computed stereo matching algorithm accurate main egs include stereo matching algorithm smooth disparity maps explicitly design system dimensional optical flow computation three dimensional method image segmentation maintain reliable motion depth rest integrated model-based multiple camera discussed 3-d affine motion model local model fitting presented initial constraints introduced recursive algorithm proposed image segmentation discussed stereo matching algorithm described motion constraints hard constraints formulations egs presented experimental proposed systems synthetic real empirical comparison ims egs work ims block diagram ims presented assume cameras optical stereo constraints regularization constraints fit 3-d affine model small 3-d scene 3-d correspondences dense scene structure simultaneously ieee transactions cybernetics--part august 2003 multiple camera multiple camera algorithms stereo analysis proposed utilize multiple cameras cameras reference basic stereo cameras provide set cameras images basic view basic stereo compute 3-d scene flow disparity map image 3-d point coordinates coordinates image plane camera point projection camera camera process stereo point image assigned disparity depth coordinates 3-d point disparity base image point stereo correspondences set represented image cameras local motion model order describe 3-d motion motion model describe nonrigid motion 2-d affine motion model image matching 2-d affine model estimate image zhou 3-d affine model analyze cloud nonrigid motion affine model nonrigid utilize 3-d affine model describe nonrigid motion consider 3-d point frame represented assume vector point moves position frame affine motion model represented 3-d affine model combine nonrigid motion structure problem motion structure analyses model fitting motion constraints stereo constraints considered model motion structure analyses motion model image order estimate 3-d point nonrigid motion field affine model assume point segment images local optimal segmentation points priori knowledge motion segment images optical flow nonrigid motion segmentation image segment images local region small approach good small constraints nonlinear model zhou introduced constraints assuming smoothness assuming temporal motion region successive frames smooth difference motion local region successive frames defined successive frames represented equation number unknowns successive frames small nonlinear motion model fitting zhou discussed fit affine local regions monocular image multiview image constraints nonlinear method estimate local gradient search algorithms find algorithm best model unknowns small unknowns assume local region motion smooth successive unknowns unknown vector represented zhang three-dimensional scene flow structure block diagram block diagram design eof local motion model fitting optical flow stereo frame base image point assigned disparity scene 3-d point 2-d motion frame frame image plane camera computed point optical flow point plane camera represented function constraints local stereo optical flow image depth basic local model fitting optimal unknown vector function define good eof rest local constraints optical flow stereo regularization constraints recursive algorithm constraints optical flow stereo optical flow image sequence camera method described preserve discontinuity motion denote optical flow point image plane step optical flow projected 2-d motion optical flow constraint represented stereo constraint essentially similarity measurement potential stereo potential stereo camera camera denote range well assigned good frame optical flow stereo constraints illustrated local region successive frames defined ieee transactions cybernetics--part august 2003 robust generally variation local optical flow error optical flow constraint motion field similarity equation solve recursive eof optical flow stereo constraints considered order estimate 3-d motion structure simultaneously initial algorithm solve numerical algorithms find algorithm best solve initial guess unknown assume small motion vector adjacent frames assumption motion parameters initial depth guess frame zhou simply assume accurate depth frame initial frame depth computed stereo regularization constraints affine model small region field noisy optical flow deal regularization constraint motion compute optical well smoothness constraints motion preserve discontinuity optical flow experiments image intensity reliable small image high regularization performed propagate flow point regularization term order discontinuity smoothness term defined denote image disparity velocity image point small segmentation constraint smooth motion local constraint velocity adjacent regions previous equation applied recursive well numerical local local search range structure small motion define constraint depth values corresponding points positive frames range large constraint eof local model eof recursive algorithm recursive algorithm designed algorithm algorithm algorithm recursive algorithm 3-d scene flow structure recovery depth map motion set constraint threshold maximum number regions step zhang three-dimensional scene flow structure stereo snapshot point triclops left view disparity map generated valid points three views disparity map generated direct second stereo matching segments valid applied rule dense disparity detected regions reference view occluded left confidence measurement map higher local model add solve region local model add solve region compute adjacent set image segmentation experiments performed egs image segmentation proposed discussed assume large motion disparity discontinuities image segment smoothness constraint applied image smoothness regions region enforce smoothness smooth regions regions easily problem motion structure estimation reliable regions smoothness smoothness applied entire image segmentation algorithm initial stereo matching reliable 3-d scene initial disparity map smooth produce region smooth disparity values small surface detected initial stereo matching algorithm explicitly report occluded area provide confidence measurement computed disparity image discussed occlusion confidence 3-d scene flow easy stereo algorithm work image error hierarchical rule-based stereo matching set rules extended gradient-based system block diagram egs combined optical flow constraints compute dense 3-d scene flow assume cameras standard set reference camera camera parameters image sequences captured cameras well easy combine constraints views assume motion depth image segment smoothness constraint image note smoothness constraint applied entire method maintain motion depth images captured camera denoted disparity point frame reference view denoted 3-d scene flow point denoted optical flow defined disparity motion ieee transactions cybernetics--part august 2003 valid points large standard small initial disparity map segment reference image small image segment structure synthetic image synthetic image illustrate scene structure recovered depth hypothesis defined matching disparity occlusion confidence computed correlation volume matching correlation images consider valid matches depth corresponding points matching compute valid disparities initial disparity views produce valid higher matching score matching score correlation volume initial valid disparity strategy suggested valid matches valid disparity points segment set invalid valid high confidence disparity map segment invalid semivalid method assumption image segments valid disparity points dense experiments assumption image segmentation segment segments reliable stereo segments define set rules stereo hypothesis process rules process valid utilize valid segments segments reliable define rule valid rule semivalid segments valid rules rest semivalid invalid equals disparity filled interpolating segment equals search rule segment equals disparity intensity segment number find segment disparity compared segment disparity segment filled interpolating segments large set equals invalid disparity segment valid image segment views disparity matching score image image matching score corresponding valid positive set disparity segment filled zhang three-dimensional scene flow structure table structure recovery evaluation interpolating segments large set rule rule set equals interpolation algorithm details rule rule applied image rules applied segment moving rule similarity measurement segments simply difference image segmentation intensity segment image disparity variation segment surface disparity variation segments rule hypothesis rule good rule designed deal rule rule higher invalid segments matching score sequence rules rule segments valid confidence map occlusion map disparity confidence measurement point assigned hierarchical assign maximum confidence values three segments assign valid semivalid segments invalid confidence maximum confidence segment assign confidence measurement matching confidence occluded detected threshold confidence confidence measurement point threshold point stereo matching algorithm image occluded area large algorithm good stereo occluded motion constraints motion constraints algorithm combine optical flow constraints optical flow constraint camera represented suppose cameras standard set reference easy combine optical flow suppose camera frame 3-d object point camera reference camera projection position point image frame plane camera camera set combine optical flow constraints standard set cameras ieee transactions cybernetics--part august 2003 combined motion constraint camera represented confidence measurement disparity stereo matching visible camera cameras motion constraints combine optical flow constraints multiview occlusion optical flow constraints cameras note object point visible cameras reference combined motion constraint optical flow hard constraints initial stereo easily add hard constraints interpolation large points high confidence disparity map hard constraints temporal correlation suppose point reference frame search correspondence point frame search area maximum frames find correspondence corresponding points motion measurement image point consider 2-d motion valid hard constraint point temporal represented valid motion large matching score motion correspondence valid motion formulation 3-d scene flow estimation assume initial disparity map accurate formulate problem image sequences cameras accurate initial disparity point compute 3-d scene flow reference denote formulation motion constraints hard energy function recovered 3-d scene flow synthetic left images projected 3-d scene images 3-d scene flow moving moving base projected frame 3-d point projection position reference camera point projected frame projection position camera camera point camera optical flow zhang three-dimensional scene flow structure energy function gradient-based optical flow energy formulation natural extension optical flow computation multiview difference optical flow constraints views minimization hard constraints algorithm formulation integrated 3-d scene flow structure estimation previous formulation assumes accurate disparity map initial stereo initial stereo matching noisy stereo matching essentially problem discussed stereo matching algorithm temporal motion better disparity formulate problem dimensional vector point reference initial disparity initial denote formulation occlusion number formulation difficult solve unknowns constraints algorithm three cameras formulation three optical flow smoothness hard constraints computed correlation volume initial stereo constraint disparity image point correlation disparity dimensional function energy stereo defined positive point occluded corresponding high confidence measurement initial disparity add energy term initial disparity confidence energy function defined energy function formulation smoothness term include smoothness measurement disparity defined three-dimensional scene flow recovery error error error defined previous smoothness term defined minimize energy function image segments enforce motion recovery smooth flow field difficult multiview constraints unknowns segment reference image segment reference camera constraints optical flow constraint smoothness point order solve three unknowns well defined constraints algorithm propagate deal occluded multi-resolution strategy minimize minimize energy function entire propagate occluded step initial values minimize energy function segments occluded second step experiments strategy maintain motion suggested method gradient method minimize gradient search clear ieee transactions cybernetics--part august 2003 snapshot images strategy discussed minimize energy function image optical flow compute optical flow reference initial guess energy algorithm experiments demonstrate systems applied synthetic real rule-based stereo matching stereo matching algorithms systems compute initial disparity synthetic scene image sequence deformable generated synthetic sphere moves camera sphere moves camera order test systems details artificial pits bumps sphere recovered structure computed ims recovered structure motion object deformable object area synthetic sequence initial depth guess ims frame computed stereo matching algorithm proposed structure correctly artificial pits bumps illustrate computed formulations initial disparity maps formulations egs computed rule-based stereo algorithm proposed egs-f1 assumes initial disparity maps notice artificial pits bumps clearly preserved recovered structure rule-based stereo matching demonstrate stereo produces smooth disparity maps clearly details notice egs-f2 produces better scene structure egs-f1 sphere preserved table initial disparities motion minimization initial scene motion constraints structure estimation table quantitative evaluation recovered structure systems disparity correctly recovered difference estimated zhang three-dimensional scene flow structure structure real image image sequence reference camera illustrate recovered scene structure disparity disparities recovered correctly evaluation disparities recovered ims egs note rule-based stereo matching performed algorithm described quantitative egs-f2 achieves best structure recovered 3-d scene flow computed egs-f2 illustrated quantitative evaluation recovered 3-d scene flow ims egs-f1 tracked parts sphere sphere moves moves ims egs produce clear egs tracked motion egs-f2 achieves best motion ims egs recover 3-d scene flow structure correctly parts synthetic egs generally produces better main egs rule-based stereo matching algorithm produces accurate initial disparity matches report occluded egs-f1 egs-f2 achieves accurate initial disparity motion minimization real scene order test systems performed experiments real scene real image sequences point triclops stereo camera real video image sequences camera test ieee transactions cybernetics--part august 2003 three-dimensional scene flow frames recovered 2-d intensity maps image sequences captured initial stereo correspondences optical flow snapshot captured image sequence illustrated images captured reference camera recovered scene structure initial disparity maps computed direct method based triclops direct method well recovered structure person scene depth recovered structure recovered motion recovered scene structure egs-f1 initial disparity maps computed rule-based stereo matching easily depth discontinuities clearly egs-f2 better compared frame mismatches person close left frame mismatches close person notice mismatches moving parts close intensity motion constraints reliable stereo motion constraints illustrate 3-d scene flow frame recovered moving parts scene tracked recovered structure moving parts preserved egs tracked motion recovered ims noisy frame disparity error close egs-f2 achieves best coupled stereo motion constraints based potential 3-d scene flow structure robust scene structure dynamic scene dynamic presented ims 3-d scene flow structure ims 3-d affine motion model fitting simultaneously compute 3-d motion egs dimensional gradient-based optical flow three dimensional formulations egs accurate initial structure estimation occlusion fusing motion designed novel rule-based stereo matching algorithm produces accurate disparity depth discontinuities well occluded zhang three-dimensional scene flow structure explicitly image segmentation preserve motion depth integrated 2-d motion stereo constraints compute 3-d motion motion stereo real 3-d motion structure analyses experiments synthetic real synthetic real empirical comparison suggested egs-f2 produces accurate rule-based stereo algorithm integration optical framework algorithm measurement robust estimation multiple image optical flow scene ieee computer computer zhang 3-d scene flow structure recovery multiview image ieee computer vision pattern 3-d scene flow structure ieee computer vision pattern nonrigid motion ieee pattern machine optical flow ieee computer vision pattern nonrigid motion 3-d structure image sequences ieee computer vision pattern nonrigid image ieee evaluation dense stereo correspondence formulation stereo correspondence ieee computer computer stereo motion depth image algorithms motion estimation based stereo image ieee computer vision pattern object ieee pattern machine motion estimation sequence noisy stereo ieee pattern machine motion depth step motion image image ieee pattern machine noisy time-varying velocity binocular image joint artificial 3-d motion point point ieee computer vision pattern correspondence motion ieee computer vision pattern translational motion structure binocular image ieee pattern machine motion structure ieee pattern machine dynamic 3-d structure stereo joint motion structure estimation stereo image multi-resolution stereo motion ieee computer stereo multi-resolution stereo image pattern image computer nonrigid motion model-based motion computer optical flow regularization ieee computer vision pattern motion estimation model-based image ieee pattern machine nonrigid motion structure 2-d cloud images ieee pattern machine zhou structure nonrigid motion recovery 2-d monocular ieee computer vision pattern flow field segmentation motion estimation robust ieee pattern machine nonrigid motion structure 2-d image sequences image numerical algorithm ieee pattern machine segmentation local ieee computer vision pattern ieee transactions cybernetics--part august 2003 stereo monocular compute dense depth maps preserve depth joint artificial matching framework stereo ieee computer computer images local ieee pattern machine surface interpolation hierarchical ieee pattern machine zhang received degrees engineering received degrees computer university include computer vision nonrigid motion stereo computer image zhang artificial university university kambhamettu received computer science engineering degrees computer science engineering university computer university pattern include computer image computer kambhamettu received well
dense scene flow based depth multi-channel bilateral filter close relationship depth scene scene flow propose method estimate scene flow appearance images corresponding depth global energy optimization bilateral filter two-step occluded pixels detected consistency appearance corresponding data errors energy appearance depth anisotropic regularization suppress multi-channel bilateral filter introduced correct scene flow non-local proposed approach middlebury captured experiment estimate dense accurate scene flow motion scene motion object approaches proposed estimate scene flow stereo multi-view camera structure scene flow estimated image areas weak dense accurate depth data light cameras cameras color camera estimate structure structure accuracy scene scene flow appearance images corresponding depth global energy optimization multi-channel bilateral filter two-step occluded pixels detected consistency appearance weights assigned heidelberg dense scene flow based depth multi-channel bilateral filter comparison optical flow middlebury stereo reference appearance ground truth optical flow pixels red box estimated optical flow pixels red box estimated proposed optical corresponding data energy suppress over-smoothing scene propose anisotropic regularization energy imposes weak smoothness appearance depth pixels change result global energy optimization multi-channel bilateral outliers accuracy scene motion result figure methods provide dense scene scene flow estimation stereo disparity scene flow estimated cameras over-smoothing motion methods large scene flow accurate robust lack constraints adjacent methods estimate scene flow based optimization global energy brightness constancy matching proposed estimate structure scene flow stereo camera estimation point smoothness constraints method methods based global energy optimization provide dense texture zhang regularization energy function result scene error approaches proposed optical flow estimation relationship appearance anisotropic regularization optical flow introduced scene flow smoothness constraints image bilateral filter optical flow filter accuracy methods relationship structure scene flow bilateral relationship appearance scene image plane point scene flow based point scene accuracy depth scene flow method color images corresponding disparity scene flow estimated motion axis camera change field field corresponding estimate scene flow camera energy function variational scene flow multi-channel bilateral scene flow scene flow scene flow energy function consistency brightness depth assumption scene flow brightness constancy assumption noise change methods based proposed deal brightness constancy multi-channel images data including brightness weight dense scene flow based depth multi-channel bilateral filter deviation large errors assigned energy data deviation influence introduced occluded robust function influence anisotropic energy function deal noise regularization defined function field anisotropic operator weight robust function introduced scene flow motion energy defined data energy estimation scene flow energy scene flow euler-lagrange small euler-lagrange div div zhang div operator perform corresponding defined euler-lagrange deal large scene image method work small large motion paper energy function variational based assumption appearance depth change image smoothed scene flow result scene flow motion depth boundaries pixels depth boundaries larger depth nearby scene flow relationship pixels boundaries pixels depth boundaries depth nearby motion close relationship appearance depth smoothed nearby including larger scene flow estimated variational appearance depth smoothed pixels larger depth field smooth window smoothed defined function tan-1 occlusion kinds motion object second change object kinds consistency appearance depth tan-1 tan-1 dense scene flow based depth multi-channel bilateral filter defined pixels large appearance depth assigned small regularization energy function motion anisotropic regularization regularization energy function deal noise result over-smoothing scene flow motion boundaries adjacent pixels motion appearance relationship scene flow depth propose anisotropic regularization energy function suppress over-smoothing motion imposes weak smooth appearance depth pixels pixel operator defined weights constraints estimated appearance depth small constraints depth pixel pixel close relationship scene multi-channel bilateral filter dense smooth scene flow energy over-smoothing great anisotropic result motion pixels motion boundaries parameters parameters outliers appear influence energy function smoothness adjacent pixels pixels directly zhang multi-channel bilateral filter introduced correct scene flow non-local considered anisotropic function weights appearance weight corresponding image pixels large weight optical influence pixels large occluded weight wsum defined wsum field pixels large marked pixels marked marked marked filter window pixels marked large accuracy paper filter scene flow window pixels pixels marked large wsum pixel corresponding second bilateral small window scene experiment middlebury perform method middlebury images captured cameras axis motion axis ground truth motion axis corresponding middlebury dense scene flow based depth multi-channel bilateral filter approach compared methods provide scene flow depth estimate optical flow appearance scene flow directly disparity depth method disparity estimated experiment methods kinds criteria scene flow flow image plane statistical deviation ground accuracy optical flow change depth criteria directly statistical deviation estimated flow ground truth depth stereo cameras light accuracy depth evaluation criteria noise second evaluation criteria accuracy result global paper evaluation evaluate accuracy optical flow ground truth motion axis error evaluate accuracy aae evaluate table method methods great proposed method imposes constraints areas weak smooth motion areas larger areas influence brightness weak errors scene flow result method accurate motion aae method table compared methods aae aae evaluation compared accuracy motion depth disparity images great disparity image optical flow accurate estimate accurate motion figure scene image over-smoothing flow estimated result scene motion foreground areas weak boundaries flow smooth zhang table errors compared methods methods basha2010 huguet2007 basha2010 huguet2007 basha2010 huguet2007 aae comparison optical flow image middlebury ground truth optical flow col reference image depth middlebury stereo col col col optical flow col images row optical flow images bottom row optical flow red box scene flow estimated approach motion evaluation appearance depth captured appearance depth range depth precision appearance image change light dense scene flow based depth multi-channel bilateral filter comparison optical flow color depth method anisotropic regularization bilateral image optical flow image optical flow occluded pixels marked optical flow estimated occluded pixels marked images bottom row corresponding areas red box row figure experiment scene areas weak bottom row figure experiment camera optical flow smooth work motion foreground areas weak boundaries flow areas smooth scene flow areas weak areas texture boundaries appear scene flow bilateral figure over-smoothing flow estimated method depth considered regularization bilateral outliers appear motion result motion zhang outliers appear scene depth regularization bilateral filter precision scene flow error scene flow large areas weak color image captured brightness constancy assumption light change camera parameters motion methods scene flow areas boundaries result ground methods perform relationship motion foreground boundaries motion scene flow pixels areas lack texture motion ground truth scene flow result method lack ground evaluate precision scene flow color images plane reference consistency occluded pixels detected approach figure scene flow estimated approach accurate motion occlusion accurate propose robust method estimate scene flow based appearance depth method global energy optimization multi-channel bilateral filter two-step dense scene flow suppress over-smoothing motion correct scene flow non-local including image plane experiment depth considered regularization bilateral method accurate scene flow work variational method scene flow estimation stereo ieee scene flow motion dense scene flow based depth multi-channel bilateral filter multi-view scene flow variational ieee large optical matching variational motion ieee transactions machine depth stereo accuracy depth ieee scene flow estimation ieee based scene flow depth ieee computer vision ieee smoothness constraints estimation image ieee transactions machine heidelberg scene flow stereo computer vision image bilateral optical flow estimation occlusion eccv heidelberg optical flow estimation ieee dense accurate multi-view heidelberg dense motion image transactions range flow computer vision image machine vision range flow eccv heidelberg accurate computer vision optical statistical approaches motion accuracy optical flow estimation based eccv heidelberg stereo matching image image
